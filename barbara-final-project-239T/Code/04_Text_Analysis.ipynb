{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the code required to do the text analysis on the tweets. Essentially, I check to see if the words MPs use in their tweets is correlated to their chowkidar score. I find that it is, meaning that we can determine an MPs ideology from what they say, or that what they say is reflective of their ideology. This would mean that we can use Twitter/online presence as a new measure for ideology.\n",
    "\n",
    "Some of the code here (the training and classification) takes an extremely long time to run. I have transformed data into datasets and back to try and cut time where possible, but if the training takes over 2 hours to run, that's normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "matplotlib.rcParams['errorbar.capsize'] = 5\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'}\n",
    "matplotlib.rcParams.update(params)\n",
    "# plt.rc('text', usetex=True)\\n\",\n",
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mp_tweets.csv` contains the handle, party name, and tweet for all 167 MPs I was able to pull data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>party_name</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>will take this issue to honourable KCR gari no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>https://t.co/q1dmCUGq3E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       handle party_name                                              tweet\n",
       "0  RaoKavitha        TRS  will take this issue to honourable KCR gari no...\n",
       "1  RaoKavitha        TRS  ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...\n",
       "2  RaoKavitha        TRS  RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...\n",
       "3  RaoKavitha        TRS  నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...\n",
       "4  RaoKavitha        TRS                            https://t.co/q1dmCUGq3E"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('../Data/mp_tweets.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         will take this issue to honourable KCR gari no...\n",
       "1         ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...\n",
       "2         RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...\n",
       "3         నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...\n",
       "4                                   https://t.co/q1dmCUGq3E\n",
       "                                ...                        \n",
       "388624    @narendramodi @BJP4India @BJP4Bengal https://t...\n",
       "388625    @narendramodi @BJP4India @BJP4Bengal https://t...\n",
       "388626    @narendramodi @BJP4India @BJP4Bengal https://t...\n",
       "388627    @PMOIndia @BJP4India @BJP4Bengal https://t.co/...\n",
       "388628    @PMOIndia @BJP4India @BJP4Bengal https://t.co/...\n",
       "Name: tweet, Length: 388629, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here you can see how some tweets are truncated and in the process of data collection, the remainder of the tweet\n",
    "# was stored in a link that we will however not be able to use for the analysis.\n",
    "tweets_df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "split_pattern = re.compile('[ \\n.,:;!?\"\\'/%&^…()\\[\\]]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stopwords.\n",
    "with open('../Data/englishST.txt') as f:\n",
    "    english_stopwords = f.read().split()\n",
    "with open('../Data/hindiST.txt') as f:\n",
    "    hindi_stopwords = f.read().split()\n",
    "stopwords = set(['RT', 'http', 'https'] + english_stopwords + hindi_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to split a tweet into words.\n",
    "def split_tweet_into_words(tweet):\n",
    "    words = re.split(split_pattern, tweet)\n",
    "    composite_list = [w.split() for w in words]\n",
    "    words = [w.lower() for sublist in composite_list for w in sublist]\n",
    "    words = [w for w in words if not w[0] == '@'] # Remove mentions\n",
    "    return [w for w in words if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dictionaries and topic modeling.\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasCorpus(object):\n",
    "        \n",
    "    def __init__(self, dataframe, stopwords=[]):\n",
    "        self.dataframe = dataframe\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"This returns every document, one at a time.\"\"\"\n",
    "        for idx, row in self.dataframe.iterrows():\n",
    "            yield split_tweet_into_words(row['tweet'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_corpus = PandasCorpus(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tweet_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'honourable', 'kcr', 'gari', 'notice', 'stop', 'protest', 'appeal', 'pharma', 'students', 'jrlgnih3nq']\n"
     ]
    }
   ],
   "source": [
    "for x in tweet_corpus:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52623"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_corpus = [dictionary.doc2bow(doc) for doc in tweet_corpus]\n",
    "tfidf_model = models.TfidfModel(proc_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need a function that turns a tweet into a tensor.\n",
    "def tweet2tensor(tweet):\n",
    "    words = split_tweet_into_words(tweet)\n",
    "    v_sparse = dictionary.doc2bow(words) # tfidf_model[dictionary.doc2bow(words)]\n",
    "    t = torch.zeros(len(dictionary))\n",
    "    for i, x in v_sparse:\n",
    "        t[i] = x\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2tensor(\"I take this issue to heart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the training, we need to create a dataset.\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, target_function=None):\n",
    "        \"\"\"The target_function must take a row of the dataframe, and return \n",
    "        a learning target.\"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.target_function = target_function\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        r = self.dataframe.loc[idx]\n",
    "        tweet = r['tweet']\n",
    "        t = tweet2tensor(tweet)\n",
    "        if self.target_function is None:\n",
    "            return t\n",
    "        else:\n",
    "            target = torch.tensor([float(self.target_function(r))])\n",
    "            return t, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>chowkidar yes/no mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>mphemantgodse</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>salimdotcomrade</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ET_MohdBasheer</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RavneetBittu</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MausamNoor</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source  chowkidar yes/no mean\n",
       "0    mphemantgodse                    0.0\n",
       "1  salimdotcomrade                    0.0\n",
       "2   ET_MohdBasheer                    0.0\n",
       "3     RavneetBittu                    0.0\n",
       "4       MausamNoor                    0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chowkidar_proportions = pd.read_csv('../Results/chowkidar_proportions.csv').drop(['Unnamed: 0'], axis=1)\n",
    "chowkidar_proportions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>party_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>chowkidar yes/no mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>will take this issue to honourable KCR gari no...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>https://t.co/q1dmCUGq3E</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       handle party_name                                              tweet  \\\n",
       "0  RaoKavitha        TRS  will take this issue to honourable KCR gari no...   \n",
       "1  RaoKavitha        TRS  ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...   \n",
       "2  RaoKavitha        TRS  RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...   \n",
       "3  RaoKavitha        TRS  నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...   \n",
       "4  RaoKavitha        TRS                            https://t.co/q1dmCUGq3E   \n",
       "\n",
       "   chowkidar yes/no mean  \n",
       "0               0.013889  \n",
       "1               0.013889  \n",
       "2               0.013889  \n",
       "3               0.013889  \n",
       "4               0.013889  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_proportions = pd.merge(left=tweets_df, right=chowkidar_proportions,\n",
    "                                   left_on='handle', right_on='source').drop(['source'], axis=1)\n",
    "tweets_with_proportions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385399"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_with_proportions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TRS', 'BJP', 'YSRCP', 'TDP', 'INC', 'LJP', 'BLSP', 'INLD',\n",
       "       'JKPDP', 'SHS', 'NCP', 'BJD', 'SAD', 'AAAP', 'SDF', 'PMK', 'ADMK',\n",
       "       'AD', 'AITC'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_proportions['party_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAUTION: \n",
    "1. `chowkidar_proportions` has 327 handles, `tweets_df` only has 167.\n",
    "2. So when we merge the two to get the Chowkidar proportion attached to the tweets, we cut down the number of handles to 167."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decide on a 120/47 train/test split, so I take a sample 120 of the unique handles in tweets_with_proportions and then pull tweets from those handles as my training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 120)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhandle = np.random.choice(tweets_with_proportions['handle'].unique(), 120, replace=False)\n",
    "train_df = tweets_with_proportions.loc[tweets_with_proportions['handle'].isin(rhandle)]\n",
    "len(train_df['handle'].unique()), len(rhandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>party_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>chowkidar yes/no mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>will take this issue to honourable KCR gari no...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>https://t.co/q1dmCUGq3E</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277535</td>\n",
       "      <td>SSAhluwaliaMP</td>\n",
       "      <td>BJP</td>\n",
       "      <td>Subject: Re: Must C SAI  Sartaj -\\nVery soul S...</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277536</td>\n",
       "      <td>SSAhluwaliaMP</td>\n",
       "      <td>BJP</td>\n",
       "      <td>हिन्दी कविताएँ  अौर कहानियाँ  http://t.co/RYMM...</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277537</td>\n",
       "      <td>SSAhluwaliaMP</td>\n",
       "      <td>BJP</td>\n",
       "      <td>Dooj is known as Gurta Gaddi divas also for Si...</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277538</td>\n",
       "      <td>SSAhluwaliaMP</td>\n",
       "      <td>BJP</td>\n",
       "      <td>Cellphone towers damaging birds, bees: Report ...</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277539</td>\n",
       "      <td>SSAhluwaliaMP</td>\n",
       "      <td>BJP</td>\n",
       "      <td>Swaraj satisfied with PM's statement; Not enou...</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277540 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               handle party_name  \\\n",
       "0          RaoKavitha        TRS   \n",
       "1          RaoKavitha        TRS   \n",
       "2          RaoKavitha        TRS   \n",
       "3          RaoKavitha        TRS   \n",
       "4          RaoKavitha        TRS   \n",
       "...               ...        ...   \n",
       "277535  SSAhluwaliaMP        BJP   \n",
       "277536  SSAhluwaliaMP        BJP   \n",
       "277537  SSAhluwaliaMP        BJP   \n",
       "277538  SSAhluwaliaMP        BJP   \n",
       "277539  SSAhluwaliaMP        BJP   \n",
       "\n",
       "                                                    tweet  \\\n",
       "0       will take this issue to honourable KCR gari no...   \n",
       "1       ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...   \n",
       "2       RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...   \n",
       "3       నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...   \n",
       "4                                 https://t.co/q1dmCUGq3E   \n",
       "...                                                   ...   \n",
       "277535  Subject: Re: Must C SAI  Sartaj -\\nVery soul S...   \n",
       "277536  हिन्दी कविताएँ  अौर कहानियाँ  http://t.co/RYMM...   \n",
       "277537  Dooj is known as Gurta Gaddi divas also for Si...   \n",
       "277538  Cellphone towers damaging birds, bees: Report ...   \n",
       "277539  Swaraj satisfied with PM's statement; Not enou...   \n",
       "\n",
       "        chowkidar yes/no mean  \n",
       "0                    0.013889  \n",
       "1                    0.013889  \n",
       "2                    0.013889  \n",
       "3                    0.013889  \n",
       "4                    0.013889  \n",
       "...                       ...  \n",
       "277535               0.307692  \n",
       "277536               0.307692  \n",
       "277537               0.307692  \n",
       "277538               0.307692  \n",
       "277539               0.307692  \n",
       "\n",
       "[277540 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.reset_index().drop(['index'], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277540"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attended for Ayyappa puja at Korutla on the occasion of vasanta panchami https://t.co/BYYA2X5Rpu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tweet'][1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(r'../Results/train_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's build a neural net for recognizing the author. \n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(Module):\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # Now we define the layers. \n",
    "        # A linear layer computes \\sum_i w_ij x_j + b_i\n",
    "        self.layer1 = nn.Linear(num_inputs, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the first layer, which is linear.\n",
    "        x = self.layer1(x)\n",
    "        # Then, applies a sigmoid function to translate [-infty +infty] to [0, 1]\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function.\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.nn import MSELoss\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "def train(net, dataset, num_epochs=1, lr=1.0):\n",
    "    # We need to define an optimizer, which is something that drives the learning.\n",
    "    params = net.parameters() # These are the parameters that need tuning. \n",
    "    optimizer = optim.Adadelta(params, lr=lr) # This is the thing that tunes them.\n",
    "    train_loader = DataLoader(dataset, shuffle=True, batch_size=100)\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        print(\"------- Epoch: {} -------\".format(epoch_idx + 1))\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # Forget old gradient. \n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(\"batch: {} loss: {}\".format(batch_idx, loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we train\n",
    "net = SimpleNet(len(dictionary))\n",
    "\n",
    "def target_function(row):\n",
    "    return row['chowkidar yes/no mean']\n",
    "    \n",
    "training_dataset = TwitterDataset(train_df, target_function=target_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013888888888888888"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying it out on the first handle in train_df\n",
    "\n",
    "target_function(train_df.iloc[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([0.5079]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.056264203041791916\n",
      "batch: 10 loss: 0.04566383361816406\n",
      "batch: 20 loss: 0.046088751405477524\n",
      "batch: 30 loss: 0.03017589822411537\n",
      "batch: 40 loss: 0.037242185324430466\n",
      "batch: 50 loss: 0.032350484281778336\n",
      "batch: 60 loss: 0.03300950303673744\n",
      "batch: 70 loss: 0.0270206481218338\n",
      "batch: 80 loss: 0.022108305245637894\n",
      "batch: 90 loss: 0.02374693937599659\n",
      "batch: 100 loss: 0.0270045455545187\n",
      "batch: 110 loss: 0.028975822031497955\n",
      "batch: 120 loss: 0.028973808512091637\n",
      "batch: 130 loss: 0.022320425137877464\n",
      "batch: 140 loss: 0.018688920885324478\n",
      "batch: 150 loss: 0.019501972943544388\n",
      "batch: 160 loss: 0.02543499693274498\n",
      "batch: 170 loss: 0.024617761373519897\n",
      "batch: 180 loss: 0.021008791401982307\n",
      "batch: 190 loss: 0.025364786386489868\n",
      "batch: 200 loss: 0.02729552611708641\n",
      "batch: 210 loss: 0.022872084751725197\n",
      "batch: 220 loss: 0.025612277910113335\n",
      "batch: 230 loss: 0.02410421147942543\n",
      "batch: 240 loss: 0.02109389193356037\n",
      "batch: 250 loss: 0.019705114886164665\n",
      "batch: 260 loss: 0.022771555930376053\n",
      "batch: 270 loss: 0.023656753823161125\n",
      "batch: 280 loss: 0.01562197133898735\n",
      "batch: 290 loss: 0.027978336438536644\n",
      "batch: 300 loss: 0.025015514343976974\n",
      "batch: 310 loss: 0.015747815370559692\n",
      "batch: 320 loss: 0.019384097307920456\n",
      "batch: 330 loss: 0.018664315342903137\n",
      "batch: 340 loss: 0.026386165991425514\n",
      "batch: 350 loss: 0.02234061248600483\n",
      "batch: 360 loss: 0.02332068793475628\n",
      "batch: 370 loss: 0.01832694187760353\n",
      "batch: 380 loss: 0.018069108948111534\n",
      "batch: 390 loss: 0.01928233541548252\n",
      "batch: 400 loss: 0.01916944794356823\n",
      "batch: 410 loss: 0.01999637857079506\n",
      "batch: 420 loss: 0.01656785048544407\n",
      "batch: 430 loss: 0.02133885957300663\n",
      "batch: 440 loss: 0.021176595240831375\n",
      "batch: 450 loss: 0.016654914245009422\n",
      "batch: 460 loss: 0.01673083007335663\n",
      "batch: 470 loss: 0.020826933905482292\n",
      "batch: 480 loss: 0.019919103011488914\n",
      "batch: 490 loss: 0.020931454375386238\n",
      "batch: 500 loss: 0.01813734509050846\n",
      "batch: 510 loss: 0.023175887763500214\n",
      "batch: 520 loss: 0.017603619024157524\n",
      "batch: 530 loss: 0.022345470264554024\n",
      "batch: 540 loss: 0.02370491810142994\n",
      "batch: 550 loss: 0.01681356318295002\n",
      "batch: 560 loss: 0.017507746815681458\n",
      "batch: 570 loss: 0.021710263565182686\n",
      "batch: 580 loss: 0.015207074582576752\n",
      "batch: 590 loss: 0.022672312334179878\n",
      "batch: 600 loss: 0.020194364711642265\n",
      "batch: 610 loss: 0.02026492916047573\n",
      "batch: 620 loss: 0.01748807355761528\n",
      "batch: 630 loss: 0.02001136727631092\n",
      "batch: 640 loss: 0.020738936960697174\n",
      "batch: 650 loss: 0.01684441976249218\n",
      "batch: 660 loss: 0.019562149420380592\n",
      "batch: 670 loss: 0.022740596905350685\n",
      "batch: 680 loss: 0.018878502771258354\n",
      "batch: 690 loss: 0.01801106333732605\n",
      "batch: 700 loss: 0.01904417760670185\n",
      "batch: 710 loss: 0.016753239557147026\n",
      "batch: 720 loss: 0.02281668595969677\n",
      "batch: 730 loss: 0.023349778726696968\n",
      "batch: 740 loss: 0.020945176482200623\n",
      "batch: 750 loss: 0.01457370538264513\n",
      "batch: 760 loss: 0.016920706257224083\n",
      "batch: 770 loss: 0.01813967153429985\n",
      "batch: 780 loss: 0.01539313793182373\n",
      "batch: 790 loss: 0.01715095154941082\n",
      "batch: 800 loss: 0.018068529665470123\n",
      "batch: 810 loss: 0.01784820668399334\n",
      "batch: 820 loss: 0.01619173027575016\n",
      "batch: 830 loss: 0.019658595323562622\n",
      "batch: 840 loss: 0.01734369993209839\n",
      "batch: 850 loss: 0.02220255881547928\n",
      "batch: 860 loss: 0.021160051226615906\n",
      "batch: 870 loss: 0.0167330801486969\n",
      "batch: 880 loss: 0.019591644406318665\n",
      "batch: 890 loss: 0.018881386145949364\n",
      "batch: 900 loss: 0.016867700964212418\n",
      "batch: 910 loss: 0.017251867800951004\n",
      "batch: 920 loss: 0.01624869741499424\n",
      "batch: 930 loss: 0.012557732872664928\n",
      "batch: 940 loss: 0.014647804200649261\n",
      "batch: 950 loss: 0.018250534310936928\n",
      "batch: 960 loss: 0.01940212771296501\n",
      "batch: 970 loss: 0.021306386217474937\n",
      "batch: 980 loss: 0.018909547477960587\n",
      "batch: 990 loss: 0.01565239019691944\n",
      "batch: 1000 loss: 0.014867198653519154\n",
      "batch: 1010 loss: 0.017858052626252174\n",
      "batch: 1020 loss: 0.018922165036201477\n",
      "batch: 1030 loss: 0.017559625208377838\n",
      "batch: 1040 loss: 0.01603957824409008\n",
      "batch: 1050 loss: 0.02074764110147953\n",
      "batch: 1060 loss: 0.014975190162658691\n",
      "batch: 1070 loss: 0.015976861119270325\n",
      "batch: 1080 loss: 0.02012079581618309\n",
      "batch: 1090 loss: 0.01911473460495472\n",
      "batch: 1100 loss: 0.016742732375860214\n",
      "batch: 1110 loss: 0.026284584775567055\n",
      "batch: 1120 loss: 0.016353599727153778\n",
      "batch: 1130 loss: 0.018107270821928978\n",
      "batch: 1140 loss: 0.021124161779880524\n",
      "batch: 1150 loss: 0.02058030664920807\n",
      "batch: 1160 loss: 0.021073857322335243\n",
      "batch: 1170 loss: 0.015208419412374496\n",
      "batch: 1180 loss: 0.019467731937766075\n",
      "batch: 1190 loss: 0.01826494000852108\n",
      "batch: 1200 loss: 0.021863676607608795\n",
      "batch: 1210 loss: 0.017029518261551857\n",
      "batch: 1220 loss: 0.019890228286385536\n",
      "batch: 1230 loss: 0.01904268004000187\n",
      "batch: 1240 loss: 0.01627720147371292\n",
      "batch: 1250 loss: 0.01918507181107998\n",
      "batch: 1260 loss: 0.01603342592716217\n",
      "batch: 1270 loss: 0.02155941165983677\n",
      "batch: 1280 loss: 0.01722061261534691\n",
      "batch: 1290 loss: 0.01847720518708229\n",
      "batch: 1300 loss: 0.017069922760128975\n",
      "batch: 1310 loss: 0.017944470047950745\n",
      "batch: 1320 loss: 0.01765933632850647\n",
      "batch: 1330 loss: 0.015741633251309395\n",
      "batch: 1340 loss: 0.017248423770070076\n",
      "batch: 1350 loss: 0.016684647649526596\n",
      "batch: 1360 loss: 0.013859858736395836\n",
      "batch: 1370 loss: 0.0169737059623003\n",
      "batch: 1380 loss: 0.019622599706053734\n",
      "batch: 1390 loss: 0.02098696306347847\n",
      "batch: 1400 loss: 0.017206421121954918\n",
      "batch: 1410 loss: 0.015466783195734024\n",
      "batch: 1420 loss: 0.021755142137408257\n",
      "batch: 1430 loss: 0.017191089689731598\n",
      "batch: 1440 loss: 0.02541535347700119\n",
      "batch: 1450 loss: 0.015674278140068054\n",
      "batch: 1460 loss: 0.01921706274151802\n",
      "batch: 1470 loss: 0.02085939608514309\n",
      "batch: 1480 loss: 0.018106067553162575\n",
      "batch: 1490 loss: 0.019616585224866867\n",
      "batch: 1500 loss: 0.016660822555422783\n",
      "batch: 1510 loss: 0.02118903584778309\n",
      "batch: 1520 loss: 0.016183583065867424\n",
      "batch: 1530 loss: 0.020515035837888718\n",
      "batch: 1540 loss: 0.013248690403997898\n",
      "batch: 1550 loss: 0.017092477530241013\n",
      "batch: 1560 loss: 0.020233962684869766\n",
      "batch: 1570 loss: 0.020611336454749107\n",
      "batch: 1580 loss: 0.016190873458981514\n",
      "batch: 1590 loss: 0.020125431939959526\n",
      "batch: 1600 loss: 0.019617879763245583\n",
      "batch: 1610 loss: 0.02082192897796631\n",
      "batch: 1620 loss: 0.018957316875457764\n",
      "batch: 1630 loss: 0.01972733996808529\n",
      "batch: 1640 loss: 0.015762927010655403\n",
      "batch: 1650 loss: 0.01842513307929039\n",
      "batch: 1660 loss: 0.017359111458063126\n",
      "batch: 1670 loss: 0.018100041896104813\n",
      "batch: 1680 loss: 0.02324488013982773\n",
      "batch: 1690 loss: 0.018715426325798035\n",
      "batch: 1700 loss: 0.01505136489868164\n",
      "batch: 1710 loss: 0.016382591798901558\n",
      "batch: 1720 loss: 0.01661183498799801\n",
      "batch: 1730 loss: 0.015100331045687199\n",
      "batch: 1740 loss: 0.021576980128884315\n",
      "batch: 1750 loss: 0.013858411461114883\n",
      "batch: 1760 loss: 0.01743294671177864\n",
      "batch: 1770 loss: 0.01878770999610424\n",
      "batch: 1780 loss: 0.014797357842326164\n",
      "batch: 1790 loss: 0.02037462778389454\n",
      "batch: 1800 loss: 0.018473805859684944\n",
      "batch: 1810 loss: 0.01710730418562889\n",
      "batch: 1820 loss: 0.01596314273774624\n",
      "batch: 1830 loss: 0.02393161877989769\n",
      "batch: 1840 loss: 0.019006753340363503\n",
      "batch: 1850 loss: 0.015422286465764046\n",
      "batch: 1860 loss: 0.01597183384001255\n",
      "batch: 1870 loss: 0.01890549249947071\n",
      "batch: 1880 loss: 0.020208893343806267\n",
      "batch: 1890 loss: 0.015162787400186062\n",
      "batch: 1900 loss: 0.02469377964735031\n",
      "batch: 1910 loss: 0.020316211506724358\n",
      "batch: 1920 loss: 0.016277840360999107\n",
      "batch: 1930 loss: 0.020891187712550163\n",
      "batch: 1940 loss: 0.01741057261824608\n",
      "batch: 1950 loss: 0.01955810748040676\n",
      "batch: 1960 loss: 0.014439296908676624\n",
      "batch: 1970 loss: 0.020282527431845665\n",
      "batch: 1980 loss: 0.016843581572175026\n",
      "batch: 1990 loss: 0.018399037420749664\n",
      "batch: 2000 loss: 0.017375970259308815\n",
      "batch: 2010 loss: 0.016832463443279266\n",
      "batch: 2020 loss: 0.019261999055743217\n",
      "batch: 2030 loss: 0.01893213950097561\n",
      "batch: 2040 loss: 0.01823420077562332\n",
      "batch: 2050 loss: 0.01780172809958458\n",
      "batch: 2060 loss: 0.017151420935988426\n",
      "batch: 2070 loss: 0.01880050264298916\n",
      "batch: 2080 loss: 0.01884918287396431\n",
      "batch: 2090 loss: 0.0184573195874691\n",
      "batch: 2100 loss: 0.016738347709178925\n",
      "batch: 2110 loss: 0.01632560044527054\n",
      "batch: 2120 loss: 0.01586942747235298\n",
      "batch: 2130 loss: 0.017949501052498817\n",
      "batch: 2140 loss: 0.019324442371726036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2150 loss: 0.013790507800877094\n",
      "batch: 2160 loss: 0.014517415314912796\n",
      "batch: 2170 loss: 0.01387226115912199\n",
      "batch: 2180 loss: 0.01825004816055298\n",
      "batch: 2190 loss: 0.025204923003911972\n",
      "batch: 2200 loss: 0.015199241228401661\n",
      "batch: 2210 loss: 0.019699586555361748\n",
      "batch: 2220 loss: 0.01975261978805065\n",
      "batch: 2230 loss: 0.014911260455846786\n",
      "batch: 2240 loss: 0.018519245088100433\n",
      "batch: 2250 loss: 0.013703232631087303\n",
      "batch: 2260 loss: 0.019980143755674362\n",
      "batch: 2270 loss: 0.01982164941728115\n",
      "batch: 2280 loss: 0.017456691712141037\n",
      "batch: 2290 loss: 0.0157234538346529\n",
      "batch: 2300 loss: 0.016792379319667816\n",
      "batch: 2310 loss: 0.016849040985107422\n",
      "batch: 2320 loss: 0.01642029732465744\n",
      "batch: 2330 loss: 0.014428081922233105\n",
      "batch: 2340 loss: 0.018535306677222252\n",
      "batch: 2350 loss: 0.017910659313201904\n",
      "batch: 2360 loss: 0.02007385343313217\n",
      "batch: 2370 loss: 0.012912854552268982\n",
      "batch: 2380 loss: 0.01925254426896572\n",
      "batch: 2390 loss: 0.01721501722931862\n",
      "batch: 2400 loss: 0.018219638615846634\n",
      "batch: 2410 loss: 0.013352885842323303\n",
      "batch: 2420 loss: 0.018229840323328972\n",
      "batch: 2430 loss: 0.02327887900173664\n",
      "batch: 2440 loss: 0.012924965471029282\n",
      "batch: 2450 loss: 0.020143302157521248\n",
      "batch: 2460 loss: 0.016673000529408455\n",
      "batch: 2470 loss: 0.018034126609563828\n",
      "batch: 2480 loss: 0.01605529524385929\n",
      "batch: 2490 loss: 0.018006250262260437\n",
      "batch: 2500 loss: 0.014043972827494144\n",
      "batch: 2510 loss: 0.016641732305288315\n",
      "batch: 2520 loss: 0.01795145869255066\n",
      "batch: 2530 loss: 0.015842149034142494\n",
      "batch: 2540 loss: 0.015461993403732777\n",
      "batch: 2550 loss: 0.018369438126683235\n",
      "batch: 2560 loss: 0.013802744448184967\n",
      "batch: 2570 loss: 0.01592555269598961\n",
      "batch: 2580 loss: 0.01933947391808033\n",
      "batch: 2590 loss: 0.02055400051176548\n",
      "batch: 2600 loss: 0.017777107656002045\n",
      "batch: 2610 loss: 0.01878831349313259\n",
      "batch: 2620 loss: 0.016750309616327286\n",
      "batch: 2630 loss: 0.017924606800079346\n",
      "batch: 2640 loss: 0.02031467668712139\n",
      "batch: 2650 loss: 0.01774689555168152\n",
      "batch: 2660 loss: 0.021090015769004822\n",
      "batch: 2670 loss: 0.017323503270745277\n",
      "batch: 2680 loss: 0.019450096413493156\n",
      "batch: 2690 loss: 0.013283757492899895\n",
      "batch: 2700 loss: 0.014833961613476276\n",
      "batch: 2710 loss: 0.019817017018795013\n",
      "batch: 2720 loss: 0.016643555834889412\n",
      "batch: 2730 loss: 0.014622317627072334\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.013354255817830563\n",
      "batch: 10 loss: 0.022608380764722824\n",
      "batch: 20 loss: 0.018498513847589493\n",
      "batch: 30 loss: 0.018006669357419014\n",
      "batch: 40 loss: 0.019844254478812218\n",
      "batch: 50 loss: 0.014329415746033192\n",
      "batch: 60 loss: 0.01672348752617836\n",
      "batch: 70 loss: 0.013438460417091846\n",
      "batch: 80 loss: 0.02163354866206646\n",
      "batch: 90 loss: 0.014517722651362419\n",
      "batch: 100 loss: 0.017191121354699135\n",
      "batch: 110 loss: 0.013564196415245533\n",
      "batch: 120 loss: 0.01586063951253891\n",
      "batch: 130 loss: 0.016438769176602364\n",
      "batch: 140 loss: 0.01699916645884514\n",
      "batch: 150 loss: 0.02061924897134304\n",
      "batch: 160 loss: 0.020197350531816483\n",
      "batch: 170 loss: 0.019486894831061363\n",
      "batch: 180 loss: 0.022935619577765465\n",
      "batch: 190 loss: 0.01934860460460186\n",
      "batch: 200 loss: 0.016036875545978546\n",
      "batch: 210 loss: 0.01864650472998619\n",
      "batch: 220 loss: 0.01420424785465002\n",
      "batch: 230 loss: 0.018159370869398117\n",
      "batch: 240 loss: 0.020067302510142326\n",
      "batch: 250 loss: 0.016629274934530258\n",
      "batch: 260 loss: 0.013769838958978653\n",
      "batch: 270 loss: 0.015248006209731102\n",
      "batch: 280 loss: 0.019736574962735176\n",
      "batch: 290 loss: 0.017740678042173386\n",
      "batch: 300 loss: 0.01760641112923622\n",
      "batch: 310 loss: 0.015379373915493488\n",
      "batch: 320 loss: 0.01745223067700863\n",
      "batch: 330 loss: 0.019617952406406403\n",
      "batch: 340 loss: 0.01525590568780899\n",
      "batch: 350 loss: 0.016385553404688835\n",
      "batch: 360 loss: 0.015560013242065907\n",
      "batch: 370 loss: 0.014038028195500374\n",
      "batch: 380 loss: 0.017741644755005836\n",
      "batch: 390 loss: 0.018229486420750618\n",
      "batch: 400 loss: 0.016580387949943542\n",
      "batch: 410 loss: 0.010821917094290257\n",
      "batch: 420 loss: 0.013732995837926865\n",
      "batch: 430 loss: 0.01642928086221218\n",
      "batch: 440 loss: 0.02396935597062111\n",
      "batch: 450 loss: 0.01953951083123684\n",
      "batch: 460 loss: 0.015943635255098343\n",
      "batch: 470 loss: 0.015120463445782661\n",
      "batch: 480 loss: 0.015944689512252808\n",
      "batch: 490 loss: 0.018048299476504326\n",
      "batch: 500 loss: 0.020345492288470268\n",
      "batch: 510 loss: 0.020741494372487068\n",
      "batch: 520 loss: 0.017601579427719116\n",
      "batch: 530 loss: 0.017422549426555634\n",
      "batch: 540 loss: 0.016997341066598892\n",
      "batch: 550 loss: 0.01674662157893181\n",
      "batch: 560 loss: 0.02161274664103985\n",
      "batch: 570 loss: 0.02303834818303585\n",
      "batch: 580 loss: 0.017189515754580498\n",
      "batch: 590 loss: 0.016726048663258553\n",
      "batch: 600 loss: 0.0196961909532547\n",
      "batch: 610 loss: 0.02078220434486866\n",
      "batch: 620 loss: 0.019015025347471237\n",
      "batch: 630 loss: 0.017257995903491974\n",
      "batch: 640 loss: 0.017096534371376038\n",
      "batch: 650 loss: 0.017941560596227646\n",
      "batch: 660 loss: 0.017803583294153214\n",
      "batch: 670 loss: 0.016894321888685226\n",
      "batch: 680 loss: 0.0189994927495718\n",
      "batch: 690 loss: 0.011614122427999973\n",
      "batch: 700 loss: 0.015141812153160572\n",
      "batch: 710 loss: 0.017238501459360123\n",
      "batch: 720 loss: 0.020323196426033974\n",
      "batch: 730 loss: 0.02144976705312729\n",
      "batch: 740 loss: 0.016663169488310814\n",
      "batch: 750 loss: 0.01467957254499197\n",
      "batch: 760 loss: 0.01870729587972164\n",
      "batch: 770 loss: 0.01360027864575386\n",
      "batch: 780 loss: 0.015680590644478798\n",
      "batch: 790 loss: 0.014446040615439415\n",
      "batch: 800 loss: 0.013717499561607838\n",
      "batch: 810 loss: 0.017857519909739494\n",
      "batch: 820 loss: 0.017139658331871033\n",
      "batch: 830 loss: 0.014467098750174046\n",
      "batch: 840 loss: 0.016083599999547005\n",
      "batch: 850 loss: 0.014059415087103844\n",
      "batch: 860 loss: 0.017444202676415443\n",
      "batch: 870 loss: 0.016216183081269264\n",
      "batch: 880 loss: 0.01427456270903349\n",
      "batch: 890 loss: 0.021612199023365974\n",
      "batch: 900 loss: 0.017590289935469627\n",
      "batch: 910 loss: 0.017204495146870613\n",
      "batch: 920 loss: 0.015987539663910866\n",
      "batch: 930 loss: 0.014793450012803078\n",
      "batch: 940 loss: 0.019777953624725342\n",
      "batch: 950 loss: 0.019705304875969887\n",
      "batch: 960 loss: 0.016473200172185898\n",
      "batch: 970 loss: 0.017370885238051414\n",
      "batch: 980 loss: 0.015473304316401482\n",
      "batch: 990 loss: 0.02019132673740387\n",
      "batch: 1000 loss: 0.015652906149625778\n",
      "batch: 1010 loss: 0.017149576917290688\n",
      "batch: 1020 loss: 0.02151574194431305\n",
      "batch: 1030 loss: 0.019566863775253296\n",
      "batch: 1040 loss: 0.018858201801776886\n",
      "batch: 1050 loss: 0.018374886363744736\n",
      "batch: 1060 loss: 0.016207825392484665\n",
      "batch: 1070 loss: 0.019378257915377617\n",
      "batch: 1080 loss: 0.015347733162343502\n",
      "batch: 1090 loss: 0.018996577709913254\n",
      "batch: 1100 loss: 0.023855246603488922\n",
      "batch: 1110 loss: 0.014235083945095539\n",
      "batch: 1120 loss: 0.018671594560146332\n",
      "batch: 1130 loss: 0.013968913815915585\n",
      "batch: 1140 loss: 0.01823238842189312\n",
      "batch: 1150 loss: 0.01611989177763462\n",
      "batch: 1160 loss: 0.01991373859345913\n",
      "batch: 1170 loss: 0.021312952041625977\n",
      "batch: 1180 loss: 0.019069524481892586\n",
      "batch: 1190 loss: 0.017590679228305817\n",
      "batch: 1200 loss: 0.013218909502029419\n",
      "batch: 1210 loss: 0.01953454129397869\n",
      "batch: 1220 loss: 0.021876558661460876\n",
      "batch: 1230 loss: 0.018430989235639572\n",
      "batch: 1240 loss: 0.018234573304653168\n",
      "batch: 1250 loss: 0.01634950190782547\n",
      "batch: 1260 loss: 0.017036722972989082\n",
      "batch: 1270 loss: 0.01690615899860859\n",
      "batch: 1280 loss: 0.01788659580051899\n",
      "batch: 1290 loss: 0.01513151079416275\n",
      "batch: 1300 loss: 0.014738055877387524\n",
      "batch: 1310 loss: 0.015462163835763931\n",
      "batch: 1320 loss: 0.016869721934199333\n",
      "batch: 1330 loss: 0.01468848530203104\n",
      "batch: 1340 loss: 0.0151051115244627\n",
      "batch: 1350 loss: 0.016871999949216843\n",
      "batch: 1360 loss: 0.014247579500079155\n",
      "batch: 1370 loss: 0.019738642498850822\n",
      "batch: 1380 loss: 0.015167927369475365\n",
      "batch: 1390 loss: 0.014155044220387936\n",
      "batch: 1400 loss: 0.01391950249671936\n",
      "batch: 1410 loss: 0.017517583444714546\n",
      "batch: 1420 loss: 0.016774719581007957\n",
      "batch: 1430 loss: 0.016237454488873482\n",
      "batch: 1440 loss: 0.013797133229672909\n",
      "batch: 1450 loss: 0.02182108536362648\n",
      "batch: 1460 loss: 0.01990325190126896\n",
      "batch: 1470 loss: 0.016888996586203575\n",
      "batch: 1480 loss: 0.014813344925642014\n",
      "batch: 1490 loss: 0.015004194341599941\n",
      "batch: 1500 loss: 0.016212433576583862\n",
      "batch: 1510 loss: 0.019222863018512726\n",
      "batch: 1520 loss: 0.014697144739329815\n",
      "batch: 1530 loss: 0.012460439465939999\n",
      "batch: 1540 loss: 0.014171991497278214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1550 loss: 0.018232643604278564\n",
      "batch: 1560 loss: 0.018131161108613014\n",
      "batch: 1570 loss: 0.015620464459061623\n",
      "batch: 1580 loss: 0.020808430388569832\n",
      "batch: 1590 loss: 0.018359452486038208\n",
      "batch: 1600 loss: 0.01863120123744011\n",
      "batch: 1610 loss: 0.0141286076977849\n",
      "batch: 1620 loss: 0.014796321280300617\n",
      "batch: 1630 loss: 0.013947018422186375\n",
      "batch: 1640 loss: 0.01408083364367485\n",
      "batch: 1650 loss: 0.016975998878479004\n",
      "batch: 1660 loss: 0.018050042912364006\n",
      "batch: 1670 loss: 0.0164007768034935\n",
      "batch: 1680 loss: 0.02133498154580593\n",
      "batch: 1690 loss: 0.017139311879873276\n",
      "batch: 1700 loss: 0.017488954588770866\n",
      "batch: 1710 loss: 0.017502447590231895\n",
      "batch: 1720 loss: 0.01623603142797947\n",
      "batch: 1730 loss: 0.01548338308930397\n",
      "batch: 1740 loss: 0.01812134124338627\n",
      "batch: 1750 loss: 0.017886711284518242\n",
      "batch: 1760 loss: 0.01886448636651039\n",
      "batch: 1770 loss: 0.02052503451704979\n",
      "batch: 1780 loss: 0.0167836956679821\n",
      "batch: 1790 loss: 0.022122766822576523\n",
      "batch: 1800 loss: 0.019917631521821022\n",
      "batch: 1810 loss: 0.018620647490024567\n",
      "batch: 1820 loss: 0.01738300919532776\n",
      "batch: 1830 loss: 0.011790874414145947\n",
      "batch: 1840 loss: 0.01776888594031334\n",
      "batch: 1850 loss: 0.01584337279200554\n",
      "batch: 1860 loss: 0.01276705414056778\n",
      "batch: 1870 loss: 0.013536404818296432\n",
      "batch: 1880 loss: 0.017283981665968895\n",
      "batch: 1890 loss: 0.01837010495364666\n",
      "batch: 1900 loss: 0.015621846541762352\n",
      "batch: 1910 loss: 0.015739433467388153\n",
      "batch: 1920 loss: 0.01617087423801422\n",
      "batch: 1930 loss: 0.015186949633061886\n",
      "batch: 1940 loss: 0.016216356307268143\n",
      "batch: 1950 loss: 0.01772785745561123\n",
      "batch: 1960 loss: 0.01409715786576271\n",
      "batch: 1970 loss: 0.016877995803952217\n",
      "batch: 1980 loss: 0.01693163998425007\n",
      "batch: 1990 loss: 0.017410140484571457\n",
      "batch: 2000 loss: 0.014341377653181553\n",
      "batch: 2010 loss: 0.016776541247963905\n",
      "batch: 2020 loss: 0.016897637397050858\n",
      "batch: 2030 loss: 0.01887686364352703\n",
      "batch: 2040 loss: 0.021533632650971413\n",
      "batch: 2050 loss: 0.022252462804317474\n",
      "batch: 2060 loss: 0.014500483870506287\n",
      "batch: 2070 loss: 0.016582487151026726\n",
      "batch: 2080 loss: 0.01886795647442341\n",
      "batch: 2090 loss: 0.018646540120244026\n",
      "batch: 2100 loss: 0.01801299676299095\n",
      "batch: 2110 loss: 0.020842991769313812\n",
      "batch: 2120 loss: 0.010890606790781021\n",
      "batch: 2130 loss: 0.015791291370987892\n",
      "batch: 2140 loss: 0.01868816278874874\n",
      "batch: 2150 loss: 0.014480678364634514\n",
      "batch: 2160 loss: 0.018085505813360214\n",
      "batch: 2170 loss: 0.017417607828974724\n",
      "batch: 2180 loss: 0.017821112647652626\n",
      "batch: 2190 loss: 0.014327773824334145\n",
      "batch: 2200 loss: 0.014060706831514835\n",
      "batch: 2210 loss: 0.01944674178957939\n",
      "batch: 2220 loss: 0.014850117266178131\n",
      "batch: 2230 loss: 0.011639145202934742\n",
      "batch: 2240 loss: 0.01744915544986725\n",
      "batch: 2250 loss: 0.01601244881749153\n",
      "batch: 2260 loss: 0.01701125130057335\n",
      "batch: 2270 loss: 0.015640918165445328\n",
      "batch: 2280 loss: 0.017758309841156006\n",
      "batch: 2290 loss: 0.017853431403636932\n",
      "batch: 2300 loss: 0.017833618447184563\n",
      "batch: 2310 loss: 0.011981062591075897\n",
      "batch: 2320 loss: 0.012303154915571213\n",
      "batch: 2330 loss: 0.01778736338019371\n",
      "batch: 2340 loss: 0.017572177574038506\n",
      "batch: 2350 loss: 0.018485549837350845\n",
      "batch: 2360 loss: 0.015874195843935013\n",
      "batch: 2370 loss: 0.01800585724413395\n",
      "batch: 2380 loss: 0.016260020434856415\n",
      "batch: 2390 loss: 0.014877060428261757\n",
      "batch: 2400 loss: 0.016117753461003304\n",
      "batch: 2410 loss: 0.018317321315407753\n",
      "batch: 2420 loss: 0.01437359768897295\n",
      "batch: 2430 loss: 0.01509001199156046\n",
      "batch: 2440 loss: 0.014970365911722183\n",
      "batch: 2450 loss: 0.011727695353329182\n",
      "batch: 2460 loss: 0.020693717524409294\n",
      "batch: 2470 loss: 0.020002666860818863\n",
      "batch: 2480 loss: 0.01677885092794895\n",
      "batch: 2490 loss: 0.01723436452448368\n",
      "batch: 2500 loss: 0.018643643707036972\n",
      "batch: 2510 loss: 0.016316894441843033\n",
      "batch: 2520 loss: 0.017474055290222168\n",
      "batch: 2530 loss: 0.016454733908176422\n",
      "batch: 2540 loss: 0.01678277738392353\n",
      "batch: 2550 loss: 0.018157605081796646\n",
      "batch: 2560 loss: 0.016956064850091934\n",
      "batch: 2570 loss: 0.021502822637557983\n",
      "batch: 2580 loss: 0.01762307435274124\n",
      "batch: 2590 loss: 0.018790023401379585\n",
      "batch: 2600 loss: 0.018028147518634796\n",
      "batch: 2610 loss: 0.016416650265455246\n",
      "batch: 2620 loss: 0.016382133588194847\n",
      "batch: 2630 loss: 0.018397973850369453\n",
      "batch: 2640 loss: 0.012385294772684574\n",
      "batch: 2650 loss: 0.01916256733238697\n",
      "batch: 2660 loss: 0.019849196076393127\n",
      "batch: 2670 loss: 0.01844385825097561\n",
      "batch: 2680 loss: 0.014771175570786\n",
      "batch: 2690 loss: 0.02020370028913021\n",
      "batch: 2700 loss: 0.015434875153005123\n",
      "batch: 2710 loss: 0.01805337890982628\n",
      "batch: 2720 loss: 0.02036827988922596\n",
      "batch: 2730 loss: 0.014547624625265598\n"
     ]
    }
   ],
   "source": [
    "train(net, training_dataset, num_epochs=2, lr=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(net, tweet):\n",
    "    t = tweet2tensor(tweet)\n",
    "    return net(t).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_mp(net, handle):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for _, row in tweets_df[tweets_df['handle'] == handle].iterrows():\n",
    "        count += 1\n",
    "        total += classify_tweet(net, row['tweet'])\n",
    "#     print(\"On the basis of:\", count, \"tweets, we classify:\", total / count)\n",
    "    return total/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>party_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>chowkidar yes/no mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Dattatreya</td>\n",
       "      <td>BJP</td>\n",
       "      <td>RT @mygovindia: What are your ideas and sugges...</td>\n",
       "      <td>0.507853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       handle party_name                                              tweet  \\\n",
       "0  Dattatreya        BJP  RT @mygovindia: What are your ideas and sugges...   \n",
       "\n",
       "   chowkidar yes/no mean  \n",
       "0               0.507853  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4412541805500555"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_mp(net, 'myogiadityanath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6818181818181818"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_proportions.loc[tweets_with_proportions['handle'] == 'myogiadityanath']['chowkidar yes/no mean'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table that has all the MPs who weren't in the training set\n",
    "\n",
    "train_mps = train_df['handle'].unique()\n",
    "not_train = tweets_with_proportions.loc[~tweets_with_proportions['handle'].isin(train_mps)]\n",
    "len(not_train['handle'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate set.\n",
    "rhandle2 = not_train['handle'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>party_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>chowkidar yes/no mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>will take this issue to honourable KCR gari no...</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       handle party_name                                              tweet  \\\n",
       "0  RaoKavitha        TRS  will take this issue to honourable KCR gari no...   \n",
       "\n",
       "   chowkidar yes/no mean  \n",
       "0               0.013889  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = tweets_with_proportions.loc[tweets_with_proportions['handle'].isin(rhandle2)]\n",
    "val.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(val['handle'].unique())\n",
    "val_unique = val['handle'].unique()\n",
    "classification = []\n",
    "for mp in val_unique:\n",
    "    classification.append([mp, classify_mp(net, mp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>0.303758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KirenRijiju</td>\n",
       "      <td>0.319382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>rgohainbjp</td>\n",
       "      <td>0.315090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sarbanandsonwal</td>\n",
       "      <td>0.319205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sanjayjaiswalMP</td>\n",
       "      <td>0.346772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            handle  classification\n",
       "0       RaoKavitha        0.303758\n",
       "1      KirenRijiju        0.319382\n",
       "2       rgohainbjp        0.315090\n",
       "3  sarbanandsonwal        0.319205\n",
       "4  sanjayjaiswalMP        0.346772"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df = pd.DataFrame(classification, columns=['handle', 'classification'])\n",
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>party_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>chowkidar yes/no mean</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>will take this issue to honourable KCR gari no...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.303758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.303758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.303758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.303758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RaoKavitha</td>\n",
       "      <td>TRS</td>\n",
       "      <td>https://t.co/q1dmCUGq3E</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.303758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       handle party_name                                              tweet  \\\n",
       "0  RaoKavitha        TRS  will take this issue to honourable KCR gari no...   \n",
       "1  RaoKavitha        TRS  ఎటువంటి జన్మ వచ్చినా.. ఎల్లప్పుడూ\\nఆ శివుడి పా...   \n",
       "2  RaoKavitha        TRS  RT @DrGattu: @RaoKavitha @TelanganaCMO @trshar...   \n",
       "3  RaoKavitha        TRS  నరత్వం దేవత్వం నగవనమృగత్వం మశకత \\nపశుత్వం కీటత...   \n",
       "4  RaoKavitha        TRS                            https://t.co/q1dmCUGq3E   \n",
       "\n",
       "   chowkidar yes/no mean  classification  \n",
       "0               0.013889        0.303758  \n",
       "1               0.013889        0.303758  \n",
       "2               0.013889        0.303758  \n",
       "3               0.013889        0.303758  \n",
       "4               0.013889        0.303758  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_classifications = pd.merge(left=val, right=classification_df,\n",
    "                               left_on='handle', right_on='handle')\n",
    "val_classifications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a254334a8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(val_classifications['chowkidar yes/no mean'], val_classifications['classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.45350997],\n",
       "       [0.45350997, 1.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.corrcoef(val_classifications['chowkidar yes/no mean'], val_classifications['classification'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4535099714715869"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].item(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.025804921984672546\n",
      "batch: 10 loss: 0.014599651098251343\n",
      "batch: 20 loss: 0.019178014248609543\n",
      "batch: 30 loss: 0.020770564675331116\n",
      "batch: 40 loss: 0.02434237115085125\n",
      "batch: 50 loss: 0.020733017474412918\n",
      "batch: 60 loss: 0.024805206805467606\n",
      "batch: 70 loss: 0.020691420882940292\n",
      "batch: 80 loss: 0.02108277752995491\n",
      "batch: 90 loss: 0.018613794818520546\n",
      "batch: 100 loss: 0.018196359276771545\n",
      "batch: 110 loss: 0.020450012758374214\n",
      "batch: 120 loss: 0.019538378342986107\n",
      "batch: 130 loss: 0.024294400587677956\n",
      "batch: 140 loss: 0.01819685474038124\n",
      "batch: 150 loss: 0.015197334811091423\n",
      "batch: 160 loss: 0.02110593020915985\n",
      "batch: 170 loss: 0.018247555941343307\n",
      "batch: 180 loss: 0.0200913418084383\n",
      "batch: 190 loss: 0.01995062828063965\n",
      "batch: 200 loss: 0.01873810589313507\n",
      "batch: 210 loss: 0.022458553314208984\n",
      "batch: 220 loss: 0.01904839277267456\n",
      "batch: 230 loss: 0.017793145030736923\n",
      "batch: 240 loss: 0.019074570387601852\n",
      "batch: 250 loss: 0.0223207026720047\n",
      "batch: 260 loss: 0.021976226940751076\n",
      "batch: 270 loss: 0.02368258871138096\n",
      "batch: 280 loss: 0.0217561274766922\n",
      "batch: 290 loss: 0.02110794000327587\n",
      "batch: 300 loss: 0.019626807421445847\n",
      "batch: 310 loss: 0.021646112203598022\n",
      "batch: 320 loss: 0.019870778545737267\n",
      "batch: 330 loss: 0.012959974817931652\n",
      "batch: 340 loss: 0.02205650508403778\n",
      "batch: 350 loss: 0.021572867408394814\n",
      "batch: 360 loss: 0.0231645330786705\n",
      "batch: 370 loss: 0.022470038384199142\n",
      "batch: 380 loss: 0.021861683577299118\n",
      "batch: 390 loss: 0.020128395408391953\n",
      "batch: 400 loss: 0.01667228899896145\n",
      "batch: 410 loss: 0.02657458372414112\n",
      "batch: 420 loss: 0.024315636605024338\n",
      "batch: 430 loss: 0.024088170379400253\n",
      "batch: 440 loss: 0.02453923039138317\n",
      "batch: 450 loss: 0.019804149866104126\n",
      "batch: 460 loss: 0.020247431471943855\n",
      "batch: 470 loss: 0.022152692079544067\n",
      "batch: 480 loss: 0.02640526369214058\n",
      "batch: 490 loss: 0.022349229082465172\n",
      "batch: 500 loss: 0.02495422773063183\n",
      "batch: 510 loss: 0.019036324694752693\n",
      "batch: 520 loss: 0.01923244819045067\n",
      "batch: 530 loss: 0.021232565864920616\n",
      "batch: 540 loss: 0.016912026330828667\n",
      "batch: 550 loss: 0.023261379450559616\n",
      "batch: 560 loss: 0.01601610705256462\n",
      "batch: 570 loss: 0.021910816431045532\n",
      "batch: 580 loss: 0.02405455708503723\n",
      "batch: 590 loss: 0.022208435460925102\n",
      "batch: 600 loss: 0.02292489819228649\n",
      "batch: 610 loss: 0.027806518599390984\n",
      "batch: 620 loss: 0.016724105924367905\n",
      "batch: 630 loss: 0.021504051983356476\n",
      "batch: 640 loss: 0.019544804468750954\n",
      "batch: 650 loss: 0.01749947853386402\n",
      "batch: 660 loss: 0.01673831045627594\n",
      "batch: 670 loss: 0.021575411781668663\n",
      "batch: 680 loss: 0.022857584059238434\n",
      "batch: 690 loss: 0.02014833129942417\n",
      "batch: 700 loss: 0.02017657645046711\n",
      "batch: 710 loss: 0.02491111308336258\n",
      "batch: 720 loss: 0.01912226341664791\n",
      "batch: 730 loss: 0.019014941528439522\n",
      "batch: 740 loss: 0.015875019133090973\n",
      "batch: 750 loss: 0.015287937596440315\n",
      "batch: 760 loss: 0.016906075179576874\n",
      "batch: 770 loss: 0.024490224197506905\n",
      "batch: 780 loss: 0.020953956991434097\n",
      "batch: 790 loss: 0.021744420751929283\n",
      "batch: 800 loss: 0.018020831048488617\n",
      "batch: 810 loss: 0.022331586107611656\n",
      "batch: 820 loss: 0.019586239010095596\n",
      "batch: 830 loss: 0.018584074452519417\n",
      "batch: 840 loss: 0.02002311311662197\n",
      "batch: 850 loss: 0.02127203717827797\n",
      "batch: 860 loss: 0.01694214902818203\n",
      "batch: 870 loss: 0.021175038069486618\n",
      "batch: 880 loss: 0.01864604838192463\n",
      "batch: 890 loss: 0.02219628170132637\n",
      "batch: 900 loss: 0.01728692278265953\n",
      "batch: 910 loss: 0.02881607972085476\n",
      "batch: 920 loss: 0.017151230946183205\n",
      "batch: 930 loss: 0.020700031891465187\n",
      "batch: 940 loss: 0.014166106469929218\n",
      "batch: 950 loss: 0.018280407413840294\n",
      "batch: 960 loss: 0.016165753826498985\n",
      "batch: 970 loss: 0.02072400599718094\n",
      "batch: 980 loss: 0.02032780461013317\n",
      "batch: 990 loss: 0.016791677102446556\n",
      "batch: 1000 loss: 0.014443936757743359\n",
      "batch: 1010 loss: 0.020948994904756546\n",
      "batch: 1020 loss: 0.02933787740767002\n",
      "batch: 1030 loss: 0.023719197139143944\n",
      "batch: 1040 loss: 0.01678762212395668\n",
      "batch: 1050 loss: 0.0169982947409153\n",
      "batch: 1060 loss: 0.017944179475307465\n",
      "batch: 1070 loss: 0.01950356550514698\n",
      "batch: 1080 loss: 0.016305474564433098\n",
      "batch: 1090 loss: 0.019009875133633614\n",
      "batch: 1100 loss: 0.015759771689772606\n",
      "batch: 1110 loss: 0.022239074110984802\n",
      "batch: 1120 loss: 0.021626770496368408\n",
      "batch: 1130 loss: 0.02020508609712124\n",
      "batch: 1140 loss: 0.017735641449689865\n",
      "batch: 1150 loss: 0.015222666785120964\n",
      "batch: 1160 loss: 0.020286619663238525\n",
      "batch: 1170 loss: 0.021934185177087784\n",
      "batch: 1180 loss: 0.016277924180030823\n",
      "batch: 1190 loss: 0.016059933230280876\n",
      "batch: 1200 loss: 0.023783456534147263\n",
      "batch: 1210 loss: 0.023963619023561478\n",
      "batch: 1220 loss: 0.015397099778056145\n",
      "batch: 1230 loss: 0.019157467409968376\n",
      "batch: 1240 loss: 0.018836675211787224\n",
      "batch: 1250 loss: 0.02045890875160694\n",
      "batch: 1260 loss: 0.020387452095746994\n",
      "batch: 1270 loss: 0.021633576601743698\n",
      "batch: 1280 loss: 0.019855976104736328\n",
      "batch: 1290 loss: 0.01713687740266323\n",
      "batch: 1300 loss: 0.021163923665881157\n",
      "batch: 1310 loss: 0.01950954459607601\n",
      "batch: 1320 loss: 0.015504218637943268\n",
      "batch: 1330 loss: 0.021418821066617966\n",
      "batch: 1340 loss: 0.016242118552327156\n",
      "batch: 1350 loss: 0.01998594030737877\n",
      "batch: 1360 loss: 0.021190553903579712\n",
      "batch: 1370 loss: 0.019536621868610382\n",
      "batch: 1380 loss: 0.01621605083346367\n",
      "batch: 1390 loss: 0.021188024431467056\n",
      "batch: 1400 loss: 0.01912013813853264\n",
      "batch: 1410 loss: 0.01670512743294239\n",
      "batch: 1420 loss: 0.020853273570537567\n",
      "batch: 1430 loss: 0.016015609726309776\n",
      "batch: 1440 loss: 0.01927780732512474\n",
      "batch: 1450 loss: 0.021833503618836403\n",
      "batch: 1460 loss: 0.020725127309560776\n",
      "batch: 1470 loss: 0.020813707262277603\n",
      "batch: 1480 loss: 0.02199133113026619\n",
      "batch: 1490 loss: 0.02298705093562603\n",
      "batch: 1500 loss: 0.014568556100130081\n",
      "batch: 1510 loss: 0.01929941400885582\n",
      "batch: 1520 loss: 0.022230355069041252\n",
      "batch: 1530 loss: 0.02135682851076126\n",
      "batch: 1540 loss: 0.015497113578021526\n",
      "batch: 1550 loss: 0.019613804295659065\n",
      "batch: 1560 loss: 0.016783496364951134\n",
      "batch: 1570 loss: 0.020140482112765312\n",
      "batch: 1580 loss: 0.015450446866452694\n",
      "batch: 1590 loss: 0.021747542545199394\n",
      "batch: 1600 loss: 0.01749255880713463\n",
      "batch: 1610 loss: 0.017346620559692383\n",
      "batch: 1620 loss: 0.023146726191043854\n",
      "batch: 1630 loss: 0.01917453669011593\n",
      "batch: 1640 loss: 0.022484567016363144\n",
      "batch: 1650 loss: 0.020687216892838478\n",
      "batch: 1660 loss: 0.016976797953248024\n",
      "batch: 1670 loss: 0.01723289303481579\n",
      "batch: 1680 loss: 0.01752229407429695\n",
      "batch: 1690 loss: 0.021003752946853638\n",
      "batch: 1700 loss: 0.021463101729750633\n",
      "batch: 1710 loss: 0.02021188475191593\n",
      "batch: 1720 loss: 0.021141299977898598\n",
      "batch: 1730 loss: 0.02373623661696911\n",
      "batch: 1740 loss: 0.022176558151841164\n",
      "batch: 1750 loss: 0.023544590920209885\n",
      "batch: 1760 loss: 0.015496056526899338\n",
      "batch: 1770 loss: 0.02090398594737053\n",
      "batch: 1780 loss: 0.021510371938347816\n",
      "batch: 1790 loss: 0.01998326927423477\n",
      "batch: 1800 loss: 0.020174643024802208\n",
      "batch: 1810 loss: 0.02060711197555065\n",
      "batch: 1820 loss: 0.021309636533260345\n",
      "batch: 1830 loss: 0.01893574185669422\n",
      "batch: 1840 loss: 0.018015500158071518\n",
      "batch: 1850 loss: 0.023427538573741913\n",
      "batch: 1860 loss: 0.0203241053968668\n",
      "batch: 1870 loss: 0.024452680721879005\n",
      "batch: 1880 loss: 0.01852104812860489\n",
      "batch: 1890 loss: 0.021416040137410164\n",
      "batch: 1900 loss: 0.02454649657011032\n",
      "batch: 1910 loss: 0.01960962451994419\n",
      "batch: 1920 loss: 0.01868065446615219\n",
      "batch: 1930 loss: 0.017483502626419067\n",
      "batch: 1940 loss: 0.01722654141485691\n",
      "batch: 1950 loss: 0.024867551401257515\n",
      "batch: 1960 loss: 0.02342541329562664\n",
      "batch: 1970 loss: 0.015324370935559273\n",
      "batch: 1980 loss: 0.019137397408485413\n",
      "batch: 1990 loss: 0.021318653598427773\n",
      "batch: 2000 loss: 0.016231365501880646\n",
      "batch: 2010 loss: 0.019186291843652725\n",
      "batch: 2020 loss: 0.021131983026862144\n",
      "batch: 2030 loss: 0.023259390145540237\n",
      "batch: 2040 loss: 0.018664130941033363\n",
      "batch: 2050 loss: 0.030991213396191597\n",
      "batch: 2060 loss: 0.022045539692044258\n",
      "batch: 2070 loss: 0.020637545734643936\n",
      "batch: 2080 loss: 0.018411049619317055\n",
      "batch: 2090 loss: 0.018537325784564018\n",
      "batch: 2100 loss: 0.020068850368261337\n",
      "batch: 2110 loss: 0.019761210307478905\n",
      "batch: 2120 loss: 0.020313389599323273\n",
      "batch: 2130 loss: 0.022497983649373055\n",
      "batch: 2140 loss: 0.02141685038805008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2150 loss: 0.02062714286148548\n",
      "batch: 2160 loss: 0.017144745215773582\n",
      "batch: 2170 loss: 0.021336037665605545\n",
      "batch: 2180 loss: 0.019026124849915504\n",
      "batch: 2190 loss: 0.01902630180120468\n",
      "batch: 2200 loss: 0.025691505521535873\n",
      "batch: 2210 loss: 0.025636127218604088\n",
      "batch: 2220 loss: 0.012323870323598385\n",
      "batch: 2230 loss: 0.022095564752817154\n",
      "batch: 2240 loss: 0.016900736838579178\n",
      "batch: 2250 loss: 0.019161097705364227\n",
      "batch: 2260 loss: 0.023974303156137466\n",
      "batch: 2270 loss: 0.015478750690817833\n",
      "batch: 2280 loss: 0.017383061349391937\n",
      "batch: 2290 loss: 0.023297693580389023\n",
      "batch: 2300 loss: 0.020034000277519226\n",
      "batch: 2310 loss: 0.018703125417232513\n",
      "batch: 2320 loss: 0.025641020387411118\n",
      "batch: 2330 loss: 0.017390485852956772\n",
      "batch: 2340 loss: 0.018997834995388985\n",
      "batch: 2350 loss: 0.019531603902578354\n",
      "batch: 2360 loss: 0.016995340585708618\n",
      "batch: 2370 loss: 0.023542456328868866\n",
      "batch: 2380 loss: 0.019034581258893013\n",
      "batch: 2390 loss: 0.019356386736035347\n",
      "batch: 2400 loss: 0.02319985069334507\n",
      "batch: 2410 loss: 0.021318703889846802\n",
      "batch: 2420 loss: 0.02257016859948635\n",
      "batch: 2430 loss: 0.026017403230071068\n",
      "batch: 2440 loss: 0.027176888659596443\n",
      "batch: 2450 loss: 0.02013416402041912\n",
      "batch: 2460 loss: 0.025244368240237236\n",
      "batch: 2470 loss: 0.022566400468349457\n",
      "batch: 2480 loss: 0.022861134260892868\n",
      "batch: 2490 loss: 0.020539885386824608\n",
      "batch: 2500 loss: 0.02102053537964821\n",
      "batch: 2510 loss: 0.01951237954199314\n",
      "batch: 2520 loss: 0.017828773707151413\n",
      "batch: 2530 loss: 0.02239258773624897\n",
      "batch: 2540 loss: 0.01501146424561739\n",
      "batch: 2550 loss: 0.01760934665799141\n",
      "batch: 2560 loss: 0.015245108865201473\n",
      "batch: 2570 loss: 0.022567186504602432\n",
      "batch: 2580 loss: 0.02019823156297207\n",
      "batch: 2590 loss: 0.024699773639440536\n",
      "batch: 2600 loss: 0.01917477324604988\n",
      "batch: 2610 loss: 0.018536007031798363\n",
      "batch: 2620 loss: 0.02093091420829296\n",
      "batch: 2630 loss: 0.02331564389169216\n",
      "batch: 2640 loss: 0.017903387546539307\n",
      "batch: 2650 loss: 0.021908562630414963\n",
      "batch: 2660 loss: 0.01732095144689083\n",
      "batch: 2670 loss: 0.016440169885754585\n",
      "batch: 2680 loss: 0.019788524135947227\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.018623553216457367\n",
      "batch: 10 loss: 0.019997810944914818\n",
      "batch: 20 loss: 0.018855402246117592\n",
      "batch: 30 loss: 0.02279535122215748\n",
      "batch: 40 loss: 0.018726052716374397\n",
      "batch: 50 loss: 0.018238648772239685\n",
      "batch: 60 loss: 0.016736119985580444\n",
      "batch: 70 loss: 0.02216937206685543\n",
      "batch: 80 loss: 0.022219672799110413\n",
      "batch: 90 loss: 0.016214832663536072\n",
      "batch: 100 loss: 0.019892359152436256\n",
      "batch: 110 loss: 0.01784660667181015\n",
      "batch: 120 loss: 0.019718023017048836\n",
      "batch: 130 loss: 0.018024737015366554\n",
      "batch: 140 loss: 0.017577892169356346\n",
      "batch: 150 loss: 0.022547677159309387\n",
      "batch: 160 loss: 0.022964198142290115\n",
      "batch: 170 loss: 0.02000207081437111\n",
      "batch: 180 loss: 0.015125862322747707\n",
      "batch: 190 loss: 0.021122440695762634\n",
      "batch: 200 loss: 0.02012421377003193\n",
      "batch: 210 loss: 0.01947290450334549\n",
      "batch: 220 loss: 0.021215707063674927\n",
      "batch: 230 loss: 0.01723286509513855\n",
      "batch: 240 loss: 0.020101098343729973\n",
      "batch: 250 loss: 0.017342064529657364\n",
      "batch: 260 loss: 0.02277352847158909\n",
      "batch: 270 loss: 0.017635857686400414\n",
      "batch: 280 loss: 0.021506961435079575\n",
      "batch: 290 loss: 0.017929933965206146\n",
      "batch: 300 loss: 0.01691734418272972\n",
      "batch: 310 loss: 0.016827194020152092\n",
      "batch: 320 loss: 0.020124485716223717\n",
      "batch: 330 loss: 0.02110476791858673\n",
      "batch: 340 loss: 0.020654741674661636\n",
      "batch: 350 loss: 0.019434724003076553\n",
      "batch: 360 loss: 0.01174671296030283\n",
      "batch: 370 loss: 0.021611718460917473\n",
      "batch: 380 loss: 0.024714374914765358\n",
      "batch: 390 loss: 0.019156893715262413\n",
      "batch: 400 loss: 0.022682294249534607\n",
      "batch: 410 loss: 0.02499436028301716\n",
      "batch: 420 loss: 0.023302560672163963\n",
      "batch: 430 loss: 0.018081059679389\n",
      "batch: 440 loss: 0.02630535699427128\n",
      "batch: 450 loss: 0.01866634376347065\n",
      "batch: 460 loss: 0.022463692352175713\n",
      "batch: 470 loss: 0.018434997648000717\n",
      "batch: 480 loss: 0.021676523610949516\n",
      "batch: 490 loss: 0.019367383792996407\n",
      "batch: 500 loss: 0.015943681821227074\n",
      "batch: 510 loss: 0.015221988782286644\n",
      "batch: 520 loss: 0.022155214101076126\n",
      "batch: 530 loss: 0.02000034786760807\n",
      "batch: 540 loss: 0.01559821143746376\n",
      "batch: 550 loss: 0.016168396919965744\n",
      "batch: 560 loss: 0.020265212282538414\n",
      "batch: 570 loss: 0.019425347447395325\n",
      "batch: 580 loss: 0.020127559080719948\n",
      "batch: 590 loss: 0.0215684212744236\n",
      "batch: 600 loss: 0.023503167554736137\n",
      "batch: 610 loss: 0.020722776651382446\n",
      "batch: 620 loss: 0.019180070608854294\n",
      "batch: 630 loss: 0.027414068579673767\n",
      "batch: 640 loss: 0.01685645431280136\n",
      "batch: 650 loss: 0.019581615924835205\n",
      "batch: 660 loss: 0.0172046460211277\n",
      "batch: 670 loss: 0.028526652604341507\n",
      "batch: 680 loss: 0.02403084747493267\n",
      "batch: 690 loss: 0.018228385597467422\n",
      "batch: 700 loss: 0.019565524533391\n",
      "batch: 710 loss: 0.016789179295301437\n",
      "batch: 720 loss: 0.021808655932545662\n",
      "batch: 730 loss: 0.016743039712309837\n",
      "batch: 740 loss: 0.014522424899041653\n",
      "batch: 750 loss: 0.017525454983115196\n",
      "batch: 760 loss: 0.022213242948055267\n",
      "batch: 770 loss: 0.02255162037909031\n",
      "batch: 780 loss: 0.018193747848272324\n",
      "batch: 790 loss: 0.01834767498075962\n",
      "batch: 800 loss: 0.023638423532247543\n",
      "batch: 810 loss: 0.020949214696884155\n",
      "batch: 820 loss: 0.01997939869761467\n",
      "batch: 830 loss: 0.02610074356198311\n",
      "batch: 840 loss: 0.019596539437770844\n",
      "batch: 850 loss: 0.01661866344511509\n",
      "batch: 860 loss: 0.02296893671154976\n",
      "batch: 870 loss: 0.01750686764717102\n",
      "batch: 880 loss: 0.017085472121834755\n",
      "batch: 890 loss: 0.020680947229266167\n",
      "batch: 900 loss: 0.023139024153351784\n",
      "batch: 910 loss: 0.02057064324617386\n",
      "batch: 920 loss: 0.02031836099922657\n",
      "batch: 930 loss: 0.02256069704890251\n",
      "batch: 940 loss: 0.023401707410812378\n",
      "batch: 950 loss: 0.02223522588610649\n",
      "batch: 960 loss: 0.017904100939631462\n",
      "batch: 970 loss: 0.017978092655539513\n",
      "batch: 980 loss: 0.023035617545247078\n",
      "batch: 990 loss: 0.02152174524962902\n",
      "batch: 1000 loss: 0.017322877421975136\n",
      "batch: 1010 loss: 0.020223382860422134\n",
      "batch: 1020 loss: 0.023580897599458694\n",
      "batch: 1030 loss: 0.01657790131866932\n",
      "batch: 1040 loss: 0.020863741636276245\n",
      "batch: 1050 loss: 0.02073068358004093\n",
      "batch: 1060 loss: 0.019857412204146385\n",
      "batch: 1070 loss: 0.02190219610929489\n",
      "batch: 1080 loss: 0.02510066330432892\n",
      "batch: 1090 loss: 0.020146803930401802\n",
      "batch: 1100 loss: 0.020137321203947067\n",
      "batch: 1110 loss: 0.021864494308829308\n",
      "batch: 1120 loss: 0.025642262771725655\n",
      "batch: 1130 loss: 0.022964855656027794\n",
      "batch: 1140 loss: 0.020510831847786903\n",
      "batch: 1150 loss: 0.017673853784799576\n",
      "batch: 1160 loss: 0.02099580504000187\n",
      "batch: 1170 loss: 0.021108411252498627\n",
      "batch: 1180 loss: 0.017785515636205673\n",
      "batch: 1190 loss: 0.019206462427973747\n",
      "batch: 1200 loss: 0.02216380462050438\n",
      "batch: 1210 loss: 0.021435262635350227\n",
      "batch: 1220 loss: 0.015401583164930344\n",
      "batch: 1230 loss: 0.01827097311615944\n",
      "batch: 1240 loss: 0.01792021654546261\n",
      "batch: 1250 loss: 0.017447639256715775\n",
      "batch: 1260 loss: 0.02120549976825714\n",
      "batch: 1270 loss: 0.021539516746997833\n",
      "batch: 1280 loss: 0.0178173016756773\n",
      "batch: 1290 loss: 0.01666378416121006\n",
      "batch: 1300 loss: 0.02319587767124176\n",
      "batch: 1310 loss: 0.02055586501955986\n",
      "batch: 1320 loss: 0.0193256177008152\n",
      "batch: 1330 loss: 0.020046819001436234\n",
      "batch: 1340 loss: 0.015595951117575169\n",
      "batch: 1350 loss: 0.025508545339107513\n",
      "batch: 1360 loss: 0.018178343772888184\n",
      "batch: 1370 loss: 0.020281951874494553\n",
      "batch: 1380 loss: 0.016680780798196793\n",
      "batch: 1390 loss: 0.01754421927034855\n",
      "batch: 1400 loss: 0.01926676370203495\n",
      "batch: 1410 loss: 0.01654694601893425\n",
      "batch: 1420 loss: 0.01888311095535755\n",
      "batch: 1430 loss: 0.016438676044344902\n",
      "batch: 1440 loss: 0.013003811240196228\n",
      "batch: 1450 loss: 0.014880865812301636\n",
      "batch: 1460 loss: 0.016039544716477394\n",
      "batch: 1470 loss: 0.018092148005962372\n",
      "batch: 1480 loss: 0.021587951108813286\n",
      "batch: 1490 loss: 0.01788557879626751\n",
      "batch: 1500 loss: 0.02344651333987713\n",
      "batch: 1510 loss: 0.01595214195549488\n",
      "batch: 1520 loss: 0.017716115340590477\n",
      "batch: 1530 loss: 0.017982713878154755\n",
      "batch: 1540 loss: 0.021411819383502007\n",
      "batch: 1550 loss: 0.02373882196843624\n",
      "batch: 1560 loss: 0.02237558364868164\n",
      "batch: 1570 loss: 0.02313767746090889\n",
      "batch: 1580 loss: 0.02054211124777794\n",
      "batch: 1590 loss: 0.021885601803660393\n",
      "batch: 1600 loss: 0.019049881026148796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1610 loss: 0.02010808326303959\n",
      "batch: 1620 loss: 0.021749062463641167\n",
      "batch: 1630 loss: 0.01999962143599987\n",
      "batch: 1640 loss: 0.023610655218362808\n",
      "batch: 1650 loss: 0.01945221610367298\n",
      "batch: 1660 loss: 0.021760011091828346\n",
      "batch: 1670 loss: 0.01956399902701378\n",
      "batch: 1680 loss: 0.020951135084033012\n",
      "batch: 1690 loss: 0.01937592215836048\n",
      "batch: 1700 loss: 0.02032287046313286\n",
      "batch: 1710 loss: 0.0181594155728817\n",
      "batch: 1720 loss: 0.017596200108528137\n",
      "batch: 1730 loss: 0.02545509673655033\n",
      "batch: 1740 loss: 0.02294924296438694\n",
      "batch: 1750 loss: 0.019956538453698158\n",
      "batch: 1760 loss: 0.02237355336546898\n",
      "batch: 1770 loss: 0.021895918995141983\n",
      "batch: 1780 loss: 0.02090413309633732\n",
      "batch: 1790 loss: 0.020275134593248367\n",
      "batch: 1800 loss: 0.020546386018395424\n",
      "batch: 1810 loss: 0.01665913499891758\n",
      "batch: 1820 loss: 0.019289953634142876\n",
      "batch: 1830 loss: 0.016989855095744133\n",
      "batch: 1840 loss: 0.021184498444199562\n",
      "batch: 1850 loss: 0.019773172214627266\n",
      "batch: 1860 loss: 0.019011007621884346\n",
      "batch: 1870 loss: 0.015452614985406399\n",
      "batch: 1880 loss: 0.020532915368676186\n",
      "batch: 1890 loss: 0.018084237352013588\n",
      "batch: 1900 loss: 0.02138916403055191\n",
      "batch: 1910 loss: 0.024013623595237732\n",
      "batch: 1920 loss: 0.01842167228460312\n",
      "batch: 1930 loss: 0.017000583931803703\n",
      "batch: 1940 loss: 0.01590702123939991\n",
      "batch: 1950 loss: 0.020174728706479073\n",
      "batch: 1960 loss: 0.0144065972417593\n",
      "batch: 1970 loss: 0.020003288984298706\n",
      "batch: 1980 loss: 0.019101005047559738\n",
      "batch: 1990 loss: 0.01679481565952301\n",
      "batch: 2000 loss: 0.020504213869571686\n",
      "batch: 2010 loss: 0.017452266067266464\n",
      "batch: 2020 loss: 0.018945764750242233\n",
      "batch: 2030 loss: 0.026491891592741013\n",
      "batch: 2040 loss: 0.017700929194688797\n",
      "batch: 2050 loss: 0.02106548845767975\n",
      "batch: 2060 loss: 0.022147638723254204\n",
      "batch: 2070 loss: 0.01952274702489376\n",
      "batch: 2080 loss: 0.019038619473576546\n",
      "batch: 2090 loss: 0.01832018606364727\n",
      "batch: 2100 loss: 0.016529060900211334\n",
      "batch: 2110 loss: 0.018805885687470436\n",
      "batch: 2120 loss: 0.01643463969230652\n",
      "batch: 2130 loss: 0.017683571204543114\n",
      "batch: 2140 loss: 0.02227281965315342\n",
      "batch: 2150 loss: 0.021224787458777428\n",
      "batch: 2160 loss: 0.016580786556005478\n",
      "batch: 2170 loss: 0.0221379604190588\n",
      "batch: 2180 loss: 0.020413821563124657\n",
      "batch: 2190 loss: 0.023704228922724724\n",
      "batch: 2200 loss: 0.020435521379113197\n",
      "batch: 2210 loss: 0.026507550850510597\n",
      "batch: 2220 loss: 0.019185921177268028\n",
      "batch: 2230 loss: 0.019504768773913383\n",
      "batch: 2240 loss: 0.02264525182545185\n",
      "batch: 2250 loss: 0.01614219881594181\n",
      "batch: 2260 loss: 0.018020151183009148\n",
      "batch: 2270 loss: 0.021228548139333725\n",
      "batch: 2280 loss: 0.01537358108907938\n",
      "batch: 2290 loss: 0.022472290322184563\n",
      "batch: 2300 loss: 0.018269428983330727\n",
      "batch: 2310 loss: 0.01114582922309637\n",
      "batch: 2320 loss: 0.02249542810022831\n",
      "batch: 2330 loss: 0.020805688574910164\n",
      "batch: 2340 loss: 0.01851048320531845\n",
      "batch: 2350 loss: 0.017321864143013954\n",
      "batch: 2360 loss: 0.0194918904453516\n",
      "batch: 2370 loss: 0.017888767644762993\n",
      "batch: 2380 loss: 0.023680342361330986\n",
      "batch: 2390 loss: 0.016475951299071312\n",
      "batch: 2400 loss: 0.022126907482743263\n",
      "batch: 2410 loss: 0.0180937759578228\n",
      "batch: 2420 loss: 0.021486572921276093\n",
      "batch: 2430 loss: 0.01792335696518421\n",
      "batch: 2440 loss: 0.020856423303484917\n",
      "batch: 2450 loss: 0.018419986590743065\n",
      "batch: 2460 loss: 0.014567049220204353\n",
      "batch: 2470 loss: 0.01768573373556137\n",
      "batch: 2480 loss: 0.018460312858223915\n",
      "batch: 2490 loss: 0.016449837014079094\n",
      "batch: 2500 loss: 0.021768979728221893\n",
      "batch: 2510 loss: 0.019869588315486908\n",
      "batch: 2520 loss: 0.01803399622440338\n",
      "batch: 2530 loss: 0.022222895175218582\n",
      "batch: 2540 loss: 0.018981993198394775\n",
      "batch: 2550 loss: 0.018636131659150124\n",
      "batch: 2560 loss: 0.02114265225827694\n",
      "batch: 2570 loss: 0.02021791785955429\n",
      "batch: 2580 loss: 0.017422350123524666\n",
      "batch: 2590 loss: 0.020625293254852295\n",
      "batch: 2600 loss: 0.014707253314554691\n",
      "batch: 2610 loss: 0.012474782764911652\n",
      "batch: 2620 loss: 0.018006913363933563\n",
      "batch: 2630 loss: 0.021119236946105957\n",
      "batch: 2640 loss: 0.023951688781380653\n",
      "batch: 2650 loss: 0.018074188381433487\n",
      "batch: 2660 loss: 0.020787402987480164\n",
      "batch: 2670 loss: 0.019434235990047455\n",
      "batch: 2680 loss: 0.021047329530119896\n",
      "round  1 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.01926867477595806\n",
      "batch: 10 loss: 0.0192500539124012\n",
      "batch: 20 loss: 0.015989001840353012\n",
      "batch: 30 loss: 0.017952265217900276\n",
      "batch: 40 loss: 0.01869686134159565\n",
      "batch: 50 loss: 0.017515169456601143\n",
      "batch: 60 loss: 0.016864120960235596\n",
      "batch: 70 loss: 0.017916332930326462\n",
      "batch: 80 loss: 0.0171322301030159\n",
      "batch: 90 loss: 0.01992090232670307\n",
      "batch: 100 loss: 0.018718533217906952\n",
      "batch: 110 loss: 0.019006773829460144\n",
      "batch: 120 loss: 0.021427446976304054\n",
      "batch: 130 loss: 0.013318434357643127\n",
      "batch: 140 loss: 0.021054981276392937\n",
      "batch: 150 loss: 0.017912261188030243\n",
      "batch: 160 loss: 0.019269270822405815\n",
      "batch: 170 loss: 0.018036456778645515\n",
      "batch: 180 loss: 0.02200806885957718\n",
      "batch: 190 loss: 0.0174418892711401\n",
      "batch: 200 loss: 0.019252752885222435\n",
      "batch: 210 loss: 0.01824621483683586\n",
      "batch: 220 loss: 0.01747351512312889\n",
      "batch: 230 loss: 0.01660994626581669\n",
      "batch: 240 loss: 0.020012160763144493\n",
      "batch: 250 loss: 0.01681448519229889\n",
      "batch: 260 loss: 0.017357053235173225\n",
      "batch: 270 loss: 0.021874340251088142\n",
      "batch: 280 loss: 0.01933421939611435\n",
      "batch: 290 loss: 0.02294783852994442\n",
      "batch: 300 loss: 0.01752639375627041\n",
      "batch: 310 loss: 0.019000887870788574\n",
      "batch: 320 loss: 0.017657525837421417\n",
      "batch: 330 loss: 0.01637418568134308\n",
      "batch: 340 loss: 0.022322723641991615\n",
      "batch: 350 loss: 0.01640947349369526\n",
      "batch: 360 loss: 0.014286929741501808\n",
      "batch: 370 loss: 0.017448466271162033\n",
      "batch: 380 loss: 0.015847889706492424\n",
      "batch: 390 loss: 0.01921875961124897\n",
      "batch: 400 loss: 0.016826437786221504\n",
      "batch: 410 loss: 0.0146297262981534\n",
      "batch: 420 loss: 0.019383734092116356\n",
      "batch: 430 loss: 0.01620311848819256\n",
      "batch: 440 loss: 0.020541341975331306\n",
      "batch: 450 loss: 0.02104358747601509\n",
      "batch: 460 loss: 0.014460355043411255\n",
      "batch: 470 loss: 0.02022973820567131\n",
      "batch: 480 loss: 0.02004515379667282\n",
      "batch: 490 loss: 0.01810707338154316\n",
      "batch: 500 loss: 0.01643320918083191\n",
      "batch: 510 loss: 0.019496846944093704\n",
      "batch: 520 loss: 0.018884286284446716\n",
      "batch: 530 loss: 0.01582241803407669\n",
      "batch: 540 loss: 0.018211539834737778\n",
      "batch: 550 loss: 0.017824169248342514\n",
      "batch: 560 loss: 0.02028317004442215\n",
      "batch: 570 loss: 0.020750978961586952\n",
      "batch: 580 loss: 0.016582351177930832\n",
      "batch: 590 loss: 0.015699436888098717\n",
      "batch: 600 loss: 0.018798206001520157\n",
      "batch: 610 loss: 0.01906026341021061\n",
      "batch: 620 loss: 0.016034137457609177\n",
      "batch: 630 loss: 0.018129566684365273\n",
      "batch: 640 loss: 0.01919599436223507\n",
      "batch: 650 loss: 0.0163847915828228\n",
      "batch: 660 loss: 0.018410922959446907\n",
      "batch: 670 loss: 0.01705745980143547\n",
      "batch: 680 loss: 0.01590464450418949\n",
      "batch: 690 loss: 0.01840370148420334\n",
      "batch: 700 loss: 0.022957179695367813\n",
      "batch: 710 loss: 0.016146205365657806\n",
      "batch: 720 loss: 0.019943170249462128\n",
      "batch: 730 loss: 0.017939666286110878\n",
      "batch: 740 loss: 0.017690259963274002\n",
      "batch: 750 loss: 0.016830775886774063\n",
      "batch: 760 loss: 0.01650155521929264\n",
      "batch: 770 loss: 0.015774469822645187\n",
      "batch: 780 loss: 0.018004076555371284\n",
      "batch: 790 loss: 0.01766309328377247\n",
      "batch: 800 loss: 0.013371826149523258\n",
      "batch: 810 loss: 0.020654678344726562\n",
      "batch: 820 loss: 0.020204883068799973\n",
      "batch: 830 loss: 0.01775987818837166\n",
      "batch: 840 loss: 0.019711364060640335\n",
      "batch: 850 loss: 0.018009204417467117\n",
      "batch: 860 loss: 0.0204634927213192\n",
      "batch: 870 loss: 0.01681475155055523\n",
      "batch: 880 loss: 0.016475128009915352\n",
      "batch: 890 loss: 0.015764888375997543\n",
      "batch: 900 loss: 0.015947142615914345\n",
      "batch: 910 loss: 0.017697108909487724\n",
      "batch: 920 loss: 0.014261667616665363\n",
      "batch: 930 loss: 0.0171220563352108\n",
      "batch: 940 loss: 0.015163339674472809\n",
      "batch: 950 loss: 0.020364321768283844\n",
      "batch: 960 loss: 0.0180378220975399\n",
      "batch: 970 loss: 0.01522546075284481\n",
      "batch: 980 loss: 0.017041247338056564\n",
      "batch: 990 loss: 0.020022444427013397\n",
      "batch: 1000 loss: 0.020475666970014572\n",
      "batch: 1010 loss: 0.02006390132009983\n",
      "batch: 1020 loss: 0.015495582483708858\n",
      "batch: 1030 loss: 0.018180707469582558\n",
      "batch: 1040 loss: 0.018645450472831726\n",
      "batch: 1050 loss: 0.016816038638353348\n",
      "batch: 1060 loss: 0.02012486197054386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1070 loss: 0.017145223915576935\n",
      "batch: 1080 loss: 0.020332111045718193\n",
      "batch: 1090 loss: 0.014786915853619576\n",
      "batch: 1100 loss: 0.01942404732108116\n",
      "batch: 1110 loss: 0.018662037327885628\n",
      "batch: 1120 loss: 0.022600334137678146\n",
      "batch: 1130 loss: 0.020059388130903244\n",
      "batch: 1140 loss: 0.016991976648569107\n",
      "batch: 1150 loss: 0.020462380722165108\n",
      "batch: 1160 loss: 0.014008656144142151\n",
      "batch: 1170 loss: 0.01642315275967121\n",
      "batch: 1180 loss: 0.01915500871837139\n",
      "batch: 1190 loss: 0.015846028923988342\n",
      "batch: 1200 loss: 0.018897661939263344\n",
      "batch: 1210 loss: 0.015252306126058102\n",
      "batch: 1220 loss: 0.019546713680028915\n",
      "batch: 1230 loss: 0.017131343483924866\n",
      "batch: 1240 loss: 0.016526244580745697\n",
      "batch: 1250 loss: 0.022521253675222397\n",
      "batch: 1260 loss: 0.017908576875925064\n",
      "batch: 1270 loss: 0.01802179217338562\n",
      "batch: 1280 loss: 0.01959466189146042\n",
      "batch: 1290 loss: 0.0180820319801569\n",
      "batch: 1300 loss: 0.01567254588007927\n",
      "batch: 1310 loss: 0.01770060323178768\n",
      "batch: 1320 loss: 0.019210508093237877\n",
      "batch: 1330 loss: 0.015903955325484276\n",
      "batch: 1340 loss: 0.01827239990234375\n",
      "batch: 1350 loss: 0.018537331372499466\n",
      "batch: 1360 loss: 0.021484481170773506\n",
      "batch: 1370 loss: 0.014523372985422611\n",
      "batch: 1380 loss: 0.021914297714829445\n",
      "batch: 1390 loss: 0.019504182040691376\n",
      "batch: 1400 loss: 0.0170980766415596\n",
      "batch: 1410 loss: 0.01663913019001484\n",
      "batch: 1420 loss: 0.017392536625266075\n",
      "batch: 1430 loss: 0.020216558128595352\n",
      "batch: 1440 loss: 0.021101143211126328\n",
      "batch: 1450 loss: 0.016372477635741234\n",
      "batch: 1460 loss: 0.013745185919106007\n",
      "batch: 1470 loss: 0.01806006208062172\n",
      "batch: 1480 loss: 0.01605871506035328\n",
      "batch: 1490 loss: 0.01772150956094265\n",
      "batch: 1500 loss: 0.021931780502200127\n",
      "batch: 1510 loss: 0.01873571053147316\n",
      "batch: 1520 loss: 0.017281586304306984\n",
      "batch: 1530 loss: 0.017201349139213562\n",
      "batch: 1540 loss: 0.017379099503159523\n",
      "batch: 1550 loss: 0.018053166568279266\n",
      "batch: 1560 loss: 0.020958971232175827\n",
      "batch: 1570 loss: 0.020140809938311577\n",
      "batch: 1580 loss: 0.01653614081442356\n",
      "batch: 1590 loss: 0.019724823534488678\n",
      "batch: 1600 loss: 0.017566395923495293\n",
      "batch: 1610 loss: 0.01878262124955654\n",
      "batch: 1620 loss: 0.019216837361454964\n",
      "batch: 1630 loss: 0.014970722608268261\n",
      "batch: 1640 loss: 0.016462408006191254\n",
      "batch: 1650 loss: 0.014303181320428848\n",
      "batch: 1660 loss: 0.01888008788228035\n",
      "batch: 1670 loss: 0.020157720893621445\n",
      "batch: 1680 loss: 0.014985279180109501\n",
      "batch: 1690 loss: 0.01745305210351944\n",
      "batch: 1700 loss: 0.018488166853785515\n",
      "batch: 1710 loss: 0.01864462159574032\n",
      "batch: 1720 loss: 0.014556623063981533\n",
      "batch: 1730 loss: 0.02243710868060589\n",
      "batch: 1740 loss: 0.019943762570619583\n",
      "batch: 1750 loss: 0.018795127049088478\n",
      "batch: 1760 loss: 0.020450901240110397\n",
      "batch: 1770 loss: 0.020486172288656235\n",
      "batch: 1780 loss: 0.015048624016344547\n",
      "batch: 1790 loss: 0.01782688871026039\n",
      "batch: 1800 loss: 0.01851428672671318\n",
      "batch: 1810 loss: 0.01468201819807291\n",
      "batch: 1820 loss: 0.016389761120080948\n",
      "batch: 1830 loss: 0.01589098572731018\n",
      "batch: 1840 loss: 0.01779528334736824\n",
      "batch: 1850 loss: 0.017204472795128822\n",
      "batch: 1860 loss: 0.01980619505047798\n",
      "batch: 1870 loss: 0.014373588375747204\n",
      "batch: 1880 loss: 0.01576373353600502\n",
      "batch: 1890 loss: 0.017645282670855522\n",
      "batch: 1900 loss: 0.016056811437010765\n",
      "batch: 1910 loss: 0.019833959639072418\n",
      "batch: 1920 loss: 0.01602907106280327\n",
      "batch: 1930 loss: 0.021576838567852974\n",
      "batch: 1940 loss: 0.01806878298521042\n",
      "batch: 1950 loss: 0.019002549350261688\n",
      "batch: 1960 loss: 0.01588553749024868\n",
      "batch: 1970 loss: 0.016484227031469345\n",
      "batch: 1980 loss: 0.015031103044748306\n",
      "batch: 1990 loss: 0.02019915170967579\n",
      "batch: 2000 loss: 0.018594034016132355\n",
      "batch: 2010 loss: 0.013369803316891193\n",
      "batch: 2020 loss: 0.0176494549959898\n",
      "batch: 2030 loss: 0.02071635238826275\n",
      "batch: 2040 loss: 0.01926082745194435\n",
      "batch: 2050 loss: 0.014149761758744717\n",
      "batch: 2060 loss: 0.013816017657518387\n",
      "batch: 2070 loss: 0.01652873307466507\n",
      "batch: 2080 loss: 0.017557602375745773\n",
      "batch: 2090 loss: 0.017540909349918365\n",
      "batch: 2100 loss: 0.020332032814621925\n",
      "batch: 2110 loss: 0.0175361055880785\n",
      "batch: 2120 loss: 0.014881816692650318\n",
      "batch: 2130 loss: 0.01697254553437233\n",
      "batch: 2140 loss: 0.016247794032096863\n",
      "batch: 2150 loss: 0.016900774091482162\n",
      "batch: 2160 loss: 0.013714608736336231\n",
      "batch: 2170 loss: 0.01848551630973816\n",
      "batch: 2180 loss: 0.020510802045464516\n",
      "batch: 2190 loss: 0.020376775413751602\n",
      "batch: 2200 loss: 0.0170227512717247\n",
      "batch: 2210 loss: 0.015855073928833008\n",
      "batch: 2220 loss: 0.018621724098920822\n",
      "batch: 2230 loss: 0.013099940493702888\n",
      "batch: 2240 loss: 0.01672474667429924\n",
      "batch: 2250 loss: 0.02168876677751541\n",
      "batch: 2260 loss: 0.018275918439030647\n",
      "batch: 2270 loss: 0.019194576889276505\n",
      "batch: 2280 loss: 0.016132129356265068\n",
      "batch: 2290 loss: 0.01662183180451393\n",
      "batch: 2300 loss: 0.01602393016219139\n",
      "batch: 2310 loss: 0.018383877351880074\n",
      "batch: 2320 loss: 0.01754196174442768\n",
      "batch: 2330 loss: 0.013938129879534245\n",
      "batch: 2340 loss: 0.016222601756453514\n",
      "batch: 2350 loss: 0.01471372414380312\n",
      "batch: 2360 loss: 0.017934132367372513\n",
      "batch: 2370 loss: 0.017190702259540558\n",
      "batch: 2380 loss: 0.01567930541932583\n",
      "batch: 2390 loss: 0.020890451967716217\n",
      "batch: 2400 loss: 0.01827307604253292\n",
      "batch: 2410 loss: 0.018942974507808685\n",
      "batch: 2420 loss: 0.015598157420754433\n",
      "batch: 2430 loss: 0.01704852283000946\n",
      "batch: 2440 loss: 0.01614658161997795\n",
      "batch: 2450 loss: 0.01568949595093727\n",
      "batch: 2460 loss: 0.016959214583039284\n",
      "batch: 2470 loss: 0.016852818429470062\n",
      "batch: 2480 loss: 0.017453858628869057\n",
      "batch: 2490 loss: 0.019590452313423157\n",
      "batch: 2500 loss: 0.019867990165948868\n",
      "batch: 2510 loss: 0.017661772668361664\n",
      "batch: 2520 loss: 0.018230777233839035\n",
      "batch: 2530 loss: 0.018204988911747932\n",
      "batch: 2540 loss: 0.017676249146461487\n",
      "batch: 2550 loss: 0.01883474551141262\n",
      "batch: 2560 loss: 0.0158520694822073\n",
      "batch: 2570 loss: 0.022631490603089333\n",
      "batch: 2580 loss: 0.017633961513638496\n",
      "batch: 2590 loss: 0.019929004833102226\n",
      "batch: 2600 loss: 0.02203265018761158\n",
      "batch: 2610 loss: 0.017189553007483482\n",
      "batch: 2620 loss: 0.018402839079499245\n",
      "batch: 2630 loss: 0.019555142149329185\n",
      "batch: 2640 loss: 0.01752086915075779\n",
      "batch: 2650 loss: 0.013333350419998169\n",
      "batch: 2660 loss: 0.018392477184534073\n",
      "batch: 2670 loss: 0.01887771487236023\n",
      "batch: 2680 loss: 0.01643107272684574\n",
      "batch: 2690 loss: 0.02056937851011753\n",
      "batch: 2700 loss: 0.01712837629020214\n",
      "batch: 2710 loss: 0.021106353029608727\n",
      "batch: 2720 loss: 0.01832551695406437\n",
      "batch: 2730 loss: 0.016290977597236633\n",
      "batch: 2740 loss: 0.01635482721030712\n",
      "batch: 2750 loss: 0.01906791515648365\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.019134117290377617\n",
      "batch: 10 loss: 0.015928659588098526\n",
      "batch: 20 loss: 0.01948469690978527\n",
      "batch: 30 loss: 0.017764665186405182\n",
      "batch: 40 loss: 0.019985439255833626\n",
      "batch: 50 loss: 0.019140474498271942\n",
      "batch: 60 loss: 0.02016117051243782\n",
      "batch: 70 loss: 0.015945734456181526\n",
      "batch: 80 loss: 0.013625135645270348\n",
      "batch: 90 loss: 0.019702741876244545\n",
      "batch: 100 loss: 0.016542920842766762\n",
      "batch: 110 loss: 0.01619240641593933\n",
      "batch: 120 loss: 0.01883862167596817\n",
      "batch: 130 loss: 0.016499314457178116\n",
      "batch: 140 loss: 0.02183307707309723\n",
      "batch: 150 loss: 0.016753986477851868\n",
      "batch: 160 loss: 0.01575787179172039\n",
      "batch: 170 loss: 0.018602270632982254\n",
      "batch: 180 loss: 0.02078971453011036\n",
      "batch: 190 loss: 0.016786357387900352\n",
      "batch: 200 loss: 0.017008554190397263\n",
      "batch: 210 loss: 0.017786579206585884\n",
      "batch: 220 loss: 0.01733499765396118\n",
      "batch: 230 loss: 0.01920846477150917\n",
      "batch: 240 loss: 0.01806461252272129\n",
      "batch: 250 loss: 0.015708226710557938\n",
      "batch: 260 loss: 0.020736167207360268\n",
      "batch: 270 loss: 0.015939578413963318\n",
      "batch: 280 loss: 0.018299411982297897\n",
      "batch: 290 loss: 0.01738559827208519\n",
      "batch: 300 loss: 0.015747245401144028\n",
      "batch: 310 loss: 0.017066754400730133\n",
      "batch: 320 loss: 0.014749306254088879\n",
      "batch: 330 loss: 0.019916405901312828\n",
      "batch: 340 loss: 0.01981242001056671\n",
      "batch: 350 loss: 0.017976652830839157\n",
      "batch: 360 loss: 0.014989265240728855\n",
      "batch: 370 loss: 0.01766376569867134\n",
      "batch: 380 loss: 0.016635645180940628\n",
      "batch: 390 loss: 0.01875682733952999\n",
      "batch: 400 loss: 0.019990187138319016\n",
      "batch: 410 loss: 0.015450531616806984\n",
      "batch: 420 loss: 0.016226809471845627\n",
      "batch: 430 loss: 0.016977814957499504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 440 loss: 0.01592327654361725\n",
      "batch: 450 loss: 0.01757708005607128\n",
      "batch: 460 loss: 0.019756032153964043\n",
      "batch: 470 loss: 0.019672228023409843\n",
      "batch: 480 loss: 0.017948783934116364\n",
      "batch: 490 loss: 0.01724146492779255\n",
      "batch: 500 loss: 0.01576320081949234\n",
      "batch: 510 loss: 0.016779249534010887\n",
      "batch: 520 loss: 0.018087081611156464\n",
      "batch: 530 loss: 0.015200017020106316\n",
      "batch: 540 loss: 0.015933049842715263\n",
      "batch: 550 loss: 0.017530400305986404\n",
      "batch: 560 loss: 0.01652413047850132\n",
      "batch: 570 loss: 0.02186020463705063\n",
      "batch: 580 loss: 0.014795409515500069\n",
      "batch: 590 loss: 0.0175763126462698\n",
      "batch: 600 loss: 0.015682488679885864\n",
      "batch: 610 loss: 0.016776850447058678\n",
      "batch: 620 loss: 0.016003785654902458\n",
      "batch: 630 loss: 0.01613558642566204\n",
      "batch: 640 loss: 0.0182010717689991\n",
      "batch: 650 loss: 0.019280286505818367\n",
      "batch: 660 loss: 0.015289416536688805\n",
      "batch: 670 loss: 0.02005935087800026\n",
      "batch: 680 loss: 0.01935667172074318\n",
      "batch: 690 loss: 0.021159209311008453\n",
      "batch: 700 loss: 0.018674448132514954\n",
      "batch: 710 loss: 0.013588802888989449\n",
      "batch: 720 loss: 0.016451403498649597\n",
      "batch: 730 loss: 0.012627641670405865\n",
      "batch: 740 loss: 0.014837921597063541\n",
      "batch: 750 loss: 0.018257716670632362\n",
      "batch: 760 loss: 0.021388418972492218\n",
      "batch: 770 loss: 0.019206663593649864\n",
      "batch: 780 loss: 0.01787904091179371\n",
      "batch: 790 loss: 0.021129516884684563\n",
      "batch: 800 loss: 0.01705343648791313\n",
      "batch: 810 loss: 0.018436642363667488\n",
      "batch: 820 loss: 0.015613231807947159\n",
      "batch: 830 loss: 0.014553199522197247\n",
      "batch: 840 loss: 0.01465869601815939\n",
      "batch: 850 loss: 0.01830880716443062\n",
      "batch: 860 loss: 0.016990402713418007\n",
      "batch: 870 loss: 0.01610628142952919\n",
      "batch: 880 loss: 0.014776855707168579\n",
      "batch: 890 loss: 0.01710173860192299\n",
      "batch: 900 loss: 0.018654469400644302\n",
      "batch: 910 loss: 0.018381984904408455\n",
      "batch: 920 loss: 0.015397313050925732\n",
      "batch: 930 loss: 0.019303735345602036\n",
      "batch: 940 loss: 0.019074048846960068\n",
      "batch: 950 loss: 0.01908879354596138\n",
      "batch: 960 loss: 0.016419632360339165\n",
      "batch: 970 loss: 0.018917115405201912\n",
      "batch: 980 loss: 0.017805024981498718\n",
      "batch: 990 loss: 0.01853979006409645\n",
      "batch: 1000 loss: 0.018128566443920135\n",
      "batch: 1010 loss: 0.01593608409166336\n",
      "batch: 1020 loss: 0.019686885178089142\n",
      "batch: 1030 loss: 0.017162175849080086\n",
      "batch: 1040 loss: 0.020227763801813126\n",
      "batch: 1050 loss: 0.019419154152274132\n",
      "batch: 1060 loss: 0.017282424494624138\n",
      "batch: 1070 loss: 0.014988729730248451\n",
      "batch: 1080 loss: 0.02261863648891449\n",
      "batch: 1090 loss: 0.02162020280957222\n",
      "batch: 1100 loss: 0.02074061892926693\n",
      "batch: 1110 loss: 0.01540970429778099\n",
      "batch: 1120 loss: 0.017119521275162697\n",
      "batch: 1130 loss: 0.016680113971233368\n",
      "batch: 1140 loss: 0.02141994796693325\n",
      "batch: 1150 loss: 0.020961174741387367\n",
      "batch: 1160 loss: 0.012926746159791946\n",
      "batch: 1170 loss: 0.018359770998358727\n",
      "batch: 1180 loss: 0.017899245023727417\n",
      "batch: 1190 loss: 0.01740213856101036\n",
      "batch: 1200 loss: 0.02059337869286537\n",
      "batch: 1210 loss: 0.02001781389117241\n",
      "batch: 1220 loss: 0.01822592318058014\n",
      "batch: 1230 loss: 0.012344296090304852\n",
      "batch: 1240 loss: 0.015767809003591537\n",
      "batch: 1250 loss: 0.013701767660677433\n",
      "batch: 1260 loss: 0.016560830175876617\n",
      "batch: 1270 loss: 0.01589617133140564\n",
      "batch: 1280 loss: 0.019148118793964386\n",
      "batch: 1290 loss: 0.017304811626672745\n",
      "batch: 1300 loss: 0.01687905564904213\n",
      "batch: 1310 loss: 0.020407801494002342\n",
      "batch: 1320 loss: 0.0164822768419981\n",
      "batch: 1330 loss: 0.019091380760073662\n",
      "batch: 1340 loss: 0.01836099475622177\n",
      "batch: 1350 loss: 0.023962173610925674\n",
      "batch: 1360 loss: 0.013045125640928745\n",
      "batch: 1370 loss: 0.018285267055034637\n",
      "batch: 1380 loss: 0.019816040992736816\n",
      "batch: 1390 loss: 0.019200928509235382\n",
      "batch: 1400 loss: 0.01661389321088791\n",
      "batch: 1410 loss: 0.01729648746550083\n",
      "batch: 1420 loss: 0.021326584741473198\n",
      "batch: 1430 loss: 0.01872449740767479\n",
      "batch: 1440 loss: 0.017403479665517807\n",
      "batch: 1450 loss: 0.014245695434510708\n",
      "batch: 1460 loss: 0.016497191041707993\n",
      "batch: 1470 loss: 0.015346905216574669\n",
      "batch: 1480 loss: 0.01647082157433033\n",
      "batch: 1490 loss: 0.016149941831827164\n",
      "batch: 1500 loss: 0.014175447635352612\n",
      "batch: 1510 loss: 0.01890791952610016\n",
      "batch: 1520 loss: 0.018754160031676292\n",
      "batch: 1530 loss: 0.020184392109513283\n",
      "batch: 1540 loss: 0.013798083178699017\n",
      "batch: 1550 loss: 0.01863156445324421\n",
      "batch: 1560 loss: 0.01583237573504448\n",
      "batch: 1570 loss: 0.013849322684109211\n",
      "batch: 1580 loss: 0.019884483888745308\n",
      "batch: 1590 loss: 0.017151862382888794\n",
      "batch: 1600 loss: 0.016079889610409737\n",
      "batch: 1610 loss: 0.014392241835594177\n",
      "batch: 1620 loss: 0.021624676883220673\n",
      "batch: 1630 loss: 0.016379212960600853\n",
      "batch: 1640 loss: 0.018376106396317482\n",
      "batch: 1650 loss: 0.01680584065616131\n",
      "batch: 1660 loss: 0.015615750104188919\n",
      "batch: 1670 loss: 0.018963569775223732\n",
      "batch: 1680 loss: 0.01632578670978546\n",
      "batch: 1690 loss: 0.01929657533764839\n",
      "batch: 1700 loss: 0.01558281946927309\n",
      "batch: 1710 loss: 0.013572121039032936\n",
      "batch: 1720 loss: 0.022247150540351868\n",
      "batch: 1730 loss: 0.015498370863497257\n",
      "batch: 1740 loss: 0.014881303533911705\n",
      "batch: 1750 loss: 0.017452392727136612\n",
      "batch: 1760 loss: 0.02381335198879242\n",
      "batch: 1770 loss: 0.015255536884069443\n",
      "batch: 1780 loss: 0.018652906641364098\n",
      "batch: 1790 loss: 0.018560966476798058\n",
      "batch: 1800 loss: 0.019587233662605286\n",
      "batch: 1810 loss: 0.02131246030330658\n",
      "batch: 1820 loss: 0.01986103691160679\n",
      "batch: 1830 loss: 0.019504662603139877\n",
      "batch: 1840 loss: 0.016575517132878304\n",
      "batch: 1850 loss: 0.014750998467206955\n",
      "batch: 1860 loss: 0.015746943652629852\n",
      "batch: 1870 loss: 0.01863131858408451\n",
      "batch: 1880 loss: 0.01706482470035553\n",
      "batch: 1890 loss: 0.014849086292088032\n",
      "batch: 1900 loss: 0.016736580058932304\n",
      "batch: 1910 loss: 0.014214882627129555\n",
      "batch: 1920 loss: 0.018170632421970367\n",
      "batch: 1930 loss: 0.02071015164256096\n",
      "batch: 1940 loss: 0.015152592211961746\n",
      "batch: 1950 loss: 0.020397894084453583\n",
      "batch: 1960 loss: 0.01366086769849062\n",
      "batch: 1970 loss: 0.015540862455964088\n",
      "batch: 1980 loss: 0.016750730574131012\n",
      "batch: 1990 loss: 0.020112156867980957\n",
      "batch: 2000 loss: 0.022381562739610672\n",
      "batch: 2010 loss: 0.016241630539298058\n",
      "batch: 2020 loss: 0.020972521975636482\n",
      "batch: 2030 loss: 0.01797962374985218\n",
      "batch: 2040 loss: 0.0144940922036767\n",
      "batch: 2050 loss: 0.017894966527819633\n",
      "batch: 2060 loss: 0.01692376472055912\n",
      "batch: 2070 loss: 0.01418225932866335\n",
      "batch: 2080 loss: 0.016854826360940933\n",
      "batch: 2090 loss: 0.013542509637773037\n",
      "batch: 2100 loss: 0.01649848185479641\n",
      "batch: 2110 loss: 0.018109900876879692\n",
      "batch: 2120 loss: 0.017991606146097183\n",
      "batch: 2130 loss: 0.020797181874513626\n",
      "batch: 2140 loss: 0.016427427530288696\n",
      "batch: 2150 loss: 0.014170772396028042\n",
      "batch: 2160 loss: 0.01700747385621071\n",
      "batch: 2170 loss: 0.01935434155166149\n",
      "batch: 2180 loss: 0.018702330067753792\n",
      "batch: 2190 loss: 0.017970610409975052\n",
      "batch: 2200 loss: 0.016022101044654846\n",
      "batch: 2210 loss: 0.017684658989310265\n",
      "batch: 2220 loss: 0.017684556543827057\n",
      "batch: 2230 loss: 0.017639271914958954\n",
      "batch: 2240 loss: 0.019506072625517845\n",
      "batch: 2250 loss: 0.015904085710644722\n",
      "batch: 2260 loss: 0.02066872827708721\n",
      "batch: 2270 loss: 0.015648722648620605\n",
      "batch: 2280 loss: 0.015234660357236862\n",
      "batch: 2290 loss: 0.01986096426844597\n",
      "batch: 2300 loss: 0.015405758284032345\n",
      "batch: 2310 loss: 0.010095909237861633\n",
      "batch: 2320 loss: 0.021587330847978592\n",
      "batch: 2330 loss: 0.016451498493552208\n",
      "batch: 2340 loss: 0.014556366950273514\n",
      "batch: 2350 loss: 0.018480991944670677\n",
      "batch: 2360 loss: 0.017490284517407417\n",
      "batch: 2370 loss: 0.01755225658416748\n",
      "batch: 2380 loss: 0.01842736266553402\n",
      "batch: 2390 loss: 0.016468657180666924\n",
      "batch: 2400 loss: 0.014681534841656685\n",
      "batch: 2410 loss: 0.016444912180304527\n",
      "batch: 2420 loss: 0.0162988118827343\n",
      "batch: 2430 loss: 0.012718353420495987\n",
      "batch: 2440 loss: 0.01832803338766098\n",
      "batch: 2450 loss: 0.01671217568218708\n",
      "batch: 2460 loss: 0.014873764477670193\n",
      "batch: 2470 loss: 0.021558614447712898\n",
      "batch: 2480 loss: 0.018509194254875183\n",
      "batch: 2490 loss: 0.021061217412352562\n",
      "batch: 2500 loss: 0.012986830435693264\n",
      "batch: 2510 loss: 0.019286617636680603\n",
      "batch: 2520 loss: 0.01818746328353882\n",
      "batch: 2530 loss: 0.021995525807142258\n",
      "batch: 2540 loss: 0.019192246720194817\n",
      "batch: 2550 loss: 0.019542139023542404\n",
      "batch: 2560 loss: 0.01883799396455288\n",
      "batch: 2570 loss: 0.01783791370689869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2580 loss: 0.018254749476909637\n",
      "batch: 2590 loss: 0.015077812597155571\n",
      "batch: 2600 loss: 0.015467316843569279\n",
      "batch: 2610 loss: 0.011882137507200241\n",
      "batch: 2620 loss: 0.018779931589961052\n",
      "batch: 2630 loss: 0.0191311314702034\n",
      "batch: 2640 loss: 0.0147934565320611\n",
      "batch: 2650 loss: 0.018716242164373398\n",
      "batch: 2660 loss: 0.012491540983319283\n",
      "batch: 2670 loss: 0.01854288950562477\n",
      "batch: 2680 loss: 0.01879606768488884\n",
      "batch: 2690 loss: 0.01992887258529663\n",
      "batch: 2700 loss: 0.015882615000009537\n",
      "batch: 2710 loss: 0.019698401913046837\n",
      "batch: 2720 loss: 0.016306845471262932\n",
      "batch: 2730 loss: 0.01750841550529003\n",
      "batch: 2740 loss: 0.015388958156108856\n",
      "batch: 2750 loss: 0.017031053081154823\n",
      "round  2 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.015101263299584389\n",
      "batch: 10 loss: 0.02087278664112091\n",
      "batch: 20 loss: 0.021903060376644135\n",
      "batch: 30 loss: 0.017892509698867798\n",
      "batch: 40 loss: 0.01537500973790884\n",
      "batch: 50 loss: 0.01641637645661831\n",
      "batch: 60 loss: 0.01798035018146038\n",
      "batch: 70 loss: 0.018126651644706726\n",
      "batch: 80 loss: 0.02117224782705307\n",
      "batch: 90 loss: 0.014532388187944889\n",
      "batch: 100 loss: 0.01682601124048233\n",
      "batch: 110 loss: 0.02032219059765339\n",
      "batch: 120 loss: 0.014285808429121971\n",
      "batch: 130 loss: 0.02237417735159397\n",
      "batch: 140 loss: 0.01577063836157322\n",
      "batch: 150 loss: 0.02014167793095112\n",
      "batch: 160 loss: 0.018341105431318283\n",
      "batch: 170 loss: 0.01548043917864561\n",
      "batch: 180 loss: 0.01731082983314991\n",
      "batch: 190 loss: 0.017959939315915108\n",
      "batch: 200 loss: 0.016510028392076492\n",
      "batch: 210 loss: 0.015604114159941673\n",
      "batch: 220 loss: 0.021512385457754135\n",
      "batch: 230 loss: 0.012519227340817451\n",
      "batch: 240 loss: 0.0172172412276268\n",
      "batch: 250 loss: 0.014542976394295692\n",
      "batch: 260 loss: 0.015878479927778244\n",
      "batch: 270 loss: 0.019678793847560883\n",
      "batch: 280 loss: 0.016303811222314835\n",
      "batch: 290 loss: 0.01669761911034584\n",
      "batch: 300 loss: 0.015649860724806786\n",
      "batch: 310 loss: 0.018712863326072693\n",
      "batch: 320 loss: 0.02204151451587677\n",
      "batch: 330 loss: 0.01665944792330265\n",
      "batch: 340 loss: 0.018895382061600685\n",
      "batch: 350 loss: 0.01879611425101757\n",
      "batch: 360 loss: 0.018040940165519714\n",
      "batch: 370 loss: 0.01735921949148178\n",
      "batch: 380 loss: 0.020821046084165573\n",
      "batch: 390 loss: 0.013482432812452316\n",
      "batch: 400 loss: 0.019329987466335297\n",
      "batch: 410 loss: 0.015187334269285202\n",
      "batch: 420 loss: 0.017059646546840668\n",
      "batch: 430 loss: 0.017564993351697922\n",
      "batch: 440 loss: 0.019223572686314583\n",
      "batch: 450 loss: 0.014557939022779465\n",
      "batch: 460 loss: 0.01601635478436947\n",
      "batch: 470 loss: 0.020854052156209946\n",
      "batch: 480 loss: 0.016003400087356567\n",
      "batch: 490 loss: 0.015163913369178772\n",
      "batch: 500 loss: 0.01816239394247532\n",
      "batch: 510 loss: 0.019360965117812157\n",
      "batch: 520 loss: 0.020060453563928604\n",
      "batch: 530 loss: 0.019088996574282646\n",
      "batch: 540 loss: 0.014266536571085453\n",
      "batch: 550 loss: 0.015686603263020515\n",
      "batch: 560 loss: 0.019430074840784073\n",
      "batch: 570 loss: 0.0192578062415123\n",
      "batch: 580 loss: 0.014456463046371937\n",
      "batch: 590 loss: 0.020578598603606224\n",
      "batch: 600 loss: 0.01576843671500683\n",
      "batch: 610 loss: 0.020223848521709442\n",
      "batch: 620 loss: 0.018656810745596886\n",
      "batch: 630 loss: 0.018810393288731575\n",
      "batch: 640 loss: 0.017373593524098396\n",
      "batch: 650 loss: 0.013532713055610657\n",
      "batch: 660 loss: 0.016600288450717926\n",
      "batch: 670 loss: 0.01579594798386097\n",
      "batch: 680 loss: 0.01717807911336422\n",
      "batch: 690 loss: 0.02138025127351284\n",
      "batch: 700 loss: 0.01821683906018734\n",
      "batch: 710 loss: 0.016889985650777817\n",
      "batch: 720 loss: 0.019099930301308632\n",
      "batch: 730 loss: 0.018213018774986267\n",
      "batch: 740 loss: 0.013271613046526909\n",
      "batch: 750 loss: 0.014808515086770058\n",
      "batch: 760 loss: 0.014447671361267567\n",
      "batch: 770 loss: 0.018474135547876358\n",
      "batch: 780 loss: 0.020513657480478287\n",
      "batch: 790 loss: 0.01740843802690506\n",
      "batch: 800 loss: 0.018495887517929077\n",
      "batch: 810 loss: 0.015602562576532364\n",
      "batch: 820 loss: 0.01534852385520935\n",
      "batch: 830 loss: 0.013305854983627796\n",
      "batch: 840 loss: 0.01980350725352764\n",
      "batch: 850 loss: 0.021813295781612396\n",
      "batch: 860 loss: 0.015936318784952164\n",
      "batch: 870 loss: 0.017886338755488396\n",
      "batch: 880 loss: 0.01818263716995716\n",
      "batch: 890 loss: 0.016569137573242188\n",
      "batch: 900 loss: 0.019694149494171143\n",
      "batch: 910 loss: 0.014851495623588562\n",
      "batch: 920 loss: 0.016285745427012444\n",
      "batch: 930 loss: 0.019373925402760506\n",
      "batch: 940 loss: 0.01682317815721035\n",
      "batch: 950 loss: 0.0168650820851326\n",
      "batch: 960 loss: 0.016885042190551758\n",
      "batch: 970 loss: 0.01681695133447647\n",
      "batch: 980 loss: 0.014513847418129444\n",
      "batch: 990 loss: 0.013040193356573582\n",
      "batch: 1000 loss: 0.016895467415452003\n",
      "batch: 1010 loss: 0.01564881019294262\n",
      "batch: 1020 loss: 0.02031935192644596\n",
      "batch: 1030 loss: 0.02159452810883522\n",
      "batch: 1040 loss: 0.01656550168991089\n",
      "batch: 1050 loss: 0.016832711175084114\n",
      "batch: 1060 loss: 0.0142895532771945\n",
      "batch: 1070 loss: 0.01828121580183506\n",
      "batch: 1080 loss: 0.018009644001722336\n",
      "batch: 1090 loss: 0.018902676180005074\n",
      "batch: 1100 loss: 0.019584640860557556\n",
      "batch: 1110 loss: 0.01979815773665905\n",
      "batch: 1120 loss: 0.017151648178696632\n",
      "batch: 1130 loss: 0.019192669540643692\n",
      "batch: 1140 loss: 0.013721252791583538\n",
      "batch: 1150 loss: 0.017349189147353172\n",
      "batch: 1160 loss: 0.018621066585183144\n",
      "batch: 1170 loss: 0.018265822902321815\n",
      "batch: 1180 loss: 0.014676447957754135\n",
      "batch: 1190 loss: 0.016330217942595482\n",
      "batch: 1200 loss: 0.02028115652501583\n",
      "batch: 1210 loss: 0.011078568175435066\n",
      "batch: 1220 loss: 0.02003510668873787\n",
      "batch: 1230 loss: 0.018765170127153397\n",
      "batch: 1240 loss: 0.016517629846930504\n",
      "batch: 1250 loss: 0.017446842044591904\n",
      "batch: 1260 loss: 0.01784084364771843\n",
      "batch: 1270 loss: 0.017171146348118782\n",
      "batch: 1280 loss: 0.02096431702375412\n",
      "batch: 1290 loss: 0.015638412907719612\n",
      "batch: 1300 loss: 0.015057599171996117\n",
      "batch: 1310 loss: 0.022324075922369957\n",
      "batch: 1320 loss: 0.02038589119911194\n",
      "batch: 1330 loss: 0.015726450830698013\n",
      "batch: 1340 loss: 0.01634296216070652\n",
      "batch: 1350 loss: 0.020479170605540276\n",
      "batch: 1360 loss: 0.011094074696302414\n",
      "batch: 1370 loss: 0.01686089299619198\n",
      "batch: 1380 loss: 0.016900500282645226\n",
      "batch: 1390 loss: 0.01520929392427206\n",
      "batch: 1400 loss: 0.019720781594514847\n",
      "batch: 1410 loss: 0.015101336874067783\n",
      "batch: 1420 loss: 0.018427489325404167\n",
      "batch: 1430 loss: 0.01441501546651125\n",
      "batch: 1440 loss: 0.01755693182349205\n",
      "batch: 1450 loss: 0.014869180507957935\n",
      "batch: 1460 loss: 0.020352523773908615\n",
      "batch: 1470 loss: 0.014133285731077194\n",
      "batch: 1480 loss: 0.016448942944407463\n",
      "batch: 1490 loss: 0.022123005241155624\n",
      "batch: 1500 loss: 0.02200276218354702\n",
      "batch: 1510 loss: 0.016872471198439598\n",
      "batch: 1520 loss: 0.023160042241215706\n",
      "batch: 1530 loss: 0.019824761897325516\n",
      "batch: 1540 loss: 0.018937435001134872\n",
      "batch: 1550 loss: 0.01779167354106903\n",
      "batch: 1560 loss: 0.013213486410677433\n",
      "batch: 1570 loss: 0.020025454461574554\n",
      "batch: 1580 loss: 0.021569201722741127\n",
      "batch: 1590 loss: 0.018582692369818687\n",
      "batch: 1600 loss: 0.01708388887345791\n",
      "batch: 1610 loss: 0.01754029653966427\n",
      "batch: 1620 loss: 0.016132015734910965\n",
      "batch: 1630 loss: 0.017619693651795387\n",
      "batch: 1640 loss: 0.020857777446508408\n",
      "batch: 1650 loss: 0.018930738791823387\n",
      "batch: 1660 loss: 0.01691477559506893\n",
      "batch: 1670 loss: 0.014153244905173779\n",
      "batch: 1680 loss: 0.01755528151988983\n",
      "batch: 1690 loss: 0.012966619804501534\n",
      "batch: 1700 loss: 0.0179449412971735\n",
      "batch: 1710 loss: 0.019870547577738762\n",
      "batch: 1720 loss: 0.0184286218136549\n",
      "batch: 1730 loss: 0.017230721190571785\n",
      "batch: 1740 loss: 0.01998854987323284\n",
      "batch: 1750 loss: 0.020356470718979836\n",
      "batch: 1760 loss: 0.0152874905616045\n",
      "batch: 1770 loss: 0.019875846803188324\n",
      "batch: 1780 loss: 0.018840676173567772\n",
      "batch: 1790 loss: 0.017412308603525162\n",
      "batch: 1800 loss: 0.01831415481865406\n",
      "batch: 1810 loss: 0.014446438290178776\n",
      "batch: 1820 loss: 0.017273740842938423\n",
      "batch: 1830 loss: 0.016522826626896858\n",
      "batch: 1840 loss: 0.019284600391983986\n",
      "batch: 1850 loss: 0.017559103667736053\n",
      "batch: 1860 loss: 0.018223509192466736\n",
      "batch: 1870 loss: 0.017974242568016052\n",
      "batch: 1880 loss: 0.015129823237657547\n",
      "batch: 1890 loss: 0.014756484888494015\n",
      "batch: 1900 loss: 0.019861767068505287\n",
      "batch: 1910 loss: 0.015644757077097893\n",
      "batch: 1920 loss: 0.016931986436247826\n",
      "batch: 1930 loss: 0.014509991742670536\n",
      "batch: 1940 loss: 0.019447997212409973\n",
      "batch: 1950 loss: 0.015476926229894161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1960 loss: 0.01682252250611782\n",
      "batch: 1970 loss: 0.014918377622961998\n",
      "batch: 1980 loss: 0.016064681112766266\n",
      "batch: 1990 loss: 0.01739136129617691\n",
      "batch: 2000 loss: 0.014266843907535076\n",
      "batch: 2010 loss: 0.019337136298418045\n",
      "batch: 2020 loss: 0.019018882885575294\n",
      "batch: 2030 loss: 0.017259398475289345\n",
      "batch: 2040 loss: 0.015793833881616592\n",
      "batch: 2050 loss: 0.016141613945364952\n",
      "batch: 2060 loss: 0.012452165596187115\n",
      "batch: 2070 loss: 0.02039564959704876\n",
      "batch: 2080 loss: 0.015510890632867813\n",
      "batch: 2090 loss: 0.019147809594869614\n",
      "batch: 2100 loss: 0.019548678770661354\n",
      "batch: 2110 loss: 0.0253601111471653\n",
      "batch: 2120 loss: 0.014444450847804546\n",
      "batch: 2130 loss: 0.01674608699977398\n",
      "batch: 2140 loss: 0.015417368151247501\n",
      "batch: 2150 loss: 0.01751714013516903\n",
      "batch: 2160 loss: 0.013226785697042942\n",
      "batch: 2170 loss: 0.015793459489941597\n",
      "batch: 2180 loss: 0.014459781348705292\n",
      "batch: 2190 loss: 0.013577939942479134\n",
      "batch: 2200 loss: 0.014727155677974224\n",
      "batch: 2210 loss: 0.016808239743113518\n",
      "batch: 2220 loss: 0.012154118157923222\n",
      "batch: 2230 loss: 0.019397851079702377\n",
      "batch: 2240 loss: 0.01399805024266243\n",
      "batch: 2250 loss: 0.01708458550274372\n",
      "batch: 2260 loss: 0.013790398836135864\n",
      "batch: 2270 loss: 0.012353095225989819\n",
      "batch: 2280 loss: 0.01581191085278988\n",
      "batch: 2290 loss: 0.02177952043712139\n",
      "batch: 2300 loss: 0.016141021624207497\n",
      "batch: 2310 loss: 0.014406380243599415\n",
      "batch: 2320 loss: 0.015554167330265045\n",
      "batch: 2330 loss: 0.01668272726237774\n",
      "batch: 2340 loss: 0.017388340085744858\n",
      "batch: 2350 loss: 0.019001945853233337\n",
      "batch: 2360 loss: 0.02011185511946678\n",
      "batch: 2370 loss: 0.01657423935830593\n",
      "batch: 2380 loss: 0.019013280048966408\n",
      "batch: 2390 loss: 0.016339175403118134\n",
      "batch: 2400 loss: 0.01790502853691578\n",
      "batch: 2410 loss: 0.0182100310921669\n",
      "batch: 2420 loss: 0.017514130100607872\n",
      "batch: 2430 loss: 0.015314186923205853\n",
      "batch: 2440 loss: 0.013049408793449402\n",
      "batch: 2450 loss: 0.016194360330700874\n",
      "batch: 2460 loss: 0.013633020222187042\n",
      "batch: 2470 loss: 0.015960929915308952\n",
      "batch: 2480 loss: 0.015215995721518993\n",
      "batch: 2490 loss: 0.01667226478457451\n",
      "batch: 2500 loss: 0.017417436465620995\n",
      "batch: 2510 loss: 0.018162662163376808\n",
      "batch: 2520 loss: 0.018891476094722748\n",
      "batch: 2530 loss: 0.016901683062314987\n",
      "batch: 2540 loss: 0.013374068774282932\n",
      "batch: 2550 loss: 0.015377742238342762\n",
      "batch: 2560 loss: 0.012592524290084839\n",
      "batch: 2570 loss: 0.013749629259109497\n",
      "batch: 2580 loss: 0.010978220030665398\n",
      "batch: 2590 loss: 0.01760845258831978\n",
      "batch: 2600 loss: 0.015665629878640175\n",
      "batch: 2610 loss: 0.019408833235502243\n",
      "batch: 2620 loss: 0.01428676676005125\n",
      "batch: 2630 loss: 0.015814973041415215\n",
      "batch: 2640 loss: 0.02050851844251156\n",
      "batch: 2650 loss: 0.018098536878824234\n",
      "batch: 2660 loss: 0.013948380015790462\n",
      "batch: 2670 loss: 0.017302941530942917\n",
      "batch: 2680 loss: 0.01996629871428013\n",
      "batch: 2690 loss: 0.014738534577190876\n",
      "batch: 2700 loss: 0.019364066421985626\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.017789019271731377\n",
      "batch: 10 loss: 0.01481958944350481\n",
      "batch: 20 loss: 0.017352864146232605\n",
      "batch: 30 loss: 0.017750978469848633\n",
      "batch: 40 loss: 0.011437551118433475\n",
      "batch: 50 loss: 0.01632971316576004\n",
      "batch: 60 loss: 0.01743846759200096\n",
      "batch: 70 loss: 0.015892336145043373\n",
      "batch: 80 loss: 0.01732371374964714\n",
      "batch: 90 loss: 0.018146589398384094\n",
      "batch: 100 loss: 0.015850232914090157\n",
      "batch: 110 loss: 0.021278074011206627\n",
      "batch: 120 loss: 0.01614474318921566\n",
      "batch: 130 loss: 0.017900874838232994\n",
      "batch: 140 loss: 0.017581436783075333\n",
      "batch: 150 loss: 0.017837679013609886\n",
      "batch: 160 loss: 0.018400782719254494\n",
      "batch: 170 loss: 0.017753830179572105\n",
      "batch: 180 loss: 0.023316871374845505\n",
      "batch: 190 loss: 0.019221816211938858\n",
      "batch: 200 loss: 0.01851620152592659\n",
      "batch: 210 loss: 0.016714364290237427\n",
      "batch: 220 loss: 0.01912813074886799\n",
      "batch: 230 loss: 0.01396822091192007\n",
      "batch: 240 loss: 0.013525414280593395\n",
      "batch: 250 loss: 0.014806825667619705\n",
      "batch: 260 loss: 0.012906399555504322\n",
      "batch: 270 loss: 0.019419759511947632\n",
      "batch: 280 loss: 0.017586518079042435\n",
      "batch: 290 loss: 0.015082222409546375\n",
      "batch: 300 loss: 0.019697267562150955\n",
      "batch: 310 loss: 0.016104552894830704\n",
      "batch: 320 loss: 0.019149623811244965\n",
      "batch: 330 loss: 0.01849975436925888\n",
      "batch: 340 loss: 0.013368294574320316\n",
      "batch: 350 loss: 0.01705970987677574\n",
      "batch: 360 loss: 0.021761847659945488\n",
      "batch: 370 loss: 0.016476936638355255\n",
      "batch: 380 loss: 0.019824078306555748\n",
      "batch: 390 loss: 0.015449768863618374\n",
      "batch: 400 loss: 0.014363214373588562\n",
      "batch: 410 loss: 0.015723709017038345\n",
      "batch: 420 loss: 0.015167494304478168\n",
      "batch: 430 loss: 0.01340300403535366\n",
      "batch: 440 loss: 0.020313793793320656\n",
      "batch: 450 loss: 0.020785024389624596\n",
      "batch: 460 loss: 0.015500156208872795\n",
      "batch: 470 loss: 0.01602054573595524\n",
      "batch: 480 loss: 0.016548339277505875\n",
      "batch: 490 loss: 0.014820841141045094\n",
      "batch: 500 loss: 0.02030470222234726\n",
      "batch: 510 loss: 0.017064083367586136\n",
      "batch: 520 loss: 0.013680223375558853\n",
      "batch: 530 loss: 0.015269027091562748\n",
      "batch: 540 loss: 0.017583541572093964\n",
      "batch: 550 loss: 0.01743417978286743\n",
      "batch: 560 loss: 0.018992174416780472\n",
      "batch: 570 loss: 0.01714862883090973\n",
      "batch: 580 loss: 0.016675885766744614\n",
      "batch: 590 loss: 0.014808259904384613\n",
      "batch: 600 loss: 0.017818214371800423\n",
      "batch: 610 loss: 0.016504259780049324\n",
      "batch: 620 loss: 0.014553800225257874\n",
      "batch: 630 loss: 0.010051180608570576\n",
      "batch: 640 loss: 0.016901111230254173\n",
      "batch: 650 loss: 0.018463559448719025\n",
      "batch: 660 loss: 0.017897823825478554\n",
      "batch: 670 loss: 0.019537726417183876\n",
      "batch: 680 loss: 0.016362112015485764\n",
      "batch: 690 loss: 0.015661563724279404\n",
      "batch: 700 loss: 0.016873490065336227\n",
      "batch: 710 loss: 0.01758429780602455\n",
      "batch: 720 loss: 0.016525503247976303\n",
      "batch: 730 loss: 0.013720512390136719\n",
      "batch: 740 loss: 0.01829581707715988\n",
      "batch: 750 loss: 0.01724310778081417\n",
      "batch: 760 loss: 0.01720961555838585\n",
      "batch: 770 loss: 0.017064353451132774\n",
      "batch: 780 loss: 0.014918772503733635\n",
      "batch: 790 loss: 0.014817383140325546\n",
      "batch: 800 loss: 0.01587703265249729\n",
      "batch: 810 loss: 0.016864974051713943\n",
      "batch: 820 loss: 0.022619910538196564\n",
      "batch: 830 loss: 0.017852775752544403\n",
      "batch: 840 loss: 0.017907509580254555\n",
      "batch: 850 loss: 0.019426966086030006\n",
      "batch: 860 loss: 0.01860211044549942\n",
      "batch: 870 loss: 0.015140057541429996\n",
      "batch: 880 loss: 0.014769136905670166\n",
      "batch: 890 loss: 0.010569700971245766\n",
      "batch: 900 loss: 0.016117172315716743\n",
      "batch: 910 loss: 0.01660594344139099\n",
      "batch: 920 loss: 0.015347079373896122\n",
      "batch: 930 loss: 0.016558349132537842\n",
      "batch: 940 loss: 0.0170143935829401\n",
      "batch: 950 loss: 0.017412979155778885\n",
      "batch: 960 loss: 0.022052787244319916\n",
      "batch: 970 loss: 0.017149753868579865\n",
      "batch: 980 loss: 0.016857311129570007\n",
      "batch: 990 loss: 0.019389547407627106\n",
      "batch: 1000 loss: 0.018323544412851334\n",
      "batch: 1010 loss: 0.01683221571147442\n",
      "batch: 1020 loss: 0.01472614798694849\n",
      "batch: 1030 loss: 0.01325430441647768\n",
      "batch: 1040 loss: 0.02008373849093914\n",
      "batch: 1050 loss: 0.015882175415754318\n",
      "batch: 1060 loss: 0.017979826778173447\n",
      "batch: 1070 loss: 0.019215989857912064\n",
      "batch: 1080 loss: 0.017198825255036354\n",
      "batch: 1090 loss: 0.016675615683197975\n",
      "batch: 1100 loss: 0.019875362515449524\n",
      "batch: 1110 loss: 0.015535728074610233\n",
      "batch: 1120 loss: 0.014391893520951271\n",
      "batch: 1130 loss: 0.01609913446009159\n",
      "batch: 1140 loss: 0.018271800130605698\n",
      "batch: 1150 loss: 0.020820332691073418\n",
      "batch: 1160 loss: 0.017934007570147514\n",
      "batch: 1170 loss: 0.015400809235870838\n",
      "batch: 1180 loss: 0.012851137667894363\n",
      "batch: 1190 loss: 0.018408048897981644\n",
      "batch: 1200 loss: 0.015269759111106396\n",
      "batch: 1210 loss: 0.01830083504319191\n",
      "batch: 1220 loss: 0.014695367775857449\n",
      "batch: 1230 loss: 0.021430598571896553\n",
      "batch: 1240 loss: 0.015981724485754967\n",
      "batch: 1250 loss: 0.011526728048920631\n",
      "batch: 1260 loss: 0.01568453386425972\n",
      "batch: 1270 loss: 0.01352140586823225\n",
      "batch: 1280 loss: 0.016217488795518875\n",
      "batch: 1290 loss: 0.01703638769686222\n",
      "batch: 1300 loss: 0.01999172568321228\n",
      "batch: 1310 loss: 0.019241243600845337\n",
      "batch: 1320 loss: 0.018463781103491783\n",
      "batch: 1330 loss: 0.018337735906243324\n",
      "batch: 1340 loss: 0.01801350526511669\n",
      "batch: 1350 loss: 0.01687050797045231\n",
      "batch: 1360 loss: 0.014230337925255299\n",
      "batch: 1370 loss: 0.019855253398418427\n",
      "batch: 1380 loss: 0.012672408483922482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1390 loss: 0.017571603879332542\n",
      "batch: 1400 loss: 0.016841718927025795\n",
      "batch: 1410 loss: 0.016241589561104774\n",
      "batch: 1420 loss: 0.020075645297765732\n",
      "batch: 1430 loss: 0.01818140782415867\n",
      "batch: 1440 loss: 0.011829646304249763\n",
      "batch: 1450 loss: 0.017374230548739433\n",
      "batch: 1460 loss: 0.019494513049721718\n",
      "batch: 1470 loss: 0.017876453697681427\n",
      "batch: 1480 loss: 0.01600029692053795\n",
      "batch: 1490 loss: 0.017669502645730972\n",
      "batch: 1500 loss: 0.02528572268784046\n",
      "batch: 1510 loss: 0.017499370500445366\n",
      "batch: 1520 loss: 0.017531825229525566\n",
      "batch: 1530 loss: 0.016255196183919907\n",
      "batch: 1540 loss: 0.014115847647190094\n",
      "batch: 1550 loss: 0.01647789217531681\n",
      "batch: 1560 loss: 0.024477550759911537\n",
      "batch: 1570 loss: 0.02005132846534252\n",
      "batch: 1580 loss: 0.014239552430808544\n",
      "batch: 1590 loss: 0.015614726580679417\n",
      "batch: 1600 loss: 0.011612317524850368\n",
      "batch: 1610 loss: 0.0209124106913805\n",
      "batch: 1620 loss: 0.02036447450518608\n",
      "batch: 1630 loss: 0.01503723580390215\n",
      "batch: 1640 loss: 0.015056408941745758\n",
      "batch: 1650 loss: 0.014149416238069534\n",
      "batch: 1660 loss: 0.014807023108005524\n",
      "batch: 1670 loss: 0.01604950800538063\n",
      "batch: 1680 loss: 0.017599277198314667\n",
      "batch: 1690 loss: 0.0181158147752285\n",
      "batch: 1700 loss: 0.0203538928180933\n",
      "batch: 1710 loss: 0.019326837733387947\n",
      "batch: 1720 loss: 0.015752002596855164\n",
      "batch: 1730 loss: 0.013323278166353703\n",
      "batch: 1740 loss: 0.015744883567094803\n",
      "batch: 1750 loss: 0.017907490953803062\n",
      "batch: 1760 loss: 0.014499428682029247\n",
      "batch: 1770 loss: 0.019963037222623825\n",
      "batch: 1780 loss: 0.016354668885469437\n",
      "batch: 1790 loss: 0.019643817096948624\n",
      "batch: 1800 loss: 0.01258314959704876\n",
      "batch: 1810 loss: 0.01574690267443657\n",
      "batch: 1820 loss: 0.018895559012889862\n",
      "batch: 1830 loss: 0.015560292638838291\n",
      "batch: 1840 loss: 0.015023588202893734\n",
      "batch: 1850 loss: 0.017368365079164505\n",
      "batch: 1860 loss: 0.014342375099658966\n",
      "batch: 1870 loss: 0.015750428661704063\n",
      "batch: 1880 loss: 0.016122015193104744\n",
      "batch: 1890 loss: 0.018859531730413437\n",
      "batch: 1900 loss: 0.018942367285490036\n",
      "batch: 1910 loss: 0.017442762851715088\n",
      "batch: 1920 loss: 0.013384919613599777\n",
      "batch: 1930 loss: 0.020631276071071625\n",
      "batch: 1940 loss: 0.013820157386362553\n",
      "batch: 1950 loss: 0.014344996772706509\n",
      "batch: 1960 loss: 0.017285015434026718\n",
      "batch: 1970 loss: 0.01593337021768093\n",
      "batch: 1980 loss: 0.02248150110244751\n",
      "batch: 1990 loss: 0.014513581991195679\n",
      "batch: 2000 loss: 0.013180917128920555\n",
      "batch: 2010 loss: 0.01996682956814766\n",
      "batch: 2020 loss: 0.019931606948375702\n",
      "batch: 2030 loss: 0.01671157218515873\n",
      "batch: 2040 loss: 0.015249249525368214\n",
      "batch: 2050 loss: 0.018190087750554085\n",
      "batch: 2060 loss: 0.014709914103150368\n",
      "batch: 2070 loss: 0.01967843435704708\n",
      "batch: 2080 loss: 0.013387150131165981\n",
      "batch: 2090 loss: 0.015310600399971008\n",
      "batch: 2100 loss: 0.020741665735840797\n",
      "batch: 2110 loss: 0.02135278470814228\n",
      "batch: 2120 loss: 0.017783159390091896\n",
      "batch: 2130 loss: 0.022414499893784523\n",
      "batch: 2140 loss: 0.024218112230300903\n",
      "batch: 2150 loss: 0.022990470752120018\n",
      "batch: 2160 loss: 0.011549408547580242\n",
      "batch: 2170 loss: 0.01862417720258236\n",
      "batch: 2180 loss: 0.018045956268906593\n",
      "batch: 2190 loss: 0.015733420848846436\n",
      "batch: 2200 loss: 0.01364104077219963\n",
      "batch: 2210 loss: 0.016659485176205635\n",
      "batch: 2220 loss: 0.018610835075378418\n",
      "batch: 2230 loss: 0.019328312948346138\n",
      "batch: 2240 loss: 0.017749633640050888\n",
      "batch: 2250 loss: 0.017306338995695114\n",
      "batch: 2260 loss: 0.020053962245583534\n",
      "batch: 2270 loss: 0.015760045498609543\n",
      "batch: 2280 loss: 0.02401544712483883\n",
      "batch: 2290 loss: 0.01263879518955946\n",
      "batch: 2300 loss: 0.015905605629086494\n",
      "batch: 2310 loss: 0.016978612169623375\n",
      "batch: 2320 loss: 0.019879935309290886\n",
      "batch: 2330 loss: 0.015618512406945229\n",
      "batch: 2340 loss: 0.01751287467777729\n",
      "batch: 2350 loss: 0.016261383891105652\n",
      "batch: 2360 loss: 0.01391522865742445\n",
      "batch: 2370 loss: 0.01755009964108467\n",
      "batch: 2380 loss: 0.016575265675783157\n",
      "batch: 2390 loss: 0.017092270776629448\n",
      "batch: 2400 loss: 0.018278956413269043\n",
      "batch: 2410 loss: 0.01678302511572838\n",
      "batch: 2420 loss: 0.020268557593226433\n",
      "batch: 2430 loss: 0.017533665522933006\n",
      "batch: 2440 loss: 0.014575972221791744\n",
      "batch: 2450 loss: 0.02389344573020935\n",
      "batch: 2460 loss: 0.018317457288503647\n",
      "batch: 2470 loss: 0.018716786056756973\n",
      "batch: 2480 loss: 0.016842110082507133\n",
      "batch: 2490 loss: 0.021045012399554253\n",
      "batch: 2500 loss: 0.014087735675275326\n",
      "batch: 2510 loss: 0.014915903098881245\n",
      "batch: 2520 loss: 0.014428420923650265\n",
      "batch: 2530 loss: 0.017289459705352783\n",
      "batch: 2540 loss: 0.016377318650484085\n",
      "batch: 2550 loss: 0.016317415982484818\n",
      "batch: 2560 loss: 0.022828776389360428\n",
      "batch: 2570 loss: 0.01725040003657341\n",
      "batch: 2580 loss: 0.01596582680940628\n",
      "batch: 2590 loss: 0.017631443217396736\n",
      "batch: 2600 loss: 0.01582525670528412\n",
      "batch: 2610 loss: 0.01551225408911705\n",
      "batch: 2620 loss: 0.0201801098883152\n",
      "batch: 2630 loss: 0.01578112505376339\n",
      "batch: 2640 loss: 0.014621376991271973\n",
      "batch: 2650 loss: 0.01718532294034958\n",
      "batch: 2660 loss: 0.017101461067795753\n",
      "batch: 2670 loss: 0.015996532514691353\n",
      "batch: 2680 loss: 0.015187038108706474\n",
      "batch: 2690 loss: 0.016352012753486633\n",
      "batch: 2700 loss: 0.016958974301815033\n",
      "round  3 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.01861889846622944\n",
      "batch: 10 loss: 0.017000064253807068\n",
      "batch: 20 loss: 0.017641142010688782\n",
      "batch: 30 loss: 0.01679948903620243\n",
      "batch: 40 loss: 0.014905918389558792\n",
      "batch: 50 loss: 0.014494399540126324\n",
      "batch: 60 loss: 0.016921496018767357\n",
      "batch: 70 loss: 0.01873367838561535\n",
      "batch: 80 loss: 0.016217736527323723\n",
      "batch: 90 loss: 0.018733493983745575\n",
      "batch: 100 loss: 0.016201546415686607\n",
      "batch: 110 loss: 0.016503967344760895\n",
      "batch: 120 loss: 0.018857823684811592\n",
      "batch: 130 loss: 0.019465556368231773\n",
      "batch: 140 loss: 0.013648482039570808\n",
      "batch: 150 loss: 0.015048009343445301\n",
      "batch: 160 loss: 0.017895882949233055\n",
      "batch: 170 loss: 0.017479047179222107\n",
      "batch: 180 loss: 0.01556536927819252\n",
      "batch: 190 loss: 0.013973701745271683\n",
      "batch: 200 loss: 0.018701789900660515\n",
      "batch: 210 loss: 0.01589212939143181\n",
      "batch: 220 loss: 0.013504406437277794\n",
      "batch: 230 loss: 0.018346063792705536\n",
      "batch: 240 loss: 0.01258394680917263\n",
      "batch: 250 loss: 0.018664179369807243\n",
      "batch: 260 loss: 0.01894327811896801\n",
      "batch: 270 loss: 0.018552294000983238\n",
      "batch: 280 loss: 0.01592770405113697\n",
      "batch: 290 loss: 0.018047073855996132\n",
      "batch: 300 loss: 0.01791931875050068\n",
      "batch: 310 loss: 0.015821106731891632\n",
      "batch: 320 loss: 0.014651734381914139\n",
      "batch: 330 loss: 0.01873297430574894\n",
      "batch: 340 loss: 0.017062339931726456\n",
      "batch: 350 loss: 0.016230858862400055\n",
      "batch: 360 loss: 0.01533424761146307\n",
      "batch: 370 loss: 0.014183207415044308\n",
      "batch: 380 loss: 0.018516046926379204\n",
      "batch: 390 loss: 0.014853854663670063\n",
      "batch: 400 loss: 0.016630688682198524\n",
      "batch: 410 loss: 0.014837007038295269\n",
      "batch: 420 loss: 0.014501690864562988\n",
      "batch: 430 loss: 0.016043217852711678\n",
      "batch: 440 loss: 0.019770149141550064\n",
      "batch: 450 loss: 0.013953294605016708\n",
      "batch: 460 loss: 0.019369302317500114\n",
      "batch: 470 loss: 0.018101932480931282\n",
      "batch: 480 loss: 0.015343262813985348\n",
      "batch: 490 loss: 0.018707096576690674\n",
      "batch: 500 loss: 0.013776806183159351\n",
      "batch: 510 loss: 0.017084207385778427\n",
      "batch: 520 loss: 0.018581448122859\n",
      "batch: 530 loss: 0.018635286018252373\n",
      "batch: 540 loss: 0.01463330164551735\n",
      "batch: 550 loss: 0.016063502058386803\n",
      "batch: 560 loss: 0.018305229023098946\n",
      "batch: 570 loss: 0.016082342714071274\n",
      "batch: 580 loss: 0.015225847251713276\n",
      "batch: 590 loss: 0.015683133155107498\n",
      "batch: 600 loss: 0.014606320299208164\n",
      "batch: 610 loss: 0.017320241779088974\n",
      "batch: 620 loss: 0.01772378571331501\n",
      "batch: 630 loss: 0.0153890922665596\n",
      "batch: 640 loss: 0.01632997766137123\n",
      "batch: 650 loss: 0.01679387502372265\n",
      "batch: 660 loss: 0.01650157757103443\n",
      "batch: 670 loss: 0.014135666191577911\n",
      "batch: 680 loss: 0.017799800261855125\n",
      "batch: 690 loss: 0.0153045067563653\n",
      "batch: 700 loss: 0.02172231301665306\n",
      "batch: 710 loss: 0.01705600693821907\n",
      "batch: 720 loss: 0.016158223152160645\n",
      "batch: 730 loss: 0.015575702302157879\n",
      "batch: 740 loss: 0.01594349555671215\n",
      "batch: 750 loss: 0.01662924885749817\n",
      "batch: 760 loss: 0.017563611268997192\n",
      "batch: 770 loss: 0.01720541901886463\n",
      "batch: 780 loss: 0.014171781949698925\n",
      "batch: 790 loss: 0.01372497994452715\n",
      "batch: 800 loss: 0.015829484909772873\n",
      "batch: 810 loss: 0.017922809347510338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 820 loss: 0.016992684453725815\n",
      "batch: 830 loss: 0.01715233363211155\n",
      "batch: 840 loss: 0.016150636598467827\n",
      "batch: 850 loss: 0.016339212656021118\n",
      "batch: 860 loss: 0.015911966562271118\n",
      "batch: 870 loss: 0.01967845857143402\n",
      "batch: 880 loss: 0.01943111978471279\n",
      "batch: 890 loss: 0.018566077575087547\n",
      "batch: 900 loss: 0.01588633470237255\n",
      "batch: 910 loss: 0.014294722117483616\n",
      "batch: 920 loss: 0.012793821282684803\n",
      "batch: 930 loss: 0.016120020300149918\n",
      "batch: 940 loss: 0.015608785673975945\n",
      "batch: 950 loss: 0.02170662395656109\n",
      "batch: 960 loss: 0.01654261350631714\n",
      "batch: 970 loss: 0.018971402198076248\n",
      "batch: 980 loss: 0.01429937407374382\n",
      "batch: 990 loss: 0.01839258149266243\n",
      "batch: 1000 loss: 0.020229989662766457\n",
      "batch: 1010 loss: 0.015242161229252815\n",
      "batch: 1020 loss: 0.021832671016454697\n",
      "batch: 1030 loss: 0.011717681773006916\n",
      "batch: 1040 loss: 0.016345173120498657\n",
      "batch: 1050 loss: 0.01976316049695015\n",
      "batch: 1060 loss: 0.021090572699904442\n",
      "batch: 1070 loss: 0.01805202290415764\n",
      "batch: 1080 loss: 0.018107373267412186\n",
      "batch: 1090 loss: 0.017798837274312973\n",
      "batch: 1100 loss: 0.014610537327826023\n",
      "batch: 1110 loss: 0.016123104840517044\n",
      "batch: 1120 loss: 0.013780953362584114\n",
      "batch: 1130 loss: 0.012015894055366516\n",
      "batch: 1140 loss: 0.015478123910725117\n",
      "batch: 1150 loss: 0.018514594063162804\n",
      "batch: 1160 loss: 0.01563265733420849\n",
      "batch: 1170 loss: 0.013577869161963463\n",
      "batch: 1180 loss: 0.021121317520737648\n",
      "batch: 1190 loss: 0.02104576677083969\n",
      "batch: 1200 loss: 0.01725143752992153\n",
      "batch: 1210 loss: 0.015334054827690125\n",
      "batch: 1220 loss: 0.01915203034877777\n",
      "batch: 1230 loss: 0.01571153849363327\n",
      "batch: 1240 loss: 0.01437052246183157\n",
      "batch: 1250 loss: 0.02009616605937481\n",
      "batch: 1260 loss: 0.015197543427348137\n",
      "batch: 1270 loss: 0.015268414281308651\n",
      "batch: 1280 loss: 0.018070802092552185\n",
      "batch: 1290 loss: 0.013951263390481472\n",
      "batch: 1300 loss: 0.01709696091711521\n",
      "batch: 1310 loss: 0.016318725422024727\n",
      "batch: 1320 loss: 0.01613515615463257\n",
      "batch: 1330 loss: 0.01690538227558136\n",
      "batch: 1340 loss: 0.018157295882701874\n",
      "batch: 1350 loss: 0.018951090052723885\n",
      "batch: 1360 loss: 0.01916397549211979\n",
      "batch: 1370 loss: 0.016159502789378166\n",
      "batch: 1380 loss: 0.015559534542262554\n",
      "batch: 1390 loss: 0.018689211457967758\n",
      "batch: 1400 loss: 0.01689658686518669\n",
      "batch: 1410 loss: 0.020634926855564117\n",
      "batch: 1420 loss: 0.015586012043058872\n",
      "batch: 1430 loss: 0.015575876459479332\n",
      "batch: 1440 loss: 0.01853339746594429\n",
      "batch: 1450 loss: 0.015429738909006119\n",
      "batch: 1460 loss: 0.016277961432933807\n",
      "batch: 1470 loss: 0.018480634316802025\n",
      "batch: 1480 loss: 0.015125183388590813\n",
      "batch: 1490 loss: 0.015178454108536243\n",
      "batch: 1500 loss: 0.013903671875596046\n",
      "batch: 1510 loss: 0.017218030989170074\n",
      "batch: 1520 loss: 0.018316706642508507\n",
      "batch: 1530 loss: 0.012288016267120838\n",
      "batch: 1540 loss: 0.017094051465392113\n",
      "batch: 1550 loss: 0.02157958224415779\n",
      "batch: 1560 loss: 0.01790250651538372\n",
      "batch: 1570 loss: 0.017915721982717514\n",
      "batch: 1580 loss: 0.01689973473548889\n",
      "batch: 1590 loss: 0.01395641639828682\n",
      "batch: 1600 loss: 0.01859832927584648\n",
      "batch: 1610 loss: 0.016341732814908028\n",
      "batch: 1620 loss: 0.019037840887904167\n",
      "batch: 1630 loss: 0.022176533937454224\n",
      "batch: 1640 loss: 0.01693730056285858\n",
      "batch: 1650 loss: 0.013098662719130516\n",
      "batch: 1660 loss: 0.0177705567330122\n",
      "batch: 1670 loss: 0.016751229763031006\n",
      "batch: 1680 loss: 0.018242398276925087\n",
      "batch: 1690 loss: 0.01546893734484911\n",
      "batch: 1700 loss: 0.014544330537319183\n",
      "batch: 1710 loss: 0.017119543626904488\n",
      "batch: 1720 loss: 0.012795084156095982\n",
      "batch: 1730 loss: 0.021030984818935394\n",
      "batch: 1740 loss: 0.021051546558737755\n",
      "batch: 1750 loss: 0.01777120679616928\n",
      "batch: 1760 loss: 0.01803944632411003\n",
      "batch: 1770 loss: 0.015184182673692703\n",
      "batch: 1780 loss: 0.018105648458003998\n",
      "batch: 1790 loss: 0.016757315024733543\n",
      "batch: 1800 loss: 0.0162093173712492\n",
      "batch: 1810 loss: 0.011776216328144073\n",
      "batch: 1820 loss: 0.017429590225219727\n",
      "batch: 1830 loss: 0.018381565809249878\n",
      "batch: 1840 loss: 0.020963499322533607\n",
      "batch: 1850 loss: 0.01790359802544117\n",
      "batch: 1860 loss: 0.015169157646596432\n",
      "batch: 1870 loss: 0.017239389941096306\n",
      "batch: 1880 loss: 0.017306266352534294\n",
      "batch: 1890 loss: 0.018767230212688446\n",
      "batch: 1900 loss: 0.018297089263796806\n",
      "batch: 1910 loss: 0.019802870228886604\n",
      "batch: 1920 loss: 0.012758789584040642\n",
      "batch: 1930 loss: 0.015639502555131912\n",
      "batch: 1940 loss: 0.01981433294713497\n",
      "batch: 1950 loss: 0.019147198647260666\n",
      "batch: 1960 loss: 0.01855478063225746\n",
      "batch: 1970 loss: 0.017754528671503067\n",
      "batch: 1980 loss: 0.014642671681940556\n",
      "batch: 1990 loss: 0.016790395602583885\n",
      "batch: 2000 loss: 0.018354959785938263\n",
      "batch: 2010 loss: 0.016378212720155716\n",
      "batch: 2020 loss: 0.016163812950253487\n",
      "batch: 2030 loss: 0.01893598958849907\n",
      "batch: 2040 loss: 0.015284455381333828\n",
      "batch: 2050 loss: 0.015590814873576164\n",
      "batch: 2060 loss: 0.01463880855590105\n",
      "batch: 2070 loss: 0.01835719123482704\n",
      "batch: 2080 loss: 0.01877441070973873\n",
      "batch: 2090 loss: 0.015548668801784515\n",
      "batch: 2100 loss: 0.020716099068522453\n",
      "batch: 2110 loss: 0.017702385783195496\n",
      "batch: 2120 loss: 0.016471732407808304\n",
      "batch: 2130 loss: 0.015045993961393833\n",
      "batch: 2140 loss: 0.01874290220439434\n",
      "batch: 2150 loss: 0.0130166532471776\n",
      "batch: 2160 loss: 0.017267577350139618\n",
      "batch: 2170 loss: 0.019333433359861374\n",
      "batch: 2180 loss: 0.01688646897673607\n",
      "batch: 2190 loss: 0.016464071348309517\n",
      "batch: 2200 loss: 0.014143183827400208\n",
      "batch: 2210 loss: 0.020585602149367332\n",
      "batch: 2220 loss: 0.018270734697580338\n",
      "batch: 2230 loss: 0.01658337377011776\n",
      "batch: 2240 loss: 0.015110969543457031\n",
      "batch: 2250 loss: 0.021919695660471916\n",
      "batch: 2260 loss: 0.019436748698353767\n",
      "batch: 2270 loss: 0.018728382885456085\n",
      "batch: 2280 loss: 0.01704438216984272\n",
      "batch: 2290 loss: 0.018609050661325455\n",
      "batch: 2300 loss: 0.022828461602330208\n",
      "batch: 2310 loss: 0.019326265901327133\n",
      "batch: 2320 loss: 0.022066466510295868\n",
      "batch: 2330 loss: 0.01643545925617218\n",
      "batch: 2340 loss: 0.018270468339323997\n",
      "batch: 2350 loss: 0.018495379015803337\n",
      "batch: 2360 loss: 0.016577553004026413\n",
      "batch: 2370 loss: 0.0178880225867033\n",
      "batch: 2380 loss: 0.019716864451766014\n",
      "batch: 2390 loss: 0.017375478520989418\n",
      "batch: 2400 loss: 0.014192571863532066\n",
      "batch: 2410 loss: 0.014716305769979954\n",
      "batch: 2420 loss: 0.014591671526432037\n",
      "batch: 2430 loss: 0.01404702290892601\n",
      "batch: 2440 loss: 0.013043064624071121\n",
      "batch: 2450 loss: 0.01662312261760235\n",
      "batch: 2460 loss: 0.018990494310855865\n",
      "batch: 2470 loss: 0.01634743995964527\n",
      "batch: 2480 loss: 0.015852579846978188\n",
      "batch: 2490 loss: 0.015859076753258705\n",
      "batch: 2500 loss: 0.01665666699409485\n",
      "batch: 2510 loss: 0.01655244641005993\n",
      "batch: 2520 loss: 0.021177465096116066\n",
      "batch: 2530 loss: 0.017810842022299767\n",
      "batch: 2540 loss: 0.015136192552745342\n",
      "batch: 2550 loss: 0.021205272525548935\n",
      "batch: 2560 loss: 0.014864940196275711\n",
      "batch: 2570 loss: 0.013233443722128868\n",
      "batch: 2580 loss: 0.020776333287358284\n",
      "batch: 2590 loss: 0.014280903153121471\n",
      "batch: 2600 loss: 0.013879355974495411\n",
      "batch: 2610 loss: 0.01627759076654911\n",
      "batch: 2620 loss: 0.015093051828444004\n",
      "batch: 2630 loss: 0.016828488558530807\n",
      "batch: 2640 loss: 0.018386075273156166\n",
      "batch: 2650 loss: 0.014654242433607578\n",
      "batch: 2660 loss: 0.016122829169034958\n",
      "batch: 2670 loss: 0.017480148002505302\n",
      "batch: 2680 loss: 0.01659194566309452\n",
      "batch: 2690 loss: 0.012418916448950768\n",
      "batch: 2700 loss: 0.017931226640939713\n",
      "batch: 2710 loss: 0.01638096012175083\n",
      "batch: 2720 loss: 0.01435584481805563\n",
      "batch: 2730 loss: 0.017140252515673637\n",
      "batch: 2740 loss: 0.018869560211896896\n",
      "batch: 2750 loss: 0.01581612601876259\n",
      "batch: 2760 loss: 0.022660790011286736\n",
      "batch: 2770 loss: 0.01460203342139721\n",
      "batch: 2780 loss: 0.016170425340533257\n",
      "batch: 2790 loss: 0.017665477469563484\n",
      "batch: 2800 loss: 0.018182141706347466\n",
      "batch: 2810 loss: 0.013833086006343365\n",
      "batch: 2820 loss: 0.016388244926929474\n",
      "batch: 2830 loss: 0.01373542845249176\n",
      "batch: 2840 loss: 0.015195874497294426\n",
      "batch: 2850 loss: 0.014019517228007317\n",
      "batch: 2860 loss: 0.020150743424892426\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.016756022348999977\n",
      "batch: 10 loss: 0.014572381973266602\n",
      "batch: 20 loss: 0.018157975748181343\n",
      "batch: 30 loss: 0.01835145428776741\n",
      "batch: 40 loss: 0.020967645570635796\n",
      "batch: 50 loss: 0.014784316532313824\n",
      "batch: 60 loss: 0.018789688125252724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 70 loss: 0.01783163845539093\n",
      "batch: 80 loss: 0.015502364374697208\n",
      "batch: 90 loss: 0.014823411591351032\n",
      "batch: 100 loss: 0.017955316230654716\n",
      "batch: 110 loss: 0.01768435724079609\n",
      "batch: 120 loss: 0.01685628853738308\n",
      "batch: 130 loss: 0.014445237815380096\n",
      "batch: 140 loss: 0.013643370941281319\n",
      "batch: 150 loss: 0.023599298670887947\n",
      "batch: 160 loss: 0.018828054890036583\n",
      "batch: 170 loss: 0.017302436754107475\n",
      "batch: 180 loss: 0.016616376116871834\n",
      "batch: 190 loss: 0.016230463981628418\n",
      "batch: 200 loss: 0.02160274237394333\n",
      "batch: 210 loss: 0.016387971118092537\n",
      "batch: 220 loss: 0.016761913895606995\n",
      "batch: 230 loss: 0.018098365515470505\n",
      "batch: 240 loss: 0.017979145050048828\n",
      "batch: 250 loss: 0.019299643114209175\n",
      "batch: 260 loss: 0.019049398601055145\n",
      "batch: 270 loss: 0.01767234317958355\n",
      "batch: 280 loss: 0.019052838906645775\n",
      "batch: 290 loss: 0.01700916886329651\n",
      "batch: 300 loss: 0.018525686115026474\n",
      "batch: 310 loss: 0.01836835779249668\n",
      "batch: 320 loss: 0.01702144555747509\n",
      "batch: 330 loss: 0.01586551032960415\n",
      "batch: 340 loss: 0.013379354029893875\n",
      "batch: 350 loss: 0.018383583053946495\n",
      "batch: 360 loss: 0.014661737717688084\n",
      "batch: 370 loss: 0.020670846104621887\n",
      "batch: 380 loss: 0.019573112949728966\n",
      "batch: 390 loss: 0.019140707328915596\n",
      "batch: 400 loss: 0.016457173973321915\n",
      "batch: 410 loss: 0.013719741255044937\n",
      "batch: 420 loss: 0.016817806288599968\n",
      "batch: 430 loss: 0.013489732518792152\n",
      "batch: 440 loss: 0.016148898750543594\n",
      "batch: 450 loss: 0.012309041805565357\n",
      "batch: 460 loss: 0.016102084890007973\n",
      "batch: 470 loss: 0.012294361367821693\n",
      "batch: 480 loss: 0.013122253119945526\n",
      "batch: 490 loss: 0.017287088558077812\n",
      "batch: 500 loss: 0.013923260383307934\n",
      "batch: 510 loss: 0.011155459098517895\n",
      "batch: 520 loss: 0.017512734979391098\n",
      "batch: 530 loss: 0.01679571159183979\n",
      "batch: 540 loss: 0.010863284580409527\n",
      "batch: 550 loss: 0.01491504069417715\n",
      "batch: 560 loss: 0.016208743676543236\n",
      "batch: 570 loss: 0.017915762960910797\n",
      "batch: 580 loss: 0.01817978359758854\n",
      "batch: 590 loss: 0.016898663714528084\n",
      "batch: 600 loss: 0.016886811703443527\n",
      "batch: 610 loss: 0.019367925822734833\n",
      "batch: 620 loss: 0.013226816430687904\n",
      "batch: 630 loss: 0.012788192369043827\n",
      "batch: 640 loss: 0.013199697248637676\n",
      "batch: 650 loss: 0.01745494455099106\n",
      "batch: 660 loss: 0.01771330088376999\n",
      "batch: 670 loss: 0.015372724272310734\n",
      "batch: 680 loss: 0.01660165563225746\n",
      "batch: 690 loss: 0.01505151018500328\n",
      "batch: 700 loss: 0.016435785219073296\n",
      "batch: 710 loss: 0.017484107986092567\n",
      "batch: 720 loss: 0.017947377637028694\n",
      "batch: 730 loss: 0.017048222944140434\n",
      "batch: 740 loss: 0.017282798886299133\n",
      "batch: 750 loss: 0.01791919581592083\n",
      "batch: 760 loss: 0.01682954467833042\n",
      "batch: 770 loss: 0.02003367803990841\n",
      "batch: 780 loss: 0.026176059618592262\n",
      "batch: 790 loss: 0.020045699551701546\n",
      "batch: 800 loss: 0.014314492233097553\n",
      "batch: 810 loss: 0.01623888686299324\n",
      "batch: 820 loss: 0.01314583234488964\n",
      "batch: 830 loss: 0.018446018919348717\n",
      "batch: 840 loss: 0.019219618290662766\n",
      "batch: 850 loss: 0.01303750742226839\n",
      "batch: 860 loss: 0.014889417216181755\n",
      "batch: 870 loss: 0.016949588432908058\n",
      "batch: 880 loss: 0.01763463020324707\n",
      "batch: 890 loss: 0.01118980161845684\n",
      "batch: 900 loss: 0.014653130434453487\n",
      "batch: 910 loss: 0.015066496096551418\n",
      "batch: 920 loss: 0.01576569676399231\n",
      "batch: 930 loss: 0.016302749514579773\n",
      "batch: 940 loss: 0.015382622368633747\n",
      "batch: 950 loss: 0.016340097412467003\n",
      "batch: 960 loss: 0.015328196808695793\n",
      "batch: 970 loss: 0.017074111849069595\n",
      "batch: 980 loss: 0.01751021109521389\n",
      "batch: 990 loss: 0.01680433563888073\n",
      "batch: 1000 loss: 0.019756918773055077\n",
      "batch: 1010 loss: 0.016103774309158325\n",
      "batch: 1020 loss: 0.018580615520477295\n",
      "batch: 1030 loss: 0.014866659417748451\n",
      "batch: 1040 loss: 0.01613534800708294\n",
      "batch: 1050 loss: 0.01822792924940586\n",
      "batch: 1060 loss: 0.017860623076558113\n",
      "batch: 1070 loss: 0.01796192117035389\n",
      "batch: 1080 loss: 0.021683799102902412\n",
      "batch: 1090 loss: 0.013453846797347069\n",
      "batch: 1100 loss: 0.02019272930920124\n",
      "batch: 1110 loss: 0.015974245965480804\n",
      "batch: 1120 loss: 0.016250887885689735\n",
      "batch: 1130 loss: 0.017594706267118454\n",
      "batch: 1140 loss: 0.017492828890681267\n",
      "batch: 1150 loss: 0.014518842101097107\n",
      "batch: 1160 loss: 0.015106500126421452\n",
      "batch: 1170 loss: 0.020320972427725792\n",
      "batch: 1180 loss: 0.017890455201268196\n",
      "batch: 1190 loss: 0.01672999933362007\n",
      "batch: 1200 loss: 0.017525743693113327\n",
      "batch: 1210 loss: 0.019695181399583817\n",
      "batch: 1220 loss: 0.022038422524929047\n",
      "batch: 1230 loss: 0.011987728998064995\n",
      "batch: 1240 loss: 0.01608452759683132\n",
      "batch: 1250 loss: 0.016771353781223297\n",
      "batch: 1260 loss: 0.013859078288078308\n",
      "batch: 1270 loss: 0.020752834156155586\n",
      "batch: 1280 loss: 0.012895125895738602\n",
      "batch: 1290 loss: 0.017088230699300766\n",
      "batch: 1300 loss: 0.016799256205558777\n",
      "batch: 1310 loss: 0.015973703935742378\n",
      "batch: 1320 loss: 0.014462495222687721\n",
      "batch: 1330 loss: 0.016455290839076042\n",
      "batch: 1340 loss: 0.01813432388007641\n",
      "batch: 1350 loss: 0.017415549606084824\n",
      "batch: 1360 loss: 0.016501551494002342\n",
      "batch: 1370 loss: 0.017479050904512405\n",
      "batch: 1380 loss: 0.01435354445129633\n",
      "batch: 1390 loss: 0.016592595726251602\n",
      "batch: 1400 loss: 0.015727810561656952\n",
      "batch: 1410 loss: 0.015233277343213558\n",
      "batch: 1420 loss: 0.017839841544628143\n",
      "batch: 1430 loss: 0.017877599224448204\n",
      "batch: 1440 loss: 0.015868382528424263\n",
      "batch: 1450 loss: 0.017649386078119278\n",
      "batch: 1460 loss: 0.01569836214184761\n",
      "batch: 1470 loss: 0.021362006664276123\n",
      "batch: 1480 loss: 0.016397899016737938\n",
      "batch: 1490 loss: 0.016912667080760002\n",
      "batch: 1500 loss: 0.016600538045167923\n",
      "batch: 1510 loss: 0.015910563990473747\n",
      "batch: 1520 loss: 0.013929777778685093\n",
      "batch: 1530 loss: 0.01665388233959675\n",
      "batch: 1540 loss: 0.017180122435092926\n",
      "batch: 1550 loss: 0.019221631810069084\n",
      "batch: 1560 loss: 0.017738720402121544\n",
      "batch: 1570 loss: 0.016482148319482803\n",
      "batch: 1580 loss: 0.016231806948781013\n",
      "batch: 1590 loss: 0.01713750883936882\n",
      "batch: 1600 loss: 0.0187007375061512\n",
      "batch: 1610 loss: 0.019704200327396393\n",
      "batch: 1620 loss: 0.015956813469529152\n",
      "batch: 1630 loss: 0.014863366261124611\n",
      "batch: 1640 loss: 0.01988850347697735\n",
      "batch: 1650 loss: 0.020910371094942093\n",
      "batch: 1660 loss: 0.01688120700418949\n",
      "batch: 1670 loss: 0.015592094510793686\n",
      "batch: 1680 loss: 0.01622900180518627\n",
      "batch: 1690 loss: 0.01675252988934517\n",
      "batch: 1700 loss: 0.018079757690429688\n",
      "batch: 1710 loss: 0.021173527464270592\n",
      "batch: 1720 loss: 0.011544476263225079\n",
      "batch: 1730 loss: 0.012963153421878815\n",
      "batch: 1740 loss: 0.016308225691318512\n",
      "batch: 1750 loss: 0.015126131474971771\n",
      "batch: 1760 loss: 0.016910908743739128\n",
      "batch: 1770 loss: 0.017991915345191956\n",
      "batch: 1780 loss: 0.015644477680325508\n",
      "batch: 1790 loss: 0.015073352493345737\n",
      "batch: 1800 loss: 0.017423337325453758\n",
      "batch: 1810 loss: 0.020228998735547066\n",
      "batch: 1820 loss: 0.012263884767889977\n",
      "batch: 1830 loss: 0.017146816477179527\n",
      "batch: 1840 loss: 0.017406927421689034\n",
      "batch: 1850 loss: 0.01547999307513237\n",
      "batch: 1860 loss: 0.014995422214269638\n",
      "batch: 1870 loss: 0.01989377662539482\n",
      "batch: 1880 loss: 0.015521930530667305\n",
      "batch: 1890 loss: 0.017169512808322906\n",
      "batch: 1900 loss: 0.01676875166594982\n",
      "batch: 1910 loss: 0.017641941085457802\n",
      "batch: 1920 loss: 0.018322080373764038\n",
      "batch: 1930 loss: 0.01302273292094469\n",
      "batch: 1940 loss: 0.015941228717565536\n",
      "batch: 1950 loss: 0.016159964725375175\n",
      "batch: 1960 loss: 0.012631021440029144\n",
      "batch: 1970 loss: 0.019838551059365273\n",
      "batch: 1980 loss: 0.01812179572880268\n",
      "batch: 1990 loss: 0.016305197030305862\n",
      "batch: 2000 loss: 0.014485659077763557\n",
      "batch: 2010 loss: 0.016812114045023918\n",
      "batch: 2020 loss: 0.018532494083046913\n",
      "batch: 2030 loss: 0.014535353519022465\n",
      "batch: 2040 loss: 0.012181374244391918\n",
      "batch: 2050 loss: 0.01791023649275303\n",
      "batch: 2060 loss: 0.01975380629301071\n",
      "batch: 2070 loss: 0.016935868188738823\n",
      "batch: 2080 loss: 0.01779208332300186\n",
      "batch: 2090 loss: 0.017262475565075874\n",
      "batch: 2100 loss: 0.015757955610752106\n",
      "batch: 2110 loss: 0.016196783632040024\n",
      "batch: 2120 loss: 0.016440225765109062\n",
      "batch: 2130 loss: 0.01727362722158432\n",
      "batch: 2140 loss: 0.015467198565602303\n",
      "batch: 2150 loss: 0.02144678868353367\n",
      "batch: 2160 loss: 0.01707940362393856\n",
      "batch: 2170 loss: 0.0164308063685894\n",
      "batch: 2180 loss: 0.016007399186491966\n",
      "batch: 2190 loss: 0.013718541711568832\n",
      "batch: 2200 loss: 0.016835350543260574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2210 loss: 0.015175368636846542\n",
      "batch: 2220 loss: 0.014620134606957436\n",
      "batch: 2230 loss: 0.01737799495458603\n",
      "batch: 2240 loss: 0.02104843594133854\n",
      "batch: 2250 loss: 0.016946010291576385\n",
      "batch: 2260 loss: 0.015840735286474228\n",
      "batch: 2270 loss: 0.017312828451395035\n",
      "batch: 2280 loss: 0.012476161122322083\n",
      "batch: 2290 loss: 0.013323957100510597\n",
      "batch: 2300 loss: 0.015178431756794453\n",
      "batch: 2310 loss: 0.013462268747389317\n",
      "batch: 2320 loss: 0.016936374828219414\n",
      "batch: 2330 loss: 0.018903467804193497\n",
      "batch: 2340 loss: 0.017879266291856766\n",
      "batch: 2350 loss: 0.01725940965116024\n",
      "batch: 2360 loss: 0.01978001371026039\n",
      "batch: 2370 loss: 0.014345118775963783\n",
      "batch: 2380 loss: 0.015269451774656773\n",
      "batch: 2390 loss: 0.013067460618913174\n",
      "batch: 2400 loss: 0.01882164366543293\n",
      "batch: 2410 loss: 0.019355088472366333\n",
      "batch: 2420 loss: 0.01569724828004837\n",
      "batch: 2430 loss: 0.017927544191479683\n",
      "batch: 2440 loss: 0.016842644661664963\n",
      "batch: 2450 loss: 0.013564600609242916\n",
      "batch: 2460 loss: 0.021358709782361984\n",
      "batch: 2470 loss: 0.015399529598653316\n",
      "batch: 2480 loss: 0.01740015298128128\n",
      "batch: 2490 loss: 0.021103063598275185\n",
      "batch: 2500 loss: 0.019963063299655914\n",
      "batch: 2510 loss: 0.019961046054959297\n",
      "batch: 2520 loss: 0.021810535341501236\n",
      "batch: 2530 loss: 0.015762468799948692\n",
      "batch: 2540 loss: 0.020268606022000313\n",
      "batch: 2550 loss: 0.01643073558807373\n",
      "batch: 2560 loss: 0.01707087643444538\n",
      "batch: 2570 loss: 0.014034133404493332\n",
      "batch: 2580 loss: 0.014636346139013767\n",
      "batch: 2590 loss: 0.016768604516983032\n",
      "batch: 2600 loss: 0.016653727740049362\n",
      "batch: 2610 loss: 0.018666591495275497\n",
      "batch: 2620 loss: 0.011801963672041893\n",
      "batch: 2630 loss: 0.01622036285698414\n",
      "batch: 2640 loss: 0.015492962673306465\n",
      "batch: 2650 loss: 0.016086002811789513\n",
      "batch: 2660 loss: 0.01379019021987915\n",
      "batch: 2670 loss: 0.014987492002546787\n",
      "batch: 2680 loss: 0.016759108752012253\n",
      "batch: 2690 loss: 0.017936309799551964\n",
      "batch: 2700 loss: 0.019589286297559738\n",
      "batch: 2710 loss: 0.014698794111609459\n",
      "batch: 2720 loss: 0.018565448001027107\n",
      "batch: 2730 loss: 0.013112654909491539\n",
      "batch: 2740 loss: 0.01696927472949028\n",
      "batch: 2750 loss: 0.013423864729702473\n",
      "batch: 2760 loss: 0.017791152000427246\n",
      "batch: 2770 loss: 0.021242940798401833\n",
      "batch: 2780 loss: 0.015048633329570293\n",
      "batch: 2790 loss: 0.016679013147950172\n",
      "batch: 2800 loss: 0.018440864980220795\n",
      "batch: 2810 loss: 0.01620536856353283\n",
      "batch: 2820 loss: 0.014268014580011368\n",
      "batch: 2830 loss: 0.01203505601733923\n",
      "batch: 2840 loss: 0.01880517229437828\n",
      "batch: 2850 loss: 0.01565871573984623\n",
      "batch: 2860 loss: 0.01658107154071331\n",
      "round  4 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.023372311145067215\n",
      "batch: 10 loss: 0.01431973185390234\n",
      "batch: 20 loss: 0.016200492158532143\n",
      "batch: 30 loss: 0.018809905275702477\n",
      "batch: 40 loss: 0.016910281032323837\n",
      "batch: 50 loss: 0.013079792261123657\n",
      "batch: 60 loss: 0.01919947750866413\n",
      "batch: 70 loss: 0.017127668485045433\n",
      "batch: 80 loss: 0.010629326105117798\n",
      "batch: 90 loss: 0.016705526039004326\n",
      "batch: 100 loss: 0.017781313508749008\n",
      "batch: 110 loss: 0.017807073891162872\n",
      "batch: 120 loss: 0.01409501489251852\n",
      "batch: 130 loss: 0.018218107521533966\n",
      "batch: 140 loss: 0.013855570927262306\n",
      "batch: 150 loss: 0.02284327521920204\n",
      "batch: 160 loss: 0.022303428500890732\n",
      "batch: 170 loss: 0.01666291058063507\n",
      "batch: 180 loss: 0.014903223142027855\n",
      "batch: 190 loss: 0.0252530537545681\n",
      "batch: 200 loss: 0.014578461647033691\n",
      "batch: 210 loss: 0.018575724214315414\n",
      "batch: 220 loss: 0.0194294061511755\n",
      "batch: 230 loss: 0.02015451341867447\n",
      "batch: 240 loss: 0.014458827674388885\n",
      "batch: 250 loss: 0.021462246775627136\n",
      "batch: 260 loss: 0.015425954014062881\n",
      "batch: 270 loss: 0.016149338334798813\n",
      "batch: 280 loss: 0.019691815599799156\n",
      "batch: 290 loss: 0.01702112890779972\n",
      "batch: 300 loss: 0.014587048441171646\n",
      "batch: 310 loss: 0.012608401477336884\n",
      "batch: 320 loss: 0.019327793270349503\n",
      "batch: 330 loss: 0.014149526134133339\n",
      "batch: 340 loss: 0.018923072144389153\n",
      "batch: 350 loss: 0.015201836824417114\n",
      "batch: 360 loss: 0.01951558329164982\n",
      "batch: 370 loss: 0.015936141833662987\n",
      "batch: 380 loss: 0.014825875870883465\n",
      "batch: 390 loss: 0.015855317935347557\n",
      "batch: 400 loss: 0.012950196862220764\n",
      "batch: 410 loss: 0.013630635105073452\n",
      "batch: 420 loss: 0.01798366755247116\n",
      "batch: 430 loss: 0.018465977162122726\n",
      "batch: 440 loss: 0.014989163726568222\n",
      "batch: 450 loss: 0.01789158396422863\n",
      "batch: 460 loss: 0.015406180173158646\n",
      "batch: 470 loss: 0.019820036366581917\n",
      "batch: 480 loss: 0.014137428253889084\n",
      "batch: 490 loss: 0.013288555666804314\n",
      "batch: 500 loss: 0.016461919993162155\n",
      "batch: 510 loss: 0.018558211624622345\n",
      "batch: 520 loss: 0.013793205842375755\n",
      "batch: 530 loss: 0.018572160974144936\n",
      "batch: 540 loss: 0.01649322733283043\n",
      "batch: 550 loss: 0.0145642701536417\n",
      "batch: 560 loss: 0.017105335369706154\n",
      "batch: 570 loss: 0.015866100788116455\n",
      "batch: 580 loss: 0.02146214433014393\n",
      "batch: 590 loss: 0.0164595115929842\n",
      "batch: 600 loss: 0.01990286447107792\n",
      "batch: 610 loss: 0.022011633962392807\n",
      "batch: 620 loss: 0.016212839633226395\n",
      "batch: 630 loss: 0.013162031769752502\n",
      "batch: 640 loss: 0.02147633768618107\n",
      "batch: 650 loss: 0.011951726861298084\n",
      "batch: 660 loss: 0.020096920430660248\n",
      "batch: 670 loss: 0.015480532310903072\n",
      "batch: 680 loss: 0.01881089061498642\n",
      "batch: 690 loss: 0.015798300504684448\n",
      "batch: 700 loss: 0.017472874373197556\n",
      "batch: 710 loss: 0.018468787893652916\n",
      "batch: 720 loss: 0.01736939139664173\n",
      "batch: 730 loss: 0.019062260165810585\n",
      "batch: 740 loss: 0.01663075014948845\n",
      "batch: 750 loss: 0.01633298210799694\n",
      "batch: 760 loss: 0.017648281529545784\n",
      "batch: 770 loss: 0.015543624758720398\n",
      "batch: 780 loss: 0.01769835129380226\n",
      "batch: 790 loss: 0.015381615608930588\n",
      "batch: 800 loss: 0.015373192727565765\n",
      "batch: 810 loss: 0.019576361402869225\n",
      "batch: 820 loss: 0.014918230473995209\n",
      "batch: 830 loss: 0.017877789214253426\n",
      "batch: 840 loss: 0.014530417509377003\n",
      "batch: 850 loss: 0.015701910480856895\n",
      "batch: 860 loss: 0.016688864678144455\n",
      "batch: 870 loss: 0.01998855173587799\n",
      "batch: 880 loss: 0.020963838323950768\n",
      "batch: 890 loss: 0.01483161747455597\n",
      "batch: 900 loss: 0.01634633168578148\n",
      "batch: 910 loss: 0.0175197571516037\n",
      "batch: 920 loss: 0.01453065499663353\n",
      "batch: 930 loss: 0.01944422349333763\n",
      "batch: 940 loss: 0.02049488015472889\n",
      "batch: 950 loss: 0.014126035384833813\n",
      "batch: 960 loss: 0.011579163372516632\n",
      "batch: 970 loss: 0.01759374514222145\n",
      "batch: 980 loss: 0.013889270834624767\n",
      "batch: 990 loss: 0.02034422941505909\n",
      "batch: 1000 loss: 0.01692323386669159\n",
      "batch: 1010 loss: 0.013900162652134895\n",
      "batch: 1020 loss: 0.014335722662508488\n",
      "batch: 1030 loss: 0.02011619508266449\n",
      "batch: 1040 loss: 0.017872558906674385\n",
      "batch: 1050 loss: 0.013152414001524448\n",
      "batch: 1060 loss: 0.017022160813212395\n",
      "batch: 1070 loss: 0.01688171736896038\n",
      "batch: 1080 loss: 0.01835830882191658\n",
      "batch: 1090 loss: 0.012569625861942768\n",
      "batch: 1100 loss: 0.015564704313874245\n",
      "batch: 1110 loss: 0.015893597155809402\n",
      "batch: 1120 loss: 0.016964102163910866\n",
      "batch: 1130 loss: 0.021751966327428818\n",
      "batch: 1140 loss: 0.01815219596028328\n",
      "batch: 1150 loss: 0.013171857222914696\n",
      "batch: 1160 loss: 0.018621776252985\n",
      "batch: 1170 loss: 0.011404808610677719\n",
      "batch: 1180 loss: 0.016441158950328827\n",
      "batch: 1190 loss: 0.01584787294268608\n",
      "batch: 1200 loss: 0.018239473924040794\n",
      "batch: 1210 loss: 0.015896812081336975\n",
      "batch: 1220 loss: 0.018920989707112312\n",
      "batch: 1230 loss: 0.012656329199671745\n",
      "batch: 1240 loss: 0.013245509006083012\n",
      "batch: 1250 loss: 0.01907430775463581\n",
      "batch: 1260 loss: 0.013200384564697742\n",
      "batch: 1270 loss: 0.014607620425522327\n",
      "batch: 1280 loss: 0.018279708921909332\n",
      "batch: 1290 loss: 0.021993597969412804\n",
      "batch: 1300 loss: 0.012732820585370064\n",
      "batch: 1310 loss: 0.01355261355638504\n",
      "batch: 1320 loss: 0.015456506982445717\n",
      "batch: 1330 loss: 0.01926260255277157\n",
      "batch: 1340 loss: 0.013790716417133808\n",
      "batch: 1350 loss: 0.015508628450334072\n",
      "batch: 1360 loss: 0.018938593566417694\n",
      "batch: 1370 loss: 0.017273692414164543\n",
      "batch: 1380 loss: 0.013718013651669025\n",
      "batch: 1390 loss: 0.013085081242024899\n",
      "batch: 1400 loss: 0.011081630364060402\n",
      "batch: 1410 loss: 0.018656564876437187\n",
      "batch: 1420 loss: 0.012546003796160221\n",
      "batch: 1430 loss: 0.015366964042186737\n",
      "batch: 1440 loss: 0.014431608840823174\n",
      "batch: 1450 loss: 0.014465663582086563\n",
      "batch: 1460 loss: 0.017529701814055443\n",
      "batch: 1470 loss: 0.017591651529073715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1480 loss: 0.01850772835314274\n",
      "batch: 1490 loss: 0.015947513282299042\n",
      "batch: 1500 loss: 0.019344042986631393\n",
      "batch: 1510 loss: 0.017567699775099754\n",
      "batch: 1520 loss: 0.01533532701432705\n",
      "batch: 1530 loss: 0.016276685521006584\n",
      "batch: 1540 loss: 0.014069333672523499\n",
      "batch: 1550 loss: 0.012829023413360119\n",
      "batch: 1560 loss: 0.014083978720009327\n",
      "batch: 1570 loss: 0.016194671392440796\n",
      "batch: 1580 loss: 0.01639694534242153\n",
      "batch: 1590 loss: 0.01802670583128929\n",
      "batch: 1600 loss: 0.01220044493675232\n",
      "batch: 1610 loss: 0.012832310982048512\n",
      "batch: 1620 loss: 0.01565229892730713\n",
      "batch: 1630 loss: 0.019156204536557198\n",
      "batch: 1640 loss: 0.015272269025444984\n",
      "batch: 1650 loss: 0.016864802688360214\n",
      "batch: 1660 loss: 0.013930016197264194\n",
      "batch: 1670 loss: 0.012256458401679993\n",
      "batch: 1680 loss: 0.017650220543146133\n",
      "batch: 1690 loss: 0.012999361380934715\n",
      "batch: 1700 loss: 0.015745719894766808\n",
      "batch: 1710 loss: 0.021734176203608513\n",
      "batch: 1720 loss: 0.017558204010128975\n",
      "batch: 1730 loss: 0.01887432485818863\n",
      "batch: 1740 loss: 0.014687263406813145\n",
      "batch: 1750 loss: 0.013844236731529236\n",
      "batch: 1760 loss: 0.012860037386417389\n",
      "batch: 1770 loss: 0.022227518260478973\n",
      "batch: 1780 loss: 0.01527799479663372\n",
      "batch: 1790 loss: 0.01168861985206604\n",
      "batch: 1800 loss: 0.01577756181359291\n",
      "batch: 1810 loss: 0.016727309674024582\n",
      "batch: 1820 loss: 0.011500977911055088\n",
      "batch: 1830 loss: 0.013435116969048977\n",
      "batch: 1840 loss: 0.014982299879193306\n",
      "batch: 1850 loss: 0.014636668376624584\n",
      "batch: 1860 loss: 0.010855289176106453\n",
      "batch: 1870 loss: 0.018733179196715355\n",
      "batch: 1880 loss: 0.019112616777420044\n",
      "batch: 1890 loss: 0.01931610144674778\n",
      "batch: 1900 loss: 0.014288448728621006\n",
      "batch: 1910 loss: 0.014269411563873291\n",
      "batch: 1920 loss: 0.015253465622663498\n",
      "batch: 1930 loss: 0.015625927597284317\n",
      "batch: 1940 loss: 0.018322160467505455\n",
      "batch: 1950 loss: 0.016251031309366226\n",
      "batch: 1960 loss: 0.014218091033399105\n",
      "batch: 1970 loss: 0.01804320141673088\n",
      "batch: 1980 loss: 0.013240615837275982\n",
      "batch: 1990 loss: 0.016608038917183876\n",
      "batch: 2000 loss: 0.017865410074591637\n",
      "batch: 2010 loss: 0.013194958679378033\n",
      "batch: 2020 loss: 0.01765407994389534\n",
      "batch: 2030 loss: 0.01683187112212181\n",
      "batch: 2040 loss: 0.01791369915008545\n",
      "batch: 2050 loss: 0.014706629328429699\n",
      "batch: 2060 loss: 0.02123294584453106\n",
      "batch: 2070 loss: 0.012867067009210587\n",
      "batch: 2080 loss: 0.016302434727549553\n",
      "batch: 2090 loss: 0.01926429197192192\n",
      "batch: 2100 loss: 0.014850575476884842\n",
      "batch: 2110 loss: 0.016677401959896088\n",
      "batch: 2120 loss: 0.015333314426243305\n",
      "batch: 2130 loss: 0.014151407405734062\n",
      "batch: 2140 loss: 0.015296448953449726\n",
      "batch: 2150 loss: 0.017199311405420303\n",
      "batch: 2160 loss: 0.016166653484106064\n",
      "batch: 2170 loss: 0.018543634563684464\n",
      "batch: 2180 loss: 0.014641483314335346\n",
      "batch: 2190 loss: 0.01924615353345871\n",
      "batch: 2200 loss: 0.01738286204636097\n",
      "batch: 2210 loss: 0.01729707047343254\n",
      "batch: 2220 loss: 0.021358799189329147\n",
      "batch: 2230 loss: 0.015854977071285248\n",
      "batch: 2240 loss: 0.015903545543551445\n",
      "batch: 2250 loss: 0.014998313039541245\n",
      "batch: 2260 loss: 0.016310546547174454\n",
      "batch: 2270 loss: 0.02069828473031521\n",
      "batch: 2280 loss: 0.014231451787054539\n",
      "batch: 2290 loss: 0.01920144073665142\n",
      "batch: 2300 loss: 0.0161734651774168\n",
      "batch: 2310 loss: 0.012510533444583416\n",
      "batch: 2320 loss: 0.019377266988158226\n",
      "batch: 2330 loss: 0.01799553632736206\n",
      "batch: 2340 loss: 0.01241325680166483\n",
      "batch: 2350 loss: 0.0197012759745121\n",
      "batch: 2360 loss: 0.012250768952071667\n",
      "batch: 2370 loss: 0.014780751429498196\n",
      "batch: 2380 loss: 0.015151104889810085\n",
      "batch: 2390 loss: 0.017615562304854393\n",
      "batch: 2400 loss: 0.01818072609603405\n",
      "batch: 2410 loss: 0.0118633434176445\n",
      "batch: 2420 loss: 0.016302544623613358\n",
      "batch: 2430 loss: 0.012410216964781284\n",
      "batch: 2440 loss: 0.021383723244071007\n",
      "batch: 2450 loss: 0.017494071274995804\n",
      "batch: 2460 loss: 0.01918463408946991\n",
      "batch: 2470 loss: 0.012343424372375011\n",
      "batch: 2480 loss: 0.01450243592262268\n",
      "batch: 2490 loss: 0.015274721197783947\n",
      "batch: 2500 loss: 0.01904500648379326\n",
      "batch: 2510 loss: 0.015166079625487328\n",
      "batch: 2520 loss: 0.019985461607575417\n",
      "batch: 2530 loss: 0.01964045874774456\n",
      "batch: 2540 loss: 0.014985007233917713\n",
      "batch: 2550 loss: 0.016552947461605072\n",
      "batch: 2560 loss: 0.01647404581308365\n",
      "batch: 2570 loss: 0.015936413779854774\n",
      "batch: 2580 loss: 0.013774843886494637\n",
      "batch: 2590 loss: 0.01883508451282978\n",
      "batch: 2600 loss: 0.019486621022224426\n",
      "batch: 2610 loss: 0.01568339392542839\n",
      "batch: 2620 loss: 0.015566229820251465\n",
      "batch: 2630 loss: 0.016503730788826942\n",
      "batch: 2640 loss: 0.018940677866339684\n",
      "batch: 2650 loss: 0.017893917858600616\n",
      "batch: 2660 loss: 0.012163201346993446\n",
      "batch: 2670 loss: 0.012567877769470215\n",
      "batch: 2680 loss: 0.016117043793201447\n",
      "batch: 2690 loss: 0.0149952108040452\n",
      "batch: 2700 loss: 0.013498371466994286\n",
      "batch: 2710 loss: 0.012709067203104496\n",
      "batch: 2720 loss: 0.016573019325733185\n",
      "batch: 2730 loss: 0.019650621339678764\n",
      "batch: 2740 loss: 0.011587346903979778\n",
      "batch: 2750 loss: 0.016517948359251022\n",
      "batch: 2760 loss: 0.018595853820443153\n",
      "batch: 2770 loss: 0.016482286155223846\n",
      "batch: 2780 loss: 0.024569392204284668\n",
      "batch: 2790 loss: 0.02031347155570984\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.01685727946460247\n",
      "batch: 10 loss: 0.015027131885290146\n",
      "batch: 20 loss: 0.016602572053670883\n",
      "batch: 30 loss: 0.012918036431074142\n",
      "batch: 40 loss: 0.01546483114361763\n",
      "batch: 50 loss: 0.01357545517385006\n",
      "batch: 60 loss: 0.01576276868581772\n",
      "batch: 70 loss: 0.016719691455364227\n",
      "batch: 80 loss: 0.015653355047106743\n",
      "batch: 90 loss: 0.020727580413222313\n",
      "batch: 100 loss: 0.014876322820782661\n",
      "batch: 110 loss: 0.020482352003455162\n",
      "batch: 120 loss: 0.015195879153907299\n",
      "batch: 130 loss: 0.011254754848778248\n",
      "batch: 140 loss: 0.016149569302797318\n",
      "batch: 150 loss: 0.015799643471837044\n",
      "batch: 160 loss: 0.019419070333242416\n",
      "batch: 170 loss: 0.016224689781665802\n",
      "batch: 180 loss: 0.014434708282351494\n",
      "batch: 190 loss: 0.014280017465353012\n",
      "batch: 200 loss: 0.019550343975424767\n",
      "batch: 210 loss: 0.015408631414175034\n",
      "batch: 220 loss: 0.012049944140017033\n",
      "batch: 230 loss: 0.015192849561572075\n",
      "batch: 240 loss: 0.013554271310567856\n",
      "batch: 250 loss: 0.01390905398875475\n",
      "batch: 260 loss: 0.016709383577108383\n",
      "batch: 270 loss: 0.01593158207833767\n",
      "batch: 280 loss: 0.016136981546878815\n",
      "batch: 290 loss: 0.009921805933117867\n",
      "batch: 300 loss: 0.01274008210748434\n",
      "batch: 310 loss: 0.015119282528758049\n",
      "batch: 320 loss: 0.019020579755306244\n",
      "batch: 330 loss: 0.015470385551452637\n",
      "batch: 340 loss: 0.013073025271296501\n",
      "batch: 350 loss: 0.015309893526136875\n",
      "batch: 360 loss: 0.01311701349914074\n",
      "batch: 370 loss: 0.016594095155596733\n",
      "batch: 380 loss: 0.012838375754654408\n",
      "batch: 390 loss: 0.011089314706623554\n",
      "batch: 400 loss: 0.015205103904008865\n",
      "batch: 410 loss: 0.012261757627129555\n",
      "batch: 420 loss: 0.01522068027406931\n",
      "batch: 430 loss: 0.015754278749227524\n",
      "batch: 440 loss: 0.016478754580020905\n",
      "batch: 450 loss: 0.01627042330801487\n",
      "batch: 460 loss: 0.013894114643335342\n",
      "batch: 470 loss: 0.011749167926609516\n",
      "batch: 480 loss: 0.01391823310405016\n",
      "batch: 490 loss: 0.01574132591485977\n",
      "batch: 500 loss: 0.014583641663193703\n",
      "batch: 510 loss: 0.014771407470107079\n",
      "batch: 520 loss: 0.01764635182917118\n",
      "batch: 530 loss: 0.014805939979851246\n",
      "batch: 540 loss: 0.015138695016503334\n",
      "batch: 550 loss: 0.013038738630712032\n",
      "batch: 560 loss: 0.01825626567006111\n",
      "batch: 570 loss: 0.01610090397298336\n",
      "batch: 580 loss: 0.0146416574716568\n",
      "batch: 590 loss: 0.013979070819914341\n",
      "batch: 600 loss: 0.01934679038822651\n",
      "batch: 610 loss: 0.016155647113919258\n",
      "batch: 620 loss: 0.015486261807382107\n",
      "batch: 630 loss: 0.013788026757538319\n",
      "batch: 640 loss: 0.01612658053636551\n",
      "batch: 650 loss: 0.01580076664686203\n",
      "batch: 660 loss: 0.012381020002067089\n",
      "batch: 670 loss: 0.014046399854123592\n",
      "batch: 680 loss: 0.017255228012800217\n",
      "batch: 690 loss: 0.01493220217525959\n",
      "batch: 700 loss: 0.01838270202279091\n",
      "batch: 710 loss: 0.015957975760102272\n",
      "batch: 720 loss: 0.012543822638690472\n",
      "batch: 730 loss: 0.015855461359024048\n",
      "batch: 740 loss: 0.013983945362269878\n",
      "batch: 750 loss: 0.015617377124726772\n",
      "batch: 760 loss: 0.01603982038795948\n",
      "batch: 770 loss: 0.018757451325654984\n",
      "batch: 780 loss: 0.015767890959978104\n",
      "batch: 790 loss: 0.016343634575605392\n",
      "batch: 800 loss: 0.014416015706956387\n",
      "batch: 810 loss: 0.016160037368535995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 820 loss: 0.014404493384063244\n",
      "batch: 830 loss: 0.017602112144231796\n",
      "batch: 840 loss: 0.016042236238718033\n",
      "batch: 850 loss: 0.015609188936650753\n",
      "batch: 860 loss: 0.017100384458899498\n",
      "batch: 870 loss: 0.011116565205156803\n",
      "batch: 880 loss: 0.018918026238679886\n",
      "batch: 890 loss: 0.017832692712545395\n",
      "batch: 900 loss: 0.012179912067949772\n",
      "batch: 910 loss: 0.013004698790609837\n",
      "batch: 920 loss: 0.014878121204674244\n",
      "batch: 930 loss: 0.016977664083242416\n",
      "batch: 940 loss: 0.014698432758450508\n",
      "batch: 950 loss: 0.012912203557789326\n",
      "batch: 960 loss: 0.015067406930029392\n",
      "batch: 970 loss: 0.018358737230300903\n",
      "batch: 980 loss: 0.017049720510840416\n",
      "batch: 990 loss: 0.015210086479783058\n",
      "batch: 1000 loss: 0.017795145511627197\n",
      "batch: 1010 loss: 0.016806047409772873\n",
      "batch: 1020 loss: 0.013119790703058243\n",
      "batch: 1030 loss: 0.024279700592160225\n",
      "batch: 1040 loss: 0.018697479739785194\n",
      "batch: 1050 loss: 0.01875651814043522\n",
      "batch: 1060 loss: 0.014932517893612385\n",
      "batch: 1070 loss: 0.015439130365848541\n",
      "batch: 1080 loss: 0.011424349620938301\n",
      "batch: 1090 loss: 0.01991467922925949\n",
      "batch: 1100 loss: 0.01680063083767891\n",
      "batch: 1110 loss: 0.018697505816817284\n",
      "batch: 1120 loss: 0.015402523800730705\n",
      "batch: 1130 loss: 0.01335157360881567\n",
      "batch: 1140 loss: 0.015065601095557213\n",
      "batch: 1150 loss: 0.018379390239715576\n",
      "batch: 1160 loss: 0.018513163551688194\n",
      "batch: 1170 loss: 0.01724967733025551\n",
      "batch: 1180 loss: 0.018764235079288483\n",
      "batch: 1190 loss: 0.019866950809955597\n",
      "batch: 1200 loss: 0.017634034156799316\n",
      "batch: 1210 loss: 0.014385231770575047\n",
      "batch: 1220 loss: 0.014868558384478092\n",
      "batch: 1230 loss: 0.016926074400544167\n",
      "batch: 1240 loss: 0.011521617881953716\n",
      "batch: 1250 loss: 0.013278570957481861\n",
      "batch: 1260 loss: 0.015570823103189468\n",
      "batch: 1270 loss: 0.013486197218298912\n",
      "batch: 1280 loss: 0.013324477709829807\n",
      "batch: 1290 loss: 0.01466427743434906\n",
      "batch: 1300 loss: 0.017736060544848442\n",
      "batch: 1310 loss: 0.014026246964931488\n",
      "batch: 1320 loss: 0.014889071695506573\n",
      "batch: 1330 loss: 0.01704326458275318\n",
      "batch: 1340 loss: 0.019333666190505028\n",
      "batch: 1350 loss: 0.017838532105088234\n",
      "batch: 1360 loss: 0.01409698836505413\n",
      "batch: 1370 loss: 0.019729133695364\n",
      "batch: 1380 loss: 0.01203968282788992\n",
      "batch: 1390 loss: 0.017954222857952118\n",
      "batch: 1400 loss: 0.018695782870054245\n",
      "batch: 1410 loss: 0.015176799148321152\n",
      "batch: 1420 loss: 0.017273321747779846\n",
      "batch: 1430 loss: 0.017048534005880356\n",
      "batch: 1440 loss: 0.0175796989351511\n",
      "batch: 1450 loss: 0.01788727380335331\n",
      "batch: 1460 loss: 0.019307253882288933\n",
      "batch: 1470 loss: 0.016697416082024574\n",
      "batch: 1480 loss: 0.016003627330064774\n",
      "batch: 1490 loss: 0.013763973489403725\n",
      "batch: 1500 loss: 0.0123880784958601\n",
      "batch: 1510 loss: 0.013845930807292461\n",
      "batch: 1520 loss: 0.016418196260929108\n",
      "batch: 1530 loss: 0.01215249951928854\n",
      "batch: 1540 loss: 0.022006966173648834\n",
      "batch: 1550 loss: 0.017592458054423332\n",
      "batch: 1560 loss: 0.015584543347358704\n",
      "batch: 1570 loss: 0.015154337510466576\n",
      "batch: 1580 loss: 0.013741814531385899\n",
      "batch: 1590 loss: 0.01182093471288681\n",
      "batch: 1600 loss: 0.014814340509474277\n",
      "batch: 1610 loss: 0.0139654241502285\n",
      "batch: 1620 loss: 0.01588418148458004\n",
      "batch: 1630 loss: 0.016288943588733673\n",
      "batch: 1640 loss: 0.015412199310958385\n",
      "batch: 1650 loss: 0.021452220156788826\n",
      "batch: 1660 loss: 0.01595155894756317\n",
      "batch: 1670 loss: 0.01718723587691784\n",
      "batch: 1680 loss: 0.014428040012717247\n",
      "batch: 1690 loss: 0.010246512480080128\n",
      "batch: 1700 loss: 0.015153758227825165\n",
      "batch: 1710 loss: 0.013390637002885342\n",
      "batch: 1720 loss: 0.01686261221766472\n",
      "batch: 1730 loss: 0.01347139198333025\n",
      "batch: 1740 loss: 0.012132990173995495\n",
      "batch: 1750 loss: 0.014124859124422073\n",
      "batch: 1760 loss: 0.014943823218345642\n",
      "batch: 1770 loss: 0.014627822674810886\n",
      "batch: 1780 loss: 0.015820473432540894\n",
      "batch: 1790 loss: 0.016120653599500656\n",
      "batch: 1800 loss: 0.017418457195162773\n",
      "batch: 1810 loss: 0.017473910003900528\n",
      "batch: 1820 loss: 0.015758352354168892\n",
      "batch: 1830 loss: 0.015363853424787521\n",
      "batch: 1840 loss: 0.016902148723602295\n",
      "batch: 1850 loss: 0.015336886048316956\n",
      "batch: 1860 loss: 0.01631966046988964\n",
      "batch: 1870 loss: 0.013189462013542652\n",
      "batch: 1880 loss: 0.01660626381635666\n",
      "batch: 1890 loss: 0.012171151116490364\n",
      "batch: 1900 loss: 0.017264997586607933\n",
      "batch: 1910 loss: 0.016832714900374413\n",
      "batch: 1920 loss: 0.015341113321483135\n",
      "batch: 1930 loss: 0.014042474329471588\n",
      "batch: 1940 loss: 0.02305326797068119\n",
      "batch: 1950 loss: 0.020611772313714027\n",
      "batch: 1960 loss: 0.01591568998992443\n",
      "batch: 1970 loss: 0.0157406534999609\n",
      "batch: 1980 loss: 0.014020964503288269\n",
      "batch: 1990 loss: 0.01512723695486784\n",
      "batch: 2000 loss: 0.019040096551179886\n",
      "batch: 2010 loss: 0.02479981817305088\n",
      "batch: 2020 loss: 0.015790918841958046\n",
      "batch: 2030 loss: 0.020937925204634666\n",
      "batch: 2040 loss: 0.016197754070162773\n",
      "batch: 2050 loss: 0.015348059125244617\n",
      "batch: 2060 loss: 0.01493518054485321\n",
      "batch: 2070 loss: 0.012889315374195576\n",
      "batch: 2080 loss: 0.02178991213440895\n",
      "batch: 2090 loss: 0.017215726897120476\n",
      "batch: 2100 loss: 0.013689473271369934\n",
      "batch: 2110 loss: 0.01443940307945013\n",
      "batch: 2120 loss: 0.013518985360860825\n",
      "batch: 2130 loss: 0.016167012974619865\n",
      "batch: 2140 loss: 0.014823050238192081\n",
      "batch: 2150 loss: 0.014351469464600086\n",
      "batch: 2160 loss: 0.015156101435422897\n",
      "batch: 2170 loss: 0.014612555503845215\n",
      "batch: 2180 loss: 0.017872851341962814\n",
      "batch: 2190 loss: 0.01412294339388609\n",
      "batch: 2200 loss: 0.015977656468749046\n",
      "batch: 2210 loss: 0.01418220717459917\n",
      "batch: 2220 loss: 0.017645901069045067\n",
      "batch: 2230 loss: 0.017299655824899673\n",
      "batch: 2240 loss: 0.01503530889749527\n",
      "batch: 2250 loss: 0.016093023121356964\n",
      "batch: 2260 loss: 0.01566893793642521\n",
      "batch: 2270 loss: 0.017218822613358498\n",
      "batch: 2280 loss: 0.01460342574864626\n",
      "batch: 2290 loss: 0.018589984625577927\n",
      "batch: 2300 loss: 0.016891200095415115\n",
      "batch: 2310 loss: 0.020740894600749016\n",
      "batch: 2320 loss: 0.01818975806236267\n",
      "batch: 2330 loss: 0.016450488939881325\n",
      "batch: 2340 loss: 0.015887929126620293\n",
      "batch: 2350 loss: 0.023493219166994095\n",
      "batch: 2360 loss: 0.01468584779649973\n",
      "batch: 2370 loss: 0.016947537660598755\n",
      "batch: 2380 loss: 0.017652586102485657\n",
      "batch: 2390 loss: 0.016206946223974228\n",
      "batch: 2400 loss: 0.01981331966817379\n",
      "batch: 2410 loss: 0.017782926559448242\n",
      "batch: 2420 loss: 0.01741740293800831\n",
      "batch: 2430 loss: 0.023534515872597694\n",
      "batch: 2440 loss: 0.014505933970212936\n",
      "batch: 2450 loss: 0.017355849966406822\n",
      "batch: 2460 loss: 0.014765973202884197\n",
      "batch: 2470 loss: 0.01808447390794754\n",
      "batch: 2480 loss: 0.012269367463886738\n",
      "batch: 2490 loss: 0.01546886470168829\n",
      "batch: 2500 loss: 0.015564641915261745\n",
      "batch: 2510 loss: 0.01944059133529663\n",
      "batch: 2520 loss: 0.019467471167445183\n",
      "batch: 2530 loss: 0.014483073726296425\n",
      "batch: 2540 loss: 0.012805668637156487\n",
      "batch: 2550 loss: 0.01732494868338108\n",
      "batch: 2560 loss: 0.015550769865512848\n",
      "batch: 2570 loss: 0.011431540362536907\n",
      "batch: 2580 loss: 0.015655936673283577\n",
      "batch: 2590 loss: 0.016681522130966187\n",
      "batch: 2600 loss: 0.013246164657175541\n",
      "batch: 2610 loss: 0.016342177987098694\n",
      "batch: 2620 loss: 0.015790250152349472\n",
      "batch: 2630 loss: 0.01557895541191101\n",
      "batch: 2640 loss: 0.020252857357263565\n",
      "batch: 2650 loss: 0.014414740726351738\n",
      "batch: 2660 loss: 0.019044216722249985\n",
      "batch: 2670 loss: 0.01796203851699829\n",
      "batch: 2680 loss: 0.012317933142185211\n",
      "batch: 2690 loss: 0.013255694881081581\n",
      "batch: 2700 loss: 0.013217052444815636\n",
      "batch: 2710 loss: 0.010590564459562302\n",
      "batch: 2720 loss: 0.01251586526632309\n",
      "batch: 2730 loss: 0.014486491680145264\n",
      "batch: 2740 loss: 0.017490815371274948\n",
      "batch: 2750 loss: 0.018590472638607025\n",
      "batch: 2760 loss: 0.015507192350924015\n",
      "batch: 2770 loss: 0.01947636716067791\n",
      "batch: 2780 loss: 0.016514519229531288\n",
      "batch: 2790 loss: 0.013660578988492489\n",
      "round  5 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.016476422548294067\n",
      "batch: 10 loss: 0.01471705548465252\n",
      "batch: 20 loss: 0.015971245244145393\n",
      "batch: 30 loss: 0.018165752291679382\n",
      "batch: 40 loss: 0.016667762771248817\n",
      "batch: 50 loss: 0.0196907389909029\n",
      "batch: 60 loss: 0.012821938842535019\n",
      "batch: 70 loss: 0.01581658236682415\n",
      "batch: 80 loss: 0.01781853474676609\n",
      "batch: 90 loss: 0.016839342191815376\n",
      "batch: 100 loss: 0.01969411037862301\n",
      "batch: 110 loss: 0.022437235340476036\n",
      "batch: 120 loss: 0.01579301804304123\n",
      "batch: 130 loss: 0.01814952678978443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 140 loss: 0.01568303443491459\n",
      "batch: 150 loss: 0.012017881497740746\n",
      "batch: 160 loss: 0.01646045222878456\n",
      "batch: 170 loss: 0.014750375412404537\n",
      "batch: 180 loss: 0.020848432555794716\n",
      "batch: 190 loss: 0.020365212112665176\n",
      "batch: 200 loss: 0.0202474482357502\n",
      "batch: 210 loss: 0.01372172124683857\n",
      "batch: 220 loss: 0.016738077625632286\n",
      "batch: 230 loss: 0.020366264507174492\n",
      "batch: 240 loss: 0.017348144203424454\n",
      "batch: 250 loss: 0.01670544035732746\n",
      "batch: 260 loss: 0.015017712488770485\n",
      "batch: 270 loss: 0.02130156382918358\n",
      "batch: 280 loss: 0.017699193209409714\n",
      "batch: 290 loss: 0.01638956554234028\n",
      "batch: 300 loss: 0.014221548102796078\n",
      "batch: 310 loss: 0.017849259078502655\n",
      "batch: 320 loss: 0.020042357966303825\n",
      "batch: 330 loss: 0.015774020925164223\n",
      "batch: 340 loss: 0.022198384627699852\n",
      "batch: 350 loss: 0.013655779883265495\n",
      "batch: 360 loss: 0.016582326963543892\n",
      "batch: 370 loss: 0.016595982015132904\n",
      "batch: 380 loss: 0.011999201029539108\n",
      "batch: 390 loss: 0.020928263664245605\n",
      "batch: 400 loss: 0.014959965832531452\n",
      "batch: 410 loss: 0.016251090914011\n",
      "batch: 420 loss: 0.018907614052295685\n",
      "batch: 430 loss: 0.015948472544550896\n",
      "batch: 440 loss: 0.023741465061903\n",
      "batch: 450 loss: 0.017382828518748283\n",
      "batch: 460 loss: 0.02013719268143177\n",
      "batch: 470 loss: 0.02326580323278904\n",
      "batch: 480 loss: 0.016032259911298752\n",
      "batch: 490 loss: 0.015744497999548912\n",
      "batch: 500 loss: 0.01858437992632389\n",
      "batch: 510 loss: 0.018795985728502274\n",
      "batch: 520 loss: 0.015874527394771576\n",
      "batch: 530 loss: 0.022425493225455284\n",
      "batch: 540 loss: 0.021517151966691017\n",
      "batch: 550 loss: 0.0216828566044569\n",
      "batch: 560 loss: 0.015883274376392365\n",
      "batch: 570 loss: 0.015668610110878944\n",
      "batch: 580 loss: 0.01907954551279545\n",
      "batch: 590 loss: 0.016729140654206276\n",
      "batch: 600 loss: 0.017445944249629974\n",
      "batch: 610 loss: 0.019604215398430824\n",
      "batch: 620 loss: 0.018008645623922348\n",
      "batch: 630 loss: 0.018633391708135605\n",
      "batch: 640 loss: 0.015469587408006191\n",
      "batch: 650 loss: 0.016082804650068283\n",
      "batch: 660 loss: 0.020345676690340042\n",
      "batch: 670 loss: 0.0193315502256155\n",
      "batch: 680 loss: 0.015515152364969254\n",
      "batch: 690 loss: 0.0198636744171381\n",
      "batch: 700 loss: 0.019026998430490494\n",
      "batch: 710 loss: 0.016384216025471687\n",
      "batch: 720 loss: 0.018122855573892593\n",
      "batch: 730 loss: 0.01740054413676262\n",
      "batch: 740 loss: 0.016018658876419067\n",
      "batch: 750 loss: 0.018801452592015266\n",
      "batch: 760 loss: 0.016521412879228592\n",
      "batch: 770 loss: 0.015886660665273666\n",
      "batch: 780 loss: 0.017639048397541046\n",
      "batch: 790 loss: 0.016773255541920662\n",
      "batch: 800 loss: 0.02171090617775917\n",
      "batch: 810 loss: 0.01619218848645687\n",
      "batch: 820 loss: 0.014720370061695576\n",
      "batch: 830 loss: 0.020520661026239395\n",
      "batch: 840 loss: 0.013280831277370453\n",
      "batch: 850 loss: 0.018525797873735428\n",
      "batch: 860 loss: 0.018014609813690186\n",
      "batch: 870 loss: 0.01685028150677681\n",
      "batch: 880 loss: 0.02049398608505726\n",
      "batch: 890 loss: 0.020409077405929565\n",
      "batch: 900 loss: 0.01760500855743885\n",
      "batch: 910 loss: 0.013341821730136871\n",
      "batch: 920 loss: 0.01962822861969471\n",
      "batch: 930 loss: 0.01814637891948223\n",
      "batch: 940 loss: 0.016538577154278755\n",
      "batch: 950 loss: 0.017724337056279182\n",
      "batch: 960 loss: 0.01851031556725502\n",
      "batch: 970 loss: 0.015412206761538982\n",
      "batch: 980 loss: 0.01725667528808117\n",
      "batch: 990 loss: 0.02087259478867054\n",
      "batch: 1000 loss: 0.01985809952020645\n",
      "batch: 1010 loss: 0.018266504630446434\n",
      "batch: 1020 loss: 0.02017604187130928\n",
      "batch: 1030 loss: 0.01637044921517372\n",
      "batch: 1040 loss: 0.01800820790231228\n",
      "batch: 1050 loss: 0.017854614183306694\n",
      "batch: 1060 loss: 0.01909410022199154\n",
      "batch: 1070 loss: 0.014033493585884571\n",
      "batch: 1080 loss: 0.01581837236881256\n",
      "batch: 1090 loss: 0.017026429995894432\n",
      "batch: 1100 loss: 0.01570906862616539\n",
      "batch: 1110 loss: 0.013918142765760422\n",
      "batch: 1120 loss: 0.01633302867412567\n",
      "batch: 1130 loss: 0.01845204457640648\n",
      "batch: 1140 loss: 0.018125999718904495\n",
      "batch: 1150 loss: 0.016864420846104622\n",
      "batch: 1160 loss: 0.019107118248939514\n",
      "batch: 1170 loss: 0.01550378929823637\n",
      "batch: 1180 loss: 0.01628006249666214\n",
      "batch: 1190 loss: 0.02013198845088482\n",
      "batch: 1200 loss: 0.016668258234858513\n",
      "batch: 1210 loss: 0.017750022932887077\n",
      "batch: 1220 loss: 0.01396898552775383\n",
      "batch: 1230 loss: 0.015348436310887337\n",
      "batch: 1240 loss: 0.016275910660624504\n",
      "batch: 1250 loss: 0.02057545632123947\n",
      "batch: 1260 loss: 0.015226580202579498\n",
      "batch: 1270 loss: 0.01600600965321064\n",
      "batch: 1280 loss: 0.02208268828690052\n",
      "batch: 1290 loss: 0.023939689621329308\n",
      "batch: 1300 loss: 0.022252630442380905\n",
      "batch: 1310 loss: 0.016435248777270317\n",
      "batch: 1320 loss: 0.015436539426445961\n",
      "batch: 1330 loss: 0.01691947691142559\n",
      "batch: 1340 loss: 0.017554305493831635\n",
      "batch: 1350 loss: 0.016022222116589546\n",
      "batch: 1360 loss: 0.01901131123304367\n",
      "batch: 1370 loss: 0.019308796152472496\n",
      "batch: 1380 loss: 0.023060357198119164\n",
      "batch: 1390 loss: 0.017779553309082985\n",
      "batch: 1400 loss: 0.019552165642380714\n",
      "batch: 1410 loss: 0.017146160826086998\n",
      "batch: 1420 loss: 0.017230704426765442\n",
      "batch: 1430 loss: 0.017145836725831032\n",
      "batch: 1440 loss: 0.017816854640841484\n",
      "batch: 1450 loss: 0.016678284853696823\n",
      "batch: 1460 loss: 0.017430663108825684\n",
      "batch: 1470 loss: 0.020509593188762665\n",
      "batch: 1480 loss: 0.014648097567260265\n",
      "batch: 1490 loss: 0.014441005885601044\n",
      "batch: 1500 loss: 0.017504123970866203\n",
      "batch: 1510 loss: 0.02223154902458191\n",
      "batch: 1520 loss: 0.018035786226391792\n",
      "batch: 1530 loss: 0.013676030561327934\n",
      "batch: 1540 loss: 0.019613774493336678\n",
      "batch: 1550 loss: 0.01736760325729847\n",
      "batch: 1560 loss: 0.016064444556832314\n",
      "batch: 1570 loss: 0.015445198863744736\n",
      "batch: 1580 loss: 0.014891654253005981\n",
      "batch: 1590 loss: 0.016853278502821922\n",
      "batch: 1600 loss: 0.016504524275660515\n",
      "batch: 1610 loss: 0.02218327485024929\n",
      "batch: 1620 loss: 0.022397857159376144\n",
      "batch: 1630 loss: 0.014696766622364521\n",
      "batch: 1640 loss: 0.01419308502227068\n",
      "batch: 1650 loss: 0.01659662090241909\n",
      "batch: 1660 loss: 0.01829473115503788\n",
      "batch: 1670 loss: 0.01950734294950962\n",
      "batch: 1680 loss: 0.02268637903034687\n",
      "batch: 1690 loss: 0.01805739477276802\n",
      "batch: 1700 loss: 0.014909588731825352\n",
      "batch: 1710 loss: 0.01878589764237404\n",
      "batch: 1720 loss: 0.02048465423285961\n",
      "batch: 1730 loss: 0.018895961344242096\n",
      "batch: 1740 loss: 0.018207978457212448\n",
      "batch: 1750 loss: 0.016955040395259857\n",
      "batch: 1760 loss: 0.021599700674414635\n",
      "batch: 1770 loss: 0.017758283764123917\n",
      "batch: 1780 loss: 0.01697891391813755\n",
      "batch: 1790 loss: 0.019349364563822746\n",
      "batch: 1800 loss: 0.022048674523830414\n",
      "batch: 1810 loss: 0.017859667539596558\n",
      "batch: 1820 loss: 0.01914254203438759\n",
      "batch: 1830 loss: 0.014902550727128983\n",
      "batch: 1840 loss: 0.014203576371073723\n",
      "batch: 1850 loss: 0.01744939386844635\n",
      "batch: 1860 loss: 0.01853915862739086\n",
      "batch: 1870 loss: 0.016135064885020256\n",
      "batch: 1880 loss: 0.018916131928563118\n",
      "batch: 1890 loss: 0.017007431015372276\n",
      "batch: 1900 loss: 0.019946089014410973\n",
      "batch: 1910 loss: 0.016614053398370743\n",
      "batch: 1920 loss: 0.014873470179736614\n",
      "batch: 1930 loss: 0.0132495341822505\n",
      "batch: 1940 loss: 0.01498712319880724\n",
      "batch: 1950 loss: 0.01913408935070038\n",
      "batch: 1960 loss: 0.015201392583549023\n",
      "batch: 1970 loss: 0.016274433583021164\n",
      "batch: 1980 loss: 0.013670999556779861\n",
      "batch: 1990 loss: 0.02171163819730282\n",
      "batch: 2000 loss: 0.020488983020186424\n",
      "batch: 2010 loss: 0.016080016270279884\n",
      "batch: 2020 loss: 0.016708165407180786\n",
      "batch: 2030 loss: 0.016435418277978897\n",
      "batch: 2040 loss: 0.0144454725086689\n",
      "batch: 2050 loss: 0.017092997208237648\n",
      "batch: 2060 loss: 0.014323943294584751\n",
      "batch: 2070 loss: 0.016424108296632767\n",
      "batch: 2080 loss: 0.012289978563785553\n",
      "batch: 2090 loss: 0.017682507634162903\n",
      "batch: 2100 loss: 0.021949470043182373\n",
      "batch: 2110 loss: 0.013385430909693241\n",
      "batch: 2120 loss: 0.01872951351106167\n",
      "batch: 2130 loss: 0.015529144555330276\n",
      "batch: 2140 loss: 0.018364494666457176\n",
      "batch: 2150 loss: 0.019118936732411385\n",
      "batch: 2160 loss: 0.014988325536251068\n",
      "batch: 2170 loss: 0.016062095761299133\n",
      "batch: 2180 loss: 0.016610872000455856\n",
      "batch: 2190 loss: 0.019204318523406982\n",
      "batch: 2200 loss: 0.018615150824189186\n",
      "batch: 2210 loss: 0.022620368748903275\n",
      "batch: 2220 loss: 0.017111100256443024\n",
      "batch: 2230 loss: 0.018770581111311913\n",
      "batch: 2240 loss: 0.020967213436961174\n",
      "batch: 2250 loss: 0.01587379164993763\n",
      "batch: 2260 loss: 0.014283272437751293\n",
      "batch: 2270 loss: 0.020485002547502518\n",
      "batch: 2280 loss: 0.01978830061852932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2290 loss: 0.015431022271513939\n",
      "batch: 2300 loss: 0.016733357682824135\n",
      "batch: 2310 loss: 0.017704103142023087\n",
      "batch: 2320 loss: 0.016102924942970276\n",
      "batch: 2330 loss: 0.020609602332115173\n",
      "batch: 2340 loss: 0.015678202733397484\n",
      "batch: 2350 loss: 0.017944302409887314\n",
      "batch: 2360 loss: 0.015606838278472424\n",
      "batch: 2370 loss: 0.018795646727085114\n",
      "batch: 2380 loss: 0.015229472890496254\n",
      "batch: 2390 loss: 0.019016865640878677\n",
      "batch: 2400 loss: 0.016101479530334473\n",
      "batch: 2410 loss: 0.017973024398088455\n",
      "batch: 2420 loss: 0.01338741835206747\n",
      "batch: 2430 loss: 0.016710735857486725\n",
      "batch: 2440 loss: 0.015948515385389328\n",
      "batch: 2450 loss: 0.013445899821817875\n",
      "batch: 2460 loss: 0.014476739801466465\n",
      "batch: 2470 loss: 0.013861630111932755\n",
      "batch: 2480 loss: 0.017741862684488297\n",
      "batch: 2490 loss: 0.01609954424202442\n",
      "batch: 2500 loss: 0.020750228315591812\n",
      "batch: 2510 loss: 0.014360514469444752\n",
      "batch: 2520 loss: 0.011843143031001091\n",
      "batch: 2530 loss: 0.018217340111732483\n",
      "batch: 2540 loss: 0.015608072280883789\n",
      "batch: 2550 loss: 0.018552005290985107\n",
      "batch: 2560 loss: 0.017178470268845558\n",
      "batch: 2570 loss: 0.02113349549472332\n",
      "batch: 2580 loss: 0.017820341512560844\n",
      "batch: 2590 loss: 0.015014376491308212\n",
      "batch: 2600 loss: 0.01582009717822075\n",
      "batch: 2610 loss: 0.017590854316949844\n",
      "batch: 2620 loss: 0.01373592671006918\n",
      "batch: 2630 loss: 0.015012769028544426\n",
      "batch: 2640 loss: 0.012676713988184929\n",
      "batch: 2650 loss: 0.016788486391305923\n",
      "batch: 2660 loss: 0.014619987457990646\n",
      "batch: 2670 loss: 0.016931921243667603\n",
      "batch: 2680 loss: 0.01883735880255699\n",
      "batch: 2690 loss: 0.014903845265507698\n",
      "batch: 2700 loss: 0.013649172149598598\n",
      "batch: 2710 loss: 0.015103408135473728\n",
      "batch: 2720 loss: 0.020853200927376747\n",
      "batch: 2730 loss: 0.014523732475936413\n",
      "batch: 2740 loss: 0.01528073474764824\n",
      "batch: 2750 loss: 0.017528295516967773\n",
      "batch: 2760 loss: 0.019135113805532455\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.017848389223217964\n",
      "batch: 10 loss: 0.015141650103032589\n",
      "batch: 20 loss: 0.014867687597870827\n",
      "batch: 30 loss: 0.01760290376842022\n",
      "batch: 40 loss: 0.014942297711968422\n",
      "batch: 50 loss: 0.025328699499368668\n",
      "batch: 60 loss: 0.018846211954951286\n",
      "batch: 70 loss: 0.02013552375137806\n",
      "batch: 80 loss: 0.013717063702642918\n",
      "batch: 90 loss: 0.018399812281131744\n",
      "batch: 100 loss: 0.012176982127130032\n",
      "batch: 110 loss: 0.014482527039945126\n",
      "batch: 120 loss: 0.018338503316044807\n",
      "batch: 130 loss: 0.019697781652212143\n",
      "batch: 140 loss: 0.015367789193987846\n",
      "batch: 150 loss: 0.015959646552801132\n",
      "batch: 160 loss: 0.014532522298395634\n",
      "batch: 170 loss: 0.01715901680290699\n",
      "batch: 180 loss: 0.01991201378405094\n",
      "batch: 190 loss: 0.019809214398264885\n",
      "batch: 200 loss: 0.01445101760327816\n",
      "batch: 210 loss: 0.01682344079017639\n",
      "batch: 220 loss: 0.01746959052979946\n",
      "batch: 230 loss: 0.01442156545817852\n",
      "batch: 240 loss: 0.0197503212839365\n",
      "batch: 250 loss: 0.014391223900020123\n",
      "batch: 260 loss: 0.016564054414629936\n",
      "batch: 270 loss: 0.01615522988140583\n",
      "batch: 280 loss: 0.013872010633349419\n",
      "batch: 290 loss: 0.016596602275967598\n",
      "batch: 300 loss: 0.014615871012210846\n",
      "batch: 310 loss: 0.015032917261123657\n",
      "batch: 320 loss: 0.01240355335175991\n",
      "batch: 330 loss: 0.021291887387633324\n",
      "batch: 340 loss: 0.015354974195361137\n",
      "batch: 350 loss: 0.021087229251861572\n",
      "batch: 360 loss: 0.01737203449010849\n",
      "batch: 370 loss: 0.013849513605237007\n",
      "batch: 380 loss: 0.016497885808348656\n",
      "batch: 390 loss: 0.017650719732046127\n",
      "batch: 400 loss: 0.01929682493209839\n",
      "batch: 410 loss: 0.019131869077682495\n",
      "batch: 420 loss: 0.017957717180252075\n",
      "batch: 430 loss: 0.015682799741625786\n",
      "batch: 440 loss: 0.0178218986839056\n",
      "batch: 450 loss: 0.017091061919927597\n",
      "batch: 460 loss: 0.019963422790169716\n",
      "batch: 470 loss: 0.019640343263745308\n",
      "batch: 480 loss: 0.011823357082903385\n",
      "batch: 490 loss: 0.01605403795838356\n",
      "batch: 500 loss: 0.01911834254860878\n",
      "batch: 510 loss: 0.020799361169338226\n",
      "batch: 520 loss: 0.014547448605298996\n",
      "batch: 530 loss: 0.011579529382288456\n",
      "batch: 540 loss: 0.02366979792714119\n",
      "batch: 550 loss: 0.014020916074514389\n",
      "batch: 560 loss: 0.016193870455026627\n",
      "batch: 570 loss: 0.018914353102445602\n",
      "batch: 580 loss: 0.018274033442139626\n",
      "batch: 590 loss: 0.016785645857453346\n",
      "batch: 600 loss: 0.016893669962882996\n",
      "batch: 610 loss: 0.016568070277571678\n",
      "batch: 620 loss: 0.018465038388967514\n",
      "batch: 630 loss: 0.015370870009064674\n",
      "batch: 640 loss: 0.018862668424844742\n",
      "batch: 650 loss: 0.016875259578227997\n",
      "batch: 660 loss: 0.017117364332079887\n",
      "batch: 670 loss: 0.018240606412291527\n",
      "batch: 680 loss: 0.01523183286190033\n",
      "batch: 690 loss: 0.01572253368794918\n",
      "batch: 700 loss: 0.016903648152947426\n",
      "batch: 710 loss: 0.018707631155848503\n",
      "batch: 720 loss: 0.017642518505454063\n",
      "batch: 730 loss: 0.016953356564044952\n",
      "batch: 740 loss: 0.0194710623472929\n",
      "batch: 750 loss: 0.020478148013353348\n",
      "batch: 760 loss: 0.01877450756728649\n",
      "batch: 770 loss: 0.01673778146505356\n",
      "batch: 780 loss: 0.014066358096897602\n",
      "batch: 790 loss: 0.017391467466950417\n",
      "batch: 800 loss: 0.019328467547893524\n",
      "batch: 810 loss: 0.021172044798731804\n",
      "batch: 820 loss: 0.015367869287729263\n",
      "batch: 830 loss: 0.018067602068185806\n",
      "batch: 840 loss: 0.014628679491579533\n",
      "batch: 850 loss: 0.013513618148863316\n",
      "batch: 860 loss: 0.012935376726090908\n",
      "batch: 870 loss: 0.016086112707853317\n",
      "batch: 880 loss: 0.014394847676157951\n",
      "batch: 890 loss: 0.014191536232829094\n",
      "batch: 900 loss: 0.01294332928955555\n",
      "batch: 910 loss: 0.014756251126527786\n",
      "batch: 920 loss: 0.011654006317257881\n",
      "batch: 930 loss: 0.019004875794053078\n",
      "batch: 940 loss: 0.016695765778422356\n",
      "batch: 950 loss: 0.015962662175297737\n",
      "batch: 960 loss: 0.01613917388021946\n",
      "batch: 970 loss: 0.016514144837856293\n",
      "batch: 980 loss: 0.01892765983939171\n",
      "batch: 990 loss: 0.01197343785315752\n",
      "batch: 1000 loss: 0.017080863937735558\n",
      "batch: 1010 loss: 0.016825051978230476\n",
      "batch: 1020 loss: 0.015252742916345596\n",
      "batch: 1030 loss: 0.01702670380473137\n",
      "batch: 1040 loss: 0.011479656212031841\n",
      "batch: 1050 loss: 0.01736350543797016\n",
      "batch: 1060 loss: 0.015602405183017254\n",
      "batch: 1070 loss: 0.017559755593538284\n",
      "batch: 1080 loss: 0.017714504152536392\n",
      "batch: 1090 loss: 0.020045161247253418\n",
      "batch: 1100 loss: 0.015357034280896187\n",
      "batch: 1110 loss: 0.01660328544676304\n",
      "batch: 1120 loss: 0.017516614869236946\n",
      "batch: 1130 loss: 0.015372312627732754\n",
      "batch: 1140 loss: 0.017291974276304245\n",
      "batch: 1150 loss: 0.019285036250948906\n",
      "batch: 1160 loss: 0.015524404123425484\n",
      "batch: 1170 loss: 0.021976497024297714\n",
      "batch: 1180 loss: 0.017765656113624573\n",
      "batch: 1190 loss: 0.02129903808236122\n",
      "batch: 1200 loss: 0.015921862795948982\n",
      "batch: 1210 loss: 0.015587965957820415\n",
      "batch: 1220 loss: 0.01806853711605072\n",
      "batch: 1230 loss: 0.014578576199710369\n",
      "batch: 1240 loss: 0.021648816764354706\n",
      "batch: 1250 loss: 0.013033433817327023\n",
      "batch: 1260 loss: 0.015447553247213364\n",
      "batch: 1270 loss: 0.016179798170924187\n",
      "batch: 1280 loss: 0.01869870349764824\n",
      "batch: 1290 loss: 0.016447972506284714\n",
      "batch: 1300 loss: 0.019229449331760406\n",
      "batch: 1310 loss: 0.01746499165892601\n",
      "batch: 1320 loss: 0.018040301278233528\n",
      "batch: 1330 loss: 0.017435630783438683\n",
      "batch: 1340 loss: 0.015027199871838093\n",
      "batch: 1350 loss: 0.019775837659835815\n",
      "batch: 1360 loss: 0.016392936930060387\n",
      "batch: 1370 loss: 0.01692832075059414\n",
      "batch: 1380 loss: 0.01499492209404707\n",
      "batch: 1390 loss: 0.01606205850839615\n",
      "batch: 1400 loss: 0.018527422100305557\n",
      "batch: 1410 loss: 0.013922682963311672\n",
      "batch: 1420 loss: 0.01651672087609768\n",
      "batch: 1430 loss: 0.021701786667108536\n",
      "batch: 1440 loss: 0.018023641780018806\n",
      "batch: 1450 loss: 0.01669510267674923\n",
      "batch: 1460 loss: 0.013721334747970104\n",
      "batch: 1470 loss: 0.015141931362450123\n",
      "batch: 1480 loss: 0.020747516304254532\n",
      "batch: 1490 loss: 0.018778234720230103\n",
      "batch: 1500 loss: 0.01568334922194481\n",
      "batch: 1510 loss: 0.016682932153344154\n",
      "batch: 1520 loss: 0.02089460752904415\n",
      "batch: 1530 loss: 0.015096703544259071\n",
      "batch: 1540 loss: 0.01831134781241417\n",
      "batch: 1550 loss: 0.020220452919602394\n",
      "batch: 1560 loss: 0.01776232197880745\n",
      "batch: 1570 loss: 0.016206195577979088\n",
      "batch: 1580 loss: 0.017575683072209358\n",
      "batch: 1590 loss: 0.01848694495856762\n",
      "batch: 1600 loss: 0.022408589720726013\n",
      "batch: 1610 loss: 0.014936954714357853\n",
      "batch: 1620 loss: 0.016439611092209816\n",
      "batch: 1630 loss: 0.017864808440208435\n",
      "batch: 1640 loss: 0.01952892728149891\n",
      "batch: 1650 loss: 0.015095246024429798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1660 loss: 0.01852833479642868\n",
      "batch: 1670 loss: 0.016861731186509132\n",
      "batch: 1680 loss: 0.017904022708535194\n",
      "batch: 1690 loss: 0.013932175934314728\n",
      "batch: 1700 loss: 0.013247627764940262\n",
      "batch: 1710 loss: 0.016382191330194473\n",
      "batch: 1720 loss: 0.015124820172786713\n",
      "batch: 1730 loss: 0.015516500920057297\n",
      "batch: 1740 loss: 0.019665688276290894\n",
      "batch: 1750 loss: 0.017137829214334488\n",
      "batch: 1760 loss: 0.019247066229581833\n",
      "batch: 1770 loss: 0.021370867267251015\n",
      "batch: 1780 loss: 0.016491614282131195\n",
      "batch: 1790 loss: 0.019569484516978264\n",
      "batch: 1800 loss: 0.015459912829101086\n",
      "batch: 1810 loss: 0.01476941630244255\n",
      "batch: 1820 loss: 0.013088558800518513\n",
      "batch: 1830 loss: 0.014620131812989712\n",
      "batch: 1840 loss: 0.012952045537531376\n",
      "batch: 1850 loss: 0.016585515812039375\n",
      "batch: 1860 loss: 0.023223502561450005\n",
      "batch: 1870 loss: 0.013953016139566898\n",
      "batch: 1880 loss: 0.016714733093976974\n",
      "batch: 1890 loss: 0.02285805344581604\n",
      "batch: 1900 loss: 0.01905127614736557\n",
      "batch: 1910 loss: 0.012266107834875584\n",
      "batch: 1920 loss: 0.016624273732304573\n",
      "batch: 1930 loss: 0.017853904515504837\n",
      "batch: 1940 loss: 0.0185785423964262\n",
      "batch: 1950 loss: 0.012774722650647163\n",
      "batch: 1960 loss: 0.01862931251525879\n",
      "batch: 1970 loss: 0.01635795272886753\n",
      "batch: 1980 loss: 0.01570035330951214\n",
      "batch: 1990 loss: 0.0132216177880764\n",
      "batch: 2000 loss: 0.013977338559925556\n",
      "batch: 2010 loss: 0.02188728004693985\n",
      "batch: 2020 loss: 0.015874043107032776\n",
      "batch: 2030 loss: 0.015314752236008644\n",
      "batch: 2040 loss: 0.01720775105059147\n",
      "batch: 2050 loss: 0.01251627504825592\n",
      "batch: 2060 loss: 0.01176553126424551\n",
      "batch: 2070 loss: 0.015887251123785973\n",
      "batch: 2080 loss: 0.018765205517411232\n",
      "batch: 2090 loss: 0.01575799100100994\n",
      "batch: 2100 loss: 0.015790259465575218\n",
      "batch: 2110 loss: 0.017367161810398102\n",
      "batch: 2120 loss: 0.017635483294725418\n",
      "batch: 2130 loss: 0.019380558282136917\n",
      "batch: 2140 loss: 0.019575996324419975\n",
      "batch: 2150 loss: 0.01477283425629139\n",
      "batch: 2160 loss: 0.015032578259706497\n",
      "batch: 2170 loss: 0.016517119482159615\n",
      "batch: 2180 loss: 0.017597338184714317\n",
      "batch: 2190 loss: 0.01693631149828434\n",
      "batch: 2200 loss: 0.013950175605714321\n",
      "batch: 2210 loss: 0.0163094662129879\n",
      "batch: 2220 loss: 0.016375267878174782\n",
      "batch: 2230 loss: 0.0168460663408041\n",
      "batch: 2240 loss: 0.01759450137615204\n",
      "batch: 2250 loss: 0.017035583034157753\n",
      "batch: 2260 loss: 0.013587702065706253\n",
      "batch: 2270 loss: 0.018444150686264038\n",
      "batch: 2280 loss: 0.015241852961480618\n",
      "batch: 2290 loss: 0.017441675066947937\n",
      "batch: 2300 loss: 0.01510616485029459\n",
      "batch: 2310 loss: 0.012912492267787457\n",
      "batch: 2320 loss: 0.01714402809739113\n",
      "batch: 2330 loss: 0.01597610116004944\n",
      "batch: 2340 loss: 0.016279933974146843\n",
      "batch: 2350 loss: 0.01520927157253027\n",
      "batch: 2360 loss: 0.019460180774331093\n",
      "batch: 2370 loss: 0.019797442480921745\n",
      "batch: 2380 loss: 0.01721380464732647\n",
      "batch: 2390 loss: 0.01964242197573185\n",
      "batch: 2400 loss: 0.018970534205436707\n",
      "batch: 2410 loss: 0.01279379241168499\n",
      "batch: 2420 loss: 0.01961996592581272\n",
      "batch: 2430 loss: 0.017134180292487144\n",
      "batch: 2440 loss: 0.018547479063272476\n",
      "batch: 2450 loss: 0.0172426700592041\n",
      "batch: 2460 loss: 0.016154399141669273\n",
      "batch: 2470 loss: 0.0192592591047287\n",
      "batch: 2480 loss: 0.014745124615728855\n",
      "batch: 2490 loss: 0.014961094595491886\n",
      "batch: 2500 loss: 0.014753489755094051\n",
      "batch: 2510 loss: 0.016134614124894142\n",
      "batch: 2520 loss: 0.017637966200709343\n",
      "batch: 2530 loss: 0.016913864761590958\n",
      "batch: 2540 loss: 0.015169027261435986\n",
      "batch: 2550 loss: 0.01637529581785202\n",
      "batch: 2560 loss: 0.018697436898946762\n",
      "batch: 2570 loss: 0.021838273853063583\n",
      "batch: 2580 loss: 0.01881362870335579\n",
      "batch: 2590 loss: 0.019197091460227966\n",
      "batch: 2600 loss: 0.019100772216916084\n",
      "batch: 2610 loss: 0.014075864106416702\n",
      "batch: 2620 loss: 0.014488833956420422\n",
      "batch: 2630 loss: 0.01843949593603611\n",
      "batch: 2640 loss: 0.018020208925008774\n",
      "batch: 2650 loss: 0.019235968589782715\n",
      "batch: 2660 loss: 0.017576223239302635\n",
      "batch: 2670 loss: 0.016424797475337982\n",
      "batch: 2680 loss: 0.019366810098290443\n",
      "batch: 2690 loss: 0.014456469565629959\n",
      "batch: 2700 loss: 0.017039678990840912\n",
      "batch: 2710 loss: 0.017961997538805008\n",
      "batch: 2720 loss: 0.021415021270513535\n",
      "batch: 2730 loss: 0.015326225198805332\n",
      "batch: 2740 loss: 0.016625288873910904\n",
      "batch: 2750 loss: 0.018275553360581398\n",
      "batch: 2760 loss: 0.020284423604607582\n",
      "round  6 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.018138037994503975\n",
      "batch: 10 loss: 0.018786655738949776\n",
      "batch: 20 loss: 0.015275071375072002\n",
      "batch: 30 loss: 0.019062921404838562\n",
      "batch: 40 loss: 0.020141493529081345\n",
      "batch: 50 loss: 0.015090475790202618\n",
      "batch: 60 loss: 0.020747153088450432\n",
      "batch: 70 loss: 0.015355156734585762\n",
      "batch: 80 loss: 0.015087106265127659\n",
      "batch: 90 loss: 0.014300500974059105\n",
      "batch: 100 loss: 0.017094427719712257\n",
      "batch: 110 loss: 0.013713452033698559\n",
      "batch: 120 loss: 0.013036130927503109\n",
      "batch: 130 loss: 0.017636101692914963\n",
      "batch: 140 loss: 0.017879389226436615\n",
      "batch: 150 loss: 0.02083463780581951\n",
      "batch: 160 loss: 0.02143564634025097\n",
      "batch: 170 loss: 0.01451075728982687\n",
      "batch: 180 loss: 0.01654214598238468\n",
      "batch: 190 loss: 0.012435243465006351\n",
      "batch: 200 loss: 0.020771939307451248\n",
      "batch: 210 loss: 0.017611153423786163\n",
      "batch: 220 loss: 0.011999647133052349\n",
      "batch: 230 loss: 0.015861833468079567\n",
      "batch: 240 loss: 0.014758688397705555\n",
      "batch: 250 loss: 0.018380574882030487\n",
      "batch: 260 loss: 0.018473675474524498\n",
      "batch: 270 loss: 0.013281063176691532\n",
      "batch: 280 loss: 0.014961427077651024\n",
      "batch: 290 loss: 0.015853365883231163\n",
      "batch: 300 loss: 0.017683876678347588\n",
      "batch: 310 loss: 0.01700686104595661\n",
      "batch: 320 loss: 0.016574610024690628\n",
      "batch: 330 loss: 0.015048934146761894\n",
      "batch: 340 loss: 0.015889132395386696\n",
      "batch: 350 loss: 0.01820817030966282\n",
      "batch: 360 loss: 0.018425775691866875\n",
      "batch: 370 loss: 0.013654717244207859\n",
      "batch: 380 loss: 0.015116716735064983\n",
      "batch: 390 loss: 0.017251785844564438\n",
      "batch: 400 loss: 0.01743963733315468\n",
      "batch: 410 loss: 0.016370877623558044\n",
      "batch: 420 loss: 0.015568721108138561\n",
      "batch: 430 loss: 0.010874521918594837\n",
      "batch: 440 loss: 0.013650825247168541\n",
      "batch: 450 loss: 0.01764928735792637\n",
      "batch: 460 loss: 0.02235054410994053\n",
      "batch: 470 loss: 0.015097083523869514\n",
      "batch: 480 loss: 0.015705250203609467\n",
      "batch: 490 loss: 0.020811766386032104\n",
      "batch: 500 loss: 0.016343966126441956\n",
      "batch: 510 loss: 0.01961742527782917\n",
      "batch: 520 loss: 0.011744707822799683\n",
      "batch: 530 loss: 0.015611677430570126\n",
      "batch: 540 loss: 0.013765651732683182\n",
      "batch: 550 loss: 0.013956655748188496\n",
      "batch: 560 loss: 0.015388060361146927\n",
      "batch: 570 loss: 0.01580805890262127\n",
      "batch: 580 loss: 0.01948266476392746\n",
      "batch: 590 loss: 0.016140460968017578\n",
      "batch: 600 loss: 0.0183644387871027\n",
      "batch: 610 loss: 0.015301728621125221\n",
      "batch: 620 loss: 0.015240870416164398\n",
      "batch: 630 loss: 0.013841796666383743\n",
      "batch: 640 loss: 0.015622616745531559\n",
      "batch: 650 loss: 0.018653489649295807\n",
      "batch: 660 loss: 0.014646102674305439\n",
      "batch: 670 loss: 0.016270572319626808\n",
      "batch: 680 loss: 0.01212693564593792\n",
      "batch: 690 loss: 0.01689690165221691\n",
      "batch: 700 loss: 0.01793169416487217\n",
      "batch: 710 loss: 0.013519779779016972\n",
      "batch: 720 loss: 0.014496387913823128\n",
      "batch: 730 loss: 0.012912389822304249\n",
      "batch: 740 loss: 0.016527380794286728\n",
      "batch: 750 loss: 0.015123739838600159\n",
      "batch: 760 loss: 0.01384123507887125\n",
      "batch: 770 loss: 0.017449405044317245\n",
      "batch: 780 loss: 0.014109679497778416\n",
      "batch: 790 loss: 0.020763147622346878\n",
      "batch: 800 loss: 0.015486476011574268\n",
      "batch: 810 loss: 0.018651679158210754\n",
      "batch: 820 loss: 0.01605917327105999\n",
      "batch: 830 loss: 0.012445221655070782\n",
      "batch: 840 loss: 0.014868048951029778\n",
      "batch: 850 loss: 0.017498711124062538\n",
      "batch: 860 loss: 0.01999419927597046\n",
      "batch: 870 loss: 0.013439275324344635\n",
      "batch: 880 loss: 0.01759609952569008\n",
      "batch: 890 loss: 0.016037141904234886\n",
      "batch: 900 loss: 0.014014951884746552\n",
      "batch: 910 loss: 0.016324089840054512\n",
      "batch: 920 loss: 0.01312709879130125\n",
      "batch: 930 loss: 0.01791028119623661\n",
      "batch: 940 loss: 0.015604688785970211\n",
      "batch: 950 loss: 0.01931621879339218\n",
      "batch: 960 loss: 0.022725606337189674\n",
      "batch: 970 loss: 0.013781524263322353\n",
      "batch: 980 loss: 0.017775364220142365\n",
      "batch: 990 loss: 0.01533475797623396\n",
      "batch: 1000 loss: 0.01222809124737978\n",
      "batch: 1010 loss: 0.014355464838445187\n",
      "batch: 1020 loss: 0.017581650987267494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1030 loss: 0.015100331045687199\n",
      "batch: 1040 loss: 0.019484004005789757\n",
      "batch: 1050 loss: 0.017936648800969124\n",
      "batch: 1060 loss: 0.018619921058416367\n",
      "batch: 1070 loss: 0.014430691488087177\n",
      "batch: 1080 loss: 0.018547378480434418\n",
      "batch: 1090 loss: 0.015088784508407116\n",
      "batch: 1100 loss: 0.022277530282735825\n",
      "batch: 1110 loss: 0.01765957847237587\n",
      "batch: 1120 loss: 0.012958258390426636\n",
      "batch: 1130 loss: 0.018365677446126938\n",
      "batch: 1140 loss: 0.012584753334522247\n",
      "batch: 1150 loss: 0.012241696007549763\n",
      "batch: 1160 loss: 0.011790899559855461\n",
      "batch: 1170 loss: 0.016807539388537407\n",
      "batch: 1180 loss: 0.016070816665887833\n",
      "batch: 1190 loss: 0.014673300087451935\n",
      "batch: 1200 loss: 0.017561854794621468\n",
      "batch: 1210 loss: 0.018387163057923317\n",
      "batch: 1220 loss: 0.01811523362994194\n",
      "batch: 1230 loss: 0.016581909731030464\n",
      "batch: 1240 loss: 0.01412703562527895\n",
      "batch: 1250 loss: 0.013103359378874302\n",
      "batch: 1260 loss: 0.014975429512560368\n",
      "batch: 1270 loss: 0.016010170802474022\n",
      "batch: 1280 loss: 0.012963746674358845\n",
      "batch: 1290 loss: 0.014341965317726135\n",
      "batch: 1300 loss: 0.012283779680728912\n",
      "batch: 1310 loss: 0.01946319080889225\n",
      "batch: 1320 loss: 0.016249272972345352\n",
      "batch: 1330 loss: 0.01246513333171606\n",
      "batch: 1340 loss: 0.02107064239680767\n",
      "batch: 1350 loss: 0.015292588621377945\n",
      "batch: 1360 loss: 0.016765054315328598\n",
      "batch: 1370 loss: 0.017796015366911888\n",
      "batch: 1380 loss: 0.013130231760442257\n",
      "batch: 1390 loss: 0.016715528443455696\n",
      "batch: 1400 loss: 0.021067913621664047\n",
      "batch: 1410 loss: 0.01920170523226261\n",
      "batch: 1420 loss: 0.014119858853518963\n",
      "batch: 1430 loss: 0.0140509819611907\n",
      "batch: 1440 loss: 0.01857210136950016\n",
      "batch: 1450 loss: 0.021065879613161087\n",
      "batch: 1460 loss: 0.011590417474508286\n",
      "batch: 1470 loss: 0.01583387330174446\n",
      "batch: 1480 loss: 0.015642929822206497\n",
      "batch: 1490 loss: 0.018777471035718918\n",
      "batch: 1500 loss: 0.01679464615881443\n",
      "batch: 1510 loss: 0.021389411762356758\n",
      "batch: 1520 loss: 0.016467725858092308\n",
      "batch: 1530 loss: 0.017528017982840538\n",
      "batch: 1540 loss: 0.015033765695989132\n",
      "batch: 1550 loss: 0.01530251745134592\n",
      "batch: 1560 loss: 0.013059149496257305\n",
      "batch: 1570 loss: 0.017233503982424736\n",
      "batch: 1580 loss: 0.016707424074411392\n",
      "batch: 1590 loss: 0.014913838356733322\n",
      "batch: 1600 loss: 0.01640465296804905\n",
      "batch: 1610 loss: 0.01717776618897915\n",
      "batch: 1620 loss: 0.010979454964399338\n",
      "batch: 1630 loss: 0.01792244240641594\n",
      "batch: 1640 loss: 0.019996682181954384\n",
      "batch: 1650 loss: 0.015460594557225704\n",
      "batch: 1660 loss: 0.01932395249605179\n",
      "batch: 1670 loss: 0.01732335425913334\n",
      "batch: 1680 loss: 0.015503603965044022\n",
      "batch: 1690 loss: 0.016614053398370743\n",
      "batch: 1700 loss: 0.014019505120813847\n",
      "batch: 1710 loss: 0.015304219909012318\n",
      "batch: 1720 loss: 0.01562948152422905\n",
      "batch: 1730 loss: 0.013874223455786705\n",
      "batch: 1740 loss: 0.01705174893140793\n",
      "batch: 1750 loss: 0.019360214471817017\n",
      "batch: 1760 loss: 0.014428011141717434\n",
      "batch: 1770 loss: 0.017509985715150833\n",
      "batch: 1780 loss: 0.020500225946307182\n",
      "batch: 1790 loss: 0.016316557303071022\n",
      "batch: 1800 loss: 0.01817951165139675\n",
      "batch: 1810 loss: 0.018921198323369026\n",
      "batch: 1820 loss: 0.017860043793916702\n",
      "batch: 1830 loss: 0.018605027347803116\n",
      "batch: 1840 loss: 0.015617633238434792\n",
      "batch: 1850 loss: 0.01817978173494339\n",
      "batch: 1860 loss: 0.016112850978970528\n",
      "batch: 1870 loss: 0.012793971225619316\n",
      "batch: 1880 loss: 0.019601667299866676\n",
      "batch: 1890 loss: 0.01367903407663107\n",
      "batch: 1900 loss: 0.01598779670894146\n",
      "batch: 1910 loss: 0.009595431387424469\n",
      "batch: 1920 loss: 0.014158286154270172\n",
      "batch: 1930 loss: 0.021259021013975143\n",
      "batch: 1940 loss: 0.01530609093606472\n",
      "batch: 1950 loss: 0.013932159170508385\n",
      "batch: 1960 loss: 0.019632097333669662\n",
      "batch: 1970 loss: 0.017286764457821846\n",
      "batch: 1980 loss: 0.01925279013812542\n",
      "batch: 1990 loss: 0.01948651112616062\n",
      "batch: 2000 loss: 0.019777005538344383\n",
      "batch: 2010 loss: 0.014223591424524784\n",
      "batch: 2020 loss: 0.014508732594549656\n",
      "batch: 2030 loss: 0.014959416352212429\n",
      "batch: 2040 loss: 0.014589115977287292\n",
      "batch: 2050 loss: 0.012796672061085701\n",
      "batch: 2060 loss: 0.019188271835446358\n",
      "batch: 2070 loss: 0.021077463403344154\n",
      "batch: 2080 loss: 0.014442882500588894\n",
      "batch: 2090 loss: 0.018584880977869034\n",
      "batch: 2100 loss: 0.013898629695177078\n",
      "batch: 2110 loss: 0.01686984859406948\n",
      "batch: 2120 loss: 0.01718318834900856\n",
      "batch: 2130 loss: 0.01826462708413601\n",
      "batch: 2140 loss: 0.014962956309318542\n",
      "batch: 2150 loss: 0.018040811643004417\n",
      "batch: 2160 loss: 0.015566856600344181\n",
      "batch: 2170 loss: 0.014103027991950512\n",
      "batch: 2180 loss: 0.017858961597085\n",
      "batch: 2190 loss: 0.016148045659065247\n",
      "batch: 2200 loss: 0.01673557423055172\n",
      "batch: 2210 loss: 0.018142150714993477\n",
      "batch: 2220 loss: 0.01750386878848076\n",
      "batch: 2230 loss: 0.015561766922473907\n",
      "batch: 2240 loss: 0.015984777361154556\n",
      "batch: 2250 loss: 0.015451410785317421\n",
      "batch: 2260 loss: 0.016455626115202904\n",
      "batch: 2270 loss: 0.012859889306128025\n",
      "batch: 2280 loss: 0.02009083703160286\n",
      "batch: 2290 loss: 0.015936793759465218\n",
      "batch: 2300 loss: 0.01379088219255209\n",
      "batch: 2310 loss: 0.020897390320897102\n",
      "batch: 2320 loss: 0.012109690345823765\n",
      "batch: 2330 loss: 0.017665131017565727\n",
      "batch: 2340 loss: 0.01797151006758213\n",
      "batch: 2350 loss: 0.016833532601594925\n",
      "batch: 2360 loss: 0.015277606435120106\n",
      "batch: 2370 loss: 0.01675298623740673\n",
      "batch: 2380 loss: 0.017086109146475792\n",
      "batch: 2390 loss: 0.014708301983773708\n",
      "batch: 2400 loss: 0.014428224414587021\n",
      "batch: 2410 loss: 0.015539003536105156\n",
      "batch: 2420 loss: 0.016468748450279236\n",
      "batch: 2430 loss: 0.021481037139892578\n",
      "batch: 2440 loss: 0.0162259079515934\n",
      "batch: 2450 loss: 0.013217669911682606\n",
      "batch: 2460 loss: 0.02067374251782894\n",
      "batch: 2470 loss: 0.011723858304321766\n",
      "batch: 2480 loss: 0.01651429757475853\n",
      "batch: 2490 loss: 0.01605810597538948\n",
      "batch: 2500 loss: 0.016301892697811127\n",
      "batch: 2510 loss: 0.014251484535634518\n",
      "batch: 2520 loss: 0.020586825907230377\n",
      "batch: 2530 loss: 0.012855280190706253\n",
      "batch: 2540 loss: 0.014586253091692924\n",
      "batch: 2550 loss: 0.014118069782853127\n",
      "batch: 2560 loss: 0.017317181453108788\n",
      "batch: 2570 loss: 0.015253358520567417\n",
      "batch: 2580 loss: 0.015402036719024181\n",
      "batch: 2590 loss: 0.016030246391892433\n",
      "batch: 2600 loss: 0.01733882911503315\n",
      "batch: 2610 loss: 0.02051030844449997\n",
      "batch: 2620 loss: 0.0153053505346179\n",
      "batch: 2630 loss: 0.014760233461856842\n",
      "batch: 2640 loss: 0.01342810969799757\n",
      "batch: 2650 loss: 0.013406218960881233\n",
      "batch: 2660 loss: 0.013211706653237343\n",
      "batch: 2670 loss: 0.015948019921779633\n",
      "batch: 2680 loss: 0.01826784573495388\n",
      "batch: 2690 loss: 0.018084777519106865\n",
      "batch: 2700 loss: 0.01762383244931698\n",
      "batch: 2710 loss: 0.016093431040644646\n",
      "batch: 2720 loss: 0.01704712212085724\n",
      "batch: 2730 loss: 0.013364194892346859\n",
      "batch: 2740 loss: 0.016697600483894348\n",
      "batch: 2750 loss: 0.01506003551185131\n",
      "batch: 2760 loss: 0.015160268172621727\n",
      "batch: 2770 loss: 0.015179824084043503\n",
      "batch: 2780 loss: 0.0160012636333704\n",
      "batch: 2790 loss: 0.01853726990520954\n",
      "batch: 2800 loss: 0.015489999204874039\n",
      "batch: 2810 loss: 0.01526868436485529\n",
      "batch: 2820 loss: 0.010930930264294147\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.014163512736558914\n",
      "batch: 10 loss: 0.016701094806194305\n",
      "batch: 20 loss: 0.01601778157055378\n",
      "batch: 30 loss: 0.013849847950041294\n",
      "batch: 40 loss: 0.012733958661556244\n",
      "batch: 50 loss: 0.014114784076809883\n",
      "batch: 60 loss: 0.013799004256725311\n",
      "batch: 70 loss: 0.014029326848685741\n",
      "batch: 80 loss: 0.013290188275277615\n",
      "batch: 90 loss: 0.015023286454379559\n",
      "batch: 100 loss: 0.012924792245030403\n",
      "batch: 110 loss: 0.01847275160253048\n",
      "batch: 120 loss: 0.013481177389621735\n",
      "batch: 130 loss: 0.015336495824158192\n",
      "batch: 140 loss: 0.013546289876103401\n",
      "batch: 150 loss: 0.014058787375688553\n",
      "batch: 160 loss: 0.014852242544293404\n",
      "batch: 170 loss: 0.014256210066378117\n",
      "batch: 180 loss: 0.016723981127142906\n",
      "batch: 190 loss: 0.01992635615170002\n",
      "batch: 200 loss: 0.014433552511036396\n",
      "batch: 210 loss: 0.01392315048724413\n",
      "batch: 220 loss: 0.017844906076788902\n",
      "batch: 230 loss: 0.01717892661690712\n",
      "batch: 240 loss: 0.014504143968224525\n",
      "batch: 250 loss: 0.015324104577302933\n",
      "batch: 260 loss: 0.015968360006809235\n",
      "batch: 270 loss: 0.016274049878120422\n",
      "batch: 280 loss: 0.01869257166981697\n",
      "batch: 290 loss: 0.014503445476293564\n",
      "batch: 300 loss: 0.015197191387414932\n",
      "batch: 310 loss: 0.017785407602787018\n",
      "batch: 320 loss: 0.011997414752840996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 330 loss: 0.021310904994606972\n",
      "batch: 340 loss: 0.015360064804553986\n",
      "batch: 350 loss: 0.016956482082605362\n",
      "batch: 360 loss: 0.018583333119750023\n",
      "batch: 370 loss: 0.0188651904463768\n",
      "batch: 380 loss: 0.021587787196040154\n",
      "batch: 390 loss: 0.02156706154346466\n",
      "batch: 400 loss: 0.017414212226867676\n",
      "batch: 410 loss: 0.012651750817894936\n",
      "batch: 420 loss: 0.019438473507761955\n",
      "batch: 430 loss: 0.015359834767878056\n",
      "batch: 440 loss: 0.01236298680305481\n",
      "batch: 450 loss: 0.014674843288958073\n",
      "batch: 460 loss: 0.011754032224416733\n",
      "batch: 470 loss: 0.014278887771070004\n",
      "batch: 480 loss: 0.01346044335514307\n",
      "batch: 490 loss: 0.01596326380968094\n",
      "batch: 500 loss: 0.013555273413658142\n",
      "batch: 510 loss: 0.016546662896871567\n",
      "batch: 520 loss: 0.016437100246548653\n",
      "batch: 530 loss: 0.01384586188942194\n",
      "batch: 540 loss: 0.016399061307311058\n",
      "batch: 550 loss: 0.01796155795454979\n",
      "batch: 560 loss: 0.012193076312541962\n",
      "batch: 570 loss: 0.013364381156861782\n",
      "batch: 580 loss: 0.01643962413072586\n",
      "batch: 590 loss: 0.018890725448727608\n",
      "batch: 600 loss: 0.020786523818969727\n",
      "batch: 610 loss: 0.013934383168816566\n",
      "batch: 620 loss: 0.017704598605632782\n",
      "batch: 630 loss: 0.014865349978208542\n",
      "batch: 640 loss: 0.01416181679815054\n",
      "batch: 650 loss: 0.013972810469567776\n",
      "batch: 660 loss: 0.0179540254175663\n",
      "batch: 670 loss: 0.013608632609248161\n",
      "batch: 680 loss: 0.017694519832730293\n",
      "batch: 690 loss: 0.013606006279587746\n",
      "batch: 700 loss: 0.01933879777789116\n",
      "batch: 710 loss: 0.01696372777223587\n",
      "batch: 720 loss: 0.018479514867067337\n",
      "batch: 730 loss: 0.018901625648140907\n",
      "batch: 740 loss: 0.017867278307676315\n",
      "batch: 750 loss: 0.015820715576410294\n",
      "batch: 760 loss: 0.017004569992423058\n",
      "batch: 770 loss: 0.014816956594586372\n",
      "batch: 780 loss: 0.01592029258608818\n",
      "batch: 790 loss: 0.018313920125365257\n",
      "batch: 800 loss: 0.012108084745705128\n",
      "batch: 810 loss: 0.020509997382760048\n",
      "batch: 820 loss: 0.01438655611127615\n",
      "batch: 830 loss: 0.0216086283326149\n",
      "batch: 840 loss: 0.01549606304615736\n",
      "batch: 850 loss: 0.015300855971872807\n",
      "batch: 860 loss: 0.01819535531103611\n",
      "batch: 870 loss: 0.01628238521516323\n",
      "batch: 880 loss: 0.017333822324872017\n",
      "batch: 890 loss: 0.014599434100091457\n",
      "batch: 900 loss: 0.011995715089142323\n",
      "batch: 910 loss: 0.011456718668341637\n",
      "batch: 920 loss: 0.016447462141513824\n",
      "batch: 930 loss: 0.017504414543509483\n",
      "batch: 940 loss: 0.014990059658885002\n",
      "batch: 950 loss: 0.01552264392375946\n",
      "batch: 960 loss: 0.01618078351020813\n",
      "batch: 970 loss: 0.013911116868257523\n",
      "batch: 980 loss: 0.021495627239346504\n",
      "batch: 990 loss: 0.016335410997271538\n",
      "batch: 1000 loss: 0.01645447313785553\n",
      "batch: 1010 loss: 0.02062828280031681\n",
      "batch: 1020 loss: 0.013353349640965462\n",
      "batch: 1030 loss: 0.01614532247185707\n",
      "batch: 1040 loss: 0.015844395384192467\n",
      "batch: 1050 loss: 0.016013137996196747\n",
      "batch: 1060 loss: 0.017521051689982414\n",
      "batch: 1070 loss: 0.01440243236720562\n",
      "batch: 1080 loss: 0.018609125167131424\n",
      "batch: 1090 loss: 0.014770571142435074\n",
      "batch: 1100 loss: 0.013910406269133091\n",
      "batch: 1110 loss: 0.014131066389381886\n",
      "batch: 1120 loss: 0.018166566267609596\n",
      "batch: 1130 loss: 0.01976446621119976\n",
      "batch: 1140 loss: 0.015666000545024872\n",
      "batch: 1150 loss: 0.01849527843296528\n",
      "batch: 1160 loss: 0.015950871631503105\n",
      "batch: 1170 loss: 0.012002804316580296\n",
      "batch: 1180 loss: 0.015899082645773888\n",
      "batch: 1190 loss: 0.010227096267044544\n",
      "batch: 1200 loss: 0.012585821561515331\n",
      "batch: 1210 loss: 0.017760980874300003\n",
      "batch: 1220 loss: 0.015249163843691349\n",
      "batch: 1230 loss: 0.016435008496046066\n",
      "batch: 1240 loss: 0.01690123975276947\n",
      "batch: 1250 loss: 0.017222357913851738\n",
      "batch: 1260 loss: 0.014825507067143917\n",
      "batch: 1270 loss: 0.01566818729043007\n",
      "batch: 1280 loss: 0.019657814875245094\n",
      "batch: 1290 loss: 0.011580872349441051\n",
      "batch: 1300 loss: 0.016007909551262856\n",
      "batch: 1310 loss: 0.013047382235527039\n",
      "batch: 1320 loss: 0.017730869352817535\n",
      "batch: 1330 loss: 0.01610729657113552\n",
      "batch: 1340 loss: 0.013018690049648285\n",
      "batch: 1350 loss: 0.018962686881422997\n",
      "batch: 1360 loss: 0.015101084485650063\n",
      "batch: 1370 loss: 0.01597115769982338\n",
      "batch: 1380 loss: 0.013203565962612629\n",
      "batch: 1390 loss: 0.011431286111474037\n",
      "batch: 1400 loss: 0.015719961374998093\n",
      "batch: 1410 loss: 0.01701745204627514\n",
      "batch: 1420 loss: 0.01893354393541813\n",
      "batch: 1430 loss: 0.01511429250240326\n",
      "batch: 1440 loss: 0.01595110259950161\n",
      "batch: 1450 loss: 0.017439022660255432\n",
      "batch: 1460 loss: 0.019061727449297905\n",
      "batch: 1470 loss: 0.01685563288629055\n",
      "batch: 1480 loss: 0.01566813886165619\n",
      "batch: 1490 loss: 0.015702111646533012\n",
      "batch: 1500 loss: 0.016802573576569557\n",
      "batch: 1510 loss: 0.014274442568421364\n",
      "batch: 1520 loss: 0.01745063252747059\n",
      "batch: 1530 loss: 0.0197977926582098\n",
      "batch: 1540 loss: 0.01434206124395132\n",
      "batch: 1550 loss: 0.016150346025824547\n",
      "batch: 1560 loss: 0.016572395339608192\n",
      "batch: 1570 loss: 0.014045094139873981\n",
      "batch: 1580 loss: 0.011810087598860264\n",
      "batch: 1590 loss: 0.015405235812067986\n",
      "batch: 1600 loss: 0.020610561594367027\n",
      "batch: 1610 loss: 0.01601160317659378\n",
      "batch: 1620 loss: 0.012017344124615192\n",
      "batch: 1630 loss: 0.015745719894766808\n",
      "batch: 1640 loss: 0.014982105232775211\n",
      "batch: 1650 loss: 0.016128210350871086\n",
      "batch: 1660 loss: 0.01281252596527338\n",
      "batch: 1670 loss: 0.019855540245771408\n",
      "batch: 1680 loss: 0.015063674189150333\n",
      "batch: 1690 loss: 0.012857136316597462\n",
      "batch: 1700 loss: 0.015749001875519753\n",
      "batch: 1710 loss: 0.020122142508625984\n",
      "batch: 1720 loss: 0.020240701735019684\n",
      "batch: 1730 loss: 0.015451236627995968\n",
      "batch: 1740 loss: 0.014424033463001251\n",
      "batch: 1750 loss: 0.014069260098040104\n",
      "batch: 1760 loss: 0.013454760424792767\n",
      "batch: 1770 loss: 0.015700818970799446\n",
      "batch: 1780 loss: 0.014739494770765305\n",
      "batch: 1790 loss: 0.014409481547772884\n",
      "batch: 1800 loss: 0.015801450237631798\n",
      "batch: 1810 loss: 0.018279926851391792\n",
      "batch: 1820 loss: 0.016414474695920944\n",
      "batch: 1830 loss: 0.017053479328751564\n",
      "batch: 1840 loss: 0.016561761498451233\n",
      "batch: 1850 loss: 0.019375143572688103\n",
      "batch: 1860 loss: 0.01780703291296959\n",
      "batch: 1870 loss: 0.017159465700387955\n",
      "batch: 1880 loss: 0.013764613308012486\n",
      "batch: 1890 loss: 0.018343249335885048\n",
      "batch: 1900 loss: 0.0170380100607872\n",
      "batch: 1910 loss: 0.014097429811954498\n",
      "batch: 1920 loss: 0.016770901158452034\n",
      "batch: 1930 loss: 0.017902439460158348\n",
      "batch: 1940 loss: 0.016130080446600914\n",
      "batch: 1950 loss: 0.018032336607575417\n",
      "batch: 1960 loss: 0.013839766383171082\n",
      "batch: 1970 loss: 0.013034719042479992\n",
      "batch: 1980 loss: 0.015081748366355896\n",
      "batch: 1990 loss: 0.01669965125620365\n",
      "batch: 2000 loss: 0.017480777576565742\n",
      "batch: 2010 loss: 0.011802867986261845\n",
      "batch: 2020 loss: 0.01573796384036541\n",
      "batch: 2030 loss: 0.01649961806833744\n",
      "batch: 2040 loss: 0.015430270694196224\n",
      "batch: 2050 loss: 0.01314796507358551\n",
      "batch: 2060 loss: 0.012272564694285393\n",
      "batch: 2070 loss: 0.01328984834253788\n",
      "batch: 2080 loss: 0.01434585265815258\n",
      "batch: 2090 loss: 0.018140707165002823\n",
      "batch: 2100 loss: 0.014275260269641876\n",
      "batch: 2110 loss: 0.017252149060368538\n",
      "batch: 2120 loss: 0.015116136521100998\n",
      "batch: 2130 loss: 0.012582565657794476\n",
      "batch: 2140 loss: 0.012339862994849682\n",
      "batch: 2150 loss: 0.016727492213249207\n",
      "batch: 2160 loss: 0.014721912331879139\n",
      "batch: 2170 loss: 0.012588556855916977\n",
      "batch: 2180 loss: 0.01964658498764038\n",
      "batch: 2190 loss: 0.020200978964567184\n",
      "batch: 2200 loss: 0.01571064256131649\n",
      "batch: 2210 loss: 0.01589163951575756\n",
      "batch: 2220 loss: 0.019088512286543846\n",
      "batch: 2230 loss: 0.017370175570249557\n",
      "batch: 2240 loss: 0.017223183065652847\n",
      "batch: 2250 loss: 0.01574004255235195\n",
      "batch: 2260 loss: 0.012207955121994019\n",
      "batch: 2270 loss: 0.01705714873969555\n",
      "batch: 2280 loss: 0.015948230400681496\n",
      "batch: 2290 loss: 0.017113281413912773\n",
      "batch: 2300 loss: 0.013833210803568363\n",
      "batch: 2310 loss: 0.021810099482536316\n",
      "batch: 2320 loss: 0.016698040068149567\n",
      "batch: 2330 loss: 0.01546532940119505\n",
      "batch: 2340 loss: 0.015226749703288078\n",
      "batch: 2350 loss: 0.015081913210451603\n",
      "batch: 2360 loss: 0.014337057247757912\n",
      "batch: 2370 loss: 0.015175051987171173\n",
      "batch: 2380 loss: 0.014587756246328354\n",
      "batch: 2390 loss: 0.016744110733270645\n",
      "batch: 2400 loss: 0.015930229797959328\n",
      "batch: 2410 loss: 0.01794472336769104\n",
      "batch: 2420 loss: 0.01853804662823677\n",
      "batch: 2430 loss: 0.013314896263182163\n",
      "batch: 2440 loss: 0.01336492970585823\n",
      "batch: 2450 loss: 0.01692884974181652\n",
      "batch: 2460 loss: 0.014307469129562378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2470 loss: 0.01793276146054268\n",
      "batch: 2480 loss: 0.02051488868892193\n",
      "batch: 2490 loss: 0.015842007473111153\n",
      "batch: 2500 loss: 0.013647163286805153\n",
      "batch: 2510 loss: 0.017017055302858353\n",
      "batch: 2520 loss: 0.015078729949891567\n",
      "batch: 2530 loss: 0.012228304520249367\n",
      "batch: 2540 loss: 0.01614956371486187\n",
      "batch: 2550 loss: 0.011616903357207775\n",
      "batch: 2560 loss: 0.017734218388795853\n",
      "batch: 2570 loss: 0.016646696254611015\n",
      "batch: 2580 loss: 0.015258122235536575\n",
      "batch: 2590 loss: 0.017944693565368652\n",
      "batch: 2600 loss: 0.01757715456187725\n",
      "batch: 2610 loss: 0.015597180463373661\n",
      "batch: 2620 loss: 0.019252970814704895\n",
      "batch: 2630 loss: 0.016033537685871124\n",
      "batch: 2640 loss: 0.015572864562273026\n",
      "batch: 2650 loss: 0.01704533025622368\n",
      "batch: 2660 loss: 0.021241480484604836\n",
      "batch: 2670 loss: 0.016861146315932274\n",
      "batch: 2680 loss: 0.01917967014014721\n",
      "batch: 2690 loss: 0.013398569077253342\n",
      "batch: 2700 loss: 0.012645562179386616\n",
      "batch: 2710 loss: 0.015676941722631454\n",
      "batch: 2720 loss: 0.012410970404744148\n",
      "batch: 2730 loss: 0.01846400834619999\n",
      "batch: 2740 loss: 0.016812186688184738\n",
      "batch: 2750 loss: 0.016304058954119682\n",
      "batch: 2760 loss: 0.01557396911084652\n",
      "batch: 2770 loss: 0.020445121452212334\n",
      "batch: 2780 loss: 0.014464142732322216\n",
      "batch: 2790 loss: 0.010467609390616417\n",
      "batch: 2800 loss: 0.013741033151745796\n",
      "batch: 2810 loss: 0.013309202156960964\n",
      "batch: 2820 loss: 0.01657690666615963\n",
      "round  7 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.02104056626558304\n",
      "batch: 10 loss: 0.0145307257771492\n",
      "batch: 20 loss: 0.013605617918074131\n",
      "batch: 30 loss: 0.016349194571375847\n",
      "batch: 40 loss: 0.019765056669712067\n",
      "batch: 50 loss: 0.016876401379704475\n",
      "batch: 60 loss: 0.014437727630138397\n",
      "batch: 70 loss: 0.01377497985959053\n",
      "batch: 80 loss: 0.014284678734838963\n",
      "batch: 90 loss: 0.015833687037229538\n",
      "batch: 100 loss: 0.017671825364232063\n",
      "batch: 110 loss: 0.017393790185451508\n",
      "batch: 120 loss: 0.013625329360365868\n",
      "batch: 130 loss: 0.013850677758455276\n",
      "batch: 140 loss: 0.014665639027953148\n",
      "batch: 150 loss: 0.018200555816292763\n",
      "batch: 160 loss: 0.020898612216114998\n",
      "batch: 170 loss: 0.018628278747200966\n",
      "batch: 180 loss: 0.014293136075139046\n",
      "batch: 190 loss: 0.014571623876690865\n",
      "batch: 200 loss: 0.01530720666050911\n",
      "batch: 210 loss: 0.01641259156167507\n",
      "batch: 220 loss: 0.013592178001999855\n",
      "batch: 230 loss: 0.013438187539577484\n",
      "batch: 240 loss: 0.014082716777920723\n",
      "batch: 250 loss: 0.02041911892592907\n",
      "batch: 260 loss: 0.018717775121331215\n",
      "batch: 270 loss: 0.013691960833966732\n",
      "batch: 280 loss: 0.015605934895575047\n",
      "batch: 290 loss: 0.015031824819743633\n",
      "batch: 300 loss: 0.021268613636493683\n",
      "batch: 310 loss: 0.013111869804561138\n",
      "batch: 320 loss: 0.01621788553893566\n",
      "batch: 330 loss: 0.01675334945321083\n",
      "batch: 340 loss: 0.014583922922611237\n",
      "batch: 350 loss: 0.01703682914376259\n",
      "batch: 360 loss: 0.015361940488219261\n",
      "batch: 370 loss: 0.018357599154114723\n",
      "batch: 380 loss: 0.015337212942540646\n",
      "batch: 390 loss: 0.015217278152704239\n",
      "batch: 400 loss: 0.012589548714458942\n",
      "batch: 410 loss: 0.01801082119345665\n",
      "batch: 420 loss: 0.017583057284355164\n",
      "batch: 430 loss: 0.015092906542122364\n",
      "batch: 440 loss: 0.015346952714025974\n",
      "batch: 450 loss: 0.015334292314946651\n",
      "batch: 460 loss: 0.017193716019392014\n",
      "batch: 470 loss: 0.01960989460349083\n",
      "batch: 480 loss: 0.015545406378805637\n",
      "batch: 490 loss: 0.015849770978093147\n",
      "batch: 500 loss: 0.014955098740756512\n",
      "batch: 510 loss: 0.016866536810994148\n",
      "batch: 520 loss: 0.017608538269996643\n",
      "batch: 530 loss: 0.0158748347312212\n",
      "batch: 540 loss: 0.01440231129527092\n",
      "batch: 550 loss: 0.01741190068423748\n",
      "batch: 560 loss: 0.014422455802559853\n",
      "batch: 570 loss: 0.013141036033630371\n",
      "batch: 580 loss: 0.019090700894594193\n",
      "batch: 590 loss: 0.017385350540280342\n",
      "batch: 600 loss: 0.01466386765241623\n",
      "batch: 610 loss: 0.013971403241157532\n",
      "batch: 620 loss: 0.012864316813647747\n",
      "batch: 630 loss: 0.0196984875947237\n",
      "batch: 640 loss: 0.019214745610952377\n",
      "batch: 650 loss: 0.013246702961623669\n",
      "batch: 660 loss: 0.015521079301834106\n",
      "batch: 670 loss: 0.013754533603787422\n",
      "batch: 680 loss: 0.01438660454005003\n",
      "batch: 690 loss: 0.01284660305827856\n",
      "batch: 700 loss: 0.01565524749457836\n",
      "batch: 710 loss: 0.016895215958356857\n",
      "batch: 720 loss: 0.015908703207969666\n",
      "batch: 730 loss: 0.018609682098031044\n",
      "batch: 740 loss: 0.014141516759991646\n",
      "batch: 750 loss: 0.014592980965971947\n",
      "batch: 760 loss: 0.018815893679857254\n",
      "batch: 770 loss: 0.017672913148999214\n",
      "batch: 780 loss: 0.01473047211766243\n",
      "batch: 790 loss: 0.019445056095719337\n",
      "batch: 800 loss: 0.01687527634203434\n",
      "batch: 810 loss: 0.017273029312491417\n",
      "batch: 820 loss: 0.0162791945040226\n",
      "batch: 830 loss: 0.012913184240460396\n",
      "batch: 840 loss: 0.019768429920077324\n",
      "batch: 850 loss: 0.018175819888710976\n",
      "batch: 860 loss: 0.019528038799762726\n",
      "batch: 870 loss: 0.018061712384223938\n",
      "batch: 880 loss: 0.013343516737222672\n",
      "batch: 890 loss: 0.013165168464183807\n",
      "batch: 900 loss: 0.01186427567154169\n",
      "batch: 910 loss: 0.01553456299006939\n",
      "batch: 920 loss: 0.01654527150094509\n",
      "batch: 930 loss: 0.017201589420437813\n",
      "batch: 940 loss: 0.0187004953622818\n",
      "batch: 950 loss: 0.0163058303296566\n",
      "batch: 960 loss: 0.015095539391040802\n",
      "batch: 970 loss: 0.013039986602962017\n",
      "batch: 980 loss: 0.0160236693918705\n",
      "batch: 990 loss: 0.017471682280302048\n",
      "batch: 1000 loss: 0.019608603790402412\n",
      "batch: 1010 loss: 0.019823191687464714\n",
      "batch: 1020 loss: 0.017291592434048653\n",
      "batch: 1030 loss: 0.014955236576497555\n",
      "batch: 1040 loss: 0.01734311506152153\n",
      "batch: 1050 loss: 0.016835594549775124\n",
      "batch: 1060 loss: 0.012006811797618866\n",
      "batch: 1070 loss: 0.011211233213543892\n",
      "batch: 1080 loss: 0.014487192966043949\n",
      "batch: 1090 loss: 0.015734907239675522\n",
      "batch: 1100 loss: 0.015268180519342422\n",
      "batch: 1110 loss: 0.015554999932646751\n",
      "batch: 1120 loss: 0.019096404314041138\n",
      "batch: 1130 loss: 0.016099898144602776\n",
      "batch: 1140 loss: 0.016093958169221878\n",
      "batch: 1150 loss: 0.015129036270081997\n",
      "batch: 1160 loss: 0.012470761314034462\n",
      "batch: 1170 loss: 0.012493137270212173\n",
      "batch: 1180 loss: 0.016240661963820457\n",
      "batch: 1190 loss: 0.01696605794131756\n",
      "batch: 1200 loss: 0.017102068290114403\n",
      "batch: 1210 loss: 0.020870622247457504\n",
      "batch: 1220 loss: 0.019344257190823555\n",
      "batch: 1230 loss: 0.02162245847284794\n",
      "batch: 1240 loss: 0.015738721936941147\n",
      "batch: 1250 loss: 0.014173856005072594\n",
      "batch: 1260 loss: 0.019236383959650993\n",
      "batch: 1270 loss: 0.012653514742851257\n",
      "batch: 1280 loss: 0.01615067757666111\n",
      "batch: 1290 loss: 0.016517216339707375\n",
      "batch: 1300 loss: 0.012646442279219627\n",
      "batch: 1310 loss: 0.015540453605353832\n",
      "batch: 1320 loss: 0.013015526346862316\n",
      "batch: 1330 loss: 0.015094351023435593\n",
      "batch: 1340 loss: 0.016721345484256744\n",
      "batch: 1350 loss: 0.020113505423069\n",
      "batch: 1360 loss: 0.017810113728046417\n",
      "batch: 1370 loss: 0.01544618234038353\n",
      "batch: 1380 loss: 0.02179679088294506\n",
      "batch: 1390 loss: 0.017709946259856224\n",
      "batch: 1400 loss: 0.015545130707323551\n",
      "batch: 1410 loss: 0.014143758453428745\n",
      "batch: 1420 loss: 0.014579080045223236\n",
      "batch: 1430 loss: 0.014321602880954742\n",
      "batch: 1440 loss: 0.015898432582616806\n",
      "batch: 1450 loss: 0.016709845513105392\n",
      "batch: 1460 loss: 0.019257305189967155\n",
      "batch: 1470 loss: 0.01804833486676216\n",
      "batch: 1480 loss: 0.012979008257389069\n",
      "batch: 1490 loss: 0.012301466427743435\n",
      "batch: 1500 loss: 0.01714019663631916\n",
      "batch: 1510 loss: 0.017491979524493217\n",
      "batch: 1520 loss: 0.01997116394340992\n",
      "batch: 1530 loss: 0.01414401177316904\n",
      "batch: 1540 loss: 0.017356591299176216\n",
      "batch: 1550 loss: 0.01301907654851675\n",
      "batch: 1560 loss: 0.016494974493980408\n",
      "batch: 1570 loss: 0.011441654525697231\n",
      "batch: 1580 loss: 0.012017056345939636\n",
      "batch: 1590 loss: 0.01819443143904209\n",
      "batch: 1600 loss: 0.019260413944721222\n",
      "batch: 1610 loss: 0.014388453215360641\n",
      "batch: 1620 loss: 0.01695823483169079\n",
      "batch: 1630 loss: 0.015311666764318943\n",
      "batch: 1640 loss: 0.013875320553779602\n",
      "batch: 1650 loss: 0.018897470086812973\n",
      "batch: 1660 loss: 0.012909024953842163\n",
      "batch: 1670 loss: 0.014397894032299519\n",
      "batch: 1680 loss: 0.016405625268816948\n",
      "batch: 1690 loss: 0.016376204788684845\n",
      "batch: 1700 loss: 0.015094861388206482\n",
      "batch: 1710 loss: 0.017891021445393562\n",
      "batch: 1720 loss: 0.014703989960253239\n",
      "batch: 1730 loss: 0.01584623195230961\n",
      "batch: 1740 loss: 0.012703715823590755\n",
      "batch: 1750 loss: 0.012248367071151733\n",
      "batch: 1760 loss: 0.015536608174443245\n",
      "batch: 1770 loss: 0.015960508957505226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1780 loss: 0.012223814614117146\n",
      "batch: 1790 loss: 0.01774648018181324\n",
      "batch: 1800 loss: 0.013512959703803062\n",
      "batch: 1810 loss: 0.014347653836011887\n",
      "batch: 1820 loss: 0.012228093110024929\n",
      "batch: 1830 loss: 0.0201145987957716\n",
      "batch: 1840 loss: 0.013340819627046585\n",
      "batch: 1850 loss: 0.018564531579613686\n",
      "batch: 1860 loss: 0.012995428405702114\n",
      "batch: 1870 loss: 0.014770173467695713\n",
      "batch: 1880 loss: 0.0159915778785944\n",
      "batch: 1890 loss: 0.016933953389525414\n",
      "batch: 1900 loss: 0.018167413771152496\n",
      "batch: 1910 loss: 0.01908489130437374\n",
      "batch: 1920 loss: 0.016031375154852867\n",
      "batch: 1930 loss: 0.01785615272819996\n",
      "batch: 1940 loss: 0.013295244425535202\n",
      "batch: 1950 loss: 0.018056994304060936\n",
      "batch: 1960 loss: 0.015065760351717472\n",
      "batch: 1970 loss: 0.015936877578496933\n",
      "batch: 1980 loss: 0.01752951554954052\n",
      "batch: 1990 loss: 0.013056526891887188\n",
      "batch: 2000 loss: 0.014356872998178005\n",
      "batch: 2010 loss: 0.013802291825413704\n",
      "batch: 2020 loss: 0.015339015051722527\n",
      "batch: 2030 loss: 0.014783462509512901\n",
      "batch: 2040 loss: 0.016171950846910477\n",
      "batch: 2050 loss: 0.01637358032166958\n",
      "batch: 2060 loss: 0.011826226487755775\n",
      "batch: 2070 loss: 0.016627395525574684\n",
      "batch: 2080 loss: 0.015780115500092506\n",
      "batch: 2090 loss: 0.017734667286276817\n",
      "batch: 2100 loss: 0.020666271448135376\n",
      "batch: 2110 loss: 0.015021532773971558\n",
      "batch: 2120 loss: 0.021794673055410385\n",
      "batch: 2130 loss: 0.018306421115994453\n",
      "batch: 2140 loss: 0.020208856090903282\n",
      "batch: 2150 loss: 0.015940478071570396\n",
      "batch: 2160 loss: 0.015388647094368935\n",
      "batch: 2170 loss: 0.016545459628105164\n",
      "batch: 2180 loss: 0.01691437140107155\n",
      "batch: 2190 loss: 0.012231220491230488\n",
      "batch: 2200 loss: 0.01665368676185608\n",
      "batch: 2210 loss: 0.016712021082639694\n",
      "batch: 2220 loss: 0.016138596460223198\n",
      "batch: 2230 loss: 0.01891327276825905\n",
      "batch: 2240 loss: 0.024901874363422394\n",
      "batch: 2250 loss: 0.013165968470275402\n",
      "batch: 2260 loss: 0.01641005463898182\n",
      "batch: 2270 loss: 0.01522053498774767\n",
      "batch: 2280 loss: 0.015604647807776928\n",
      "batch: 2290 loss: 0.013680033385753632\n",
      "batch: 2300 loss: 0.012625480070710182\n",
      "batch: 2310 loss: 0.013749731704592705\n",
      "batch: 2320 loss: 0.010672030970454216\n",
      "batch: 2330 loss: 0.014845495112240314\n",
      "batch: 2340 loss: 0.019849086180329323\n",
      "batch: 2350 loss: 0.01208022702485323\n",
      "batch: 2360 loss: 0.018131857737898827\n",
      "batch: 2370 loss: 0.011294715106487274\n",
      "batch: 2380 loss: 0.013950595632195473\n",
      "batch: 2390 loss: 0.01480155810713768\n",
      "batch: 2400 loss: 0.014633913524448872\n",
      "batch: 2410 loss: 0.012585535645484924\n",
      "batch: 2420 loss: 0.01359084527939558\n",
      "batch: 2430 loss: 0.014296058565378189\n",
      "batch: 2440 loss: 0.014071336016058922\n",
      "batch: 2450 loss: 0.017610304057598114\n",
      "batch: 2460 loss: 0.015488885343074799\n",
      "batch: 2470 loss: 0.016084566712379456\n",
      "batch: 2480 loss: 0.013332953676581383\n",
      "batch: 2490 loss: 0.017506983131170273\n",
      "batch: 2500 loss: 0.019908731803297997\n",
      "batch: 2510 loss: 0.01408720389008522\n",
      "batch: 2520 loss: 0.01672615483403206\n",
      "batch: 2530 loss: 0.01655466854572296\n",
      "batch: 2540 loss: 0.014439662918448448\n",
      "batch: 2550 loss: 0.018898798152804375\n",
      "batch: 2560 loss: 0.01295801904052496\n",
      "batch: 2570 loss: 0.01712913252413273\n",
      "batch: 2580 loss: 0.01310090720653534\n",
      "batch: 2590 loss: 0.011865928769111633\n",
      "batch: 2600 loss: 0.0217745378613472\n",
      "batch: 2610 loss: 0.014905617572367191\n",
      "batch: 2620 loss: 0.013080550357699394\n",
      "batch: 2630 loss: 0.017681395635008812\n",
      "batch: 2640 loss: 0.014734462834894657\n",
      "batch: 2650 loss: 0.013320062309503555\n",
      "batch: 2660 loss: 0.013200427405536175\n",
      "batch: 2670 loss: 0.016301065683364868\n",
      "batch: 2680 loss: 0.018616555258631706\n",
      "batch: 2690 loss: 0.01820787414908409\n",
      "batch: 2700 loss: 0.014264893718063831\n",
      "batch: 2710 loss: 0.015056991018354893\n",
      "batch: 2720 loss: 0.01940351352095604\n",
      "batch: 2730 loss: 0.014871174469590187\n",
      "batch: 2740 loss: 0.01642320305109024\n",
      "batch: 2750 loss: 0.014074916951358318\n",
      "batch: 2760 loss: 0.01666138879954815\n",
      "batch: 2770 loss: 0.013966350816190243\n",
      "batch: 2780 loss: 0.015022100880742073\n",
      "batch: 2790 loss: 0.018166380003094673\n",
      "batch: 2800 loss: 0.012309463694691658\n",
      "batch: 2810 loss: 0.01924341544508934\n",
      "batch: 2820 loss: 0.015563693828880787\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.016613321378827095\n",
      "batch: 10 loss: 0.014415878802537918\n",
      "batch: 20 loss: 0.014580531977117062\n",
      "batch: 30 loss: 0.014770719222724438\n",
      "batch: 40 loss: 0.014414395205676556\n",
      "batch: 50 loss: 0.01903402805328369\n",
      "batch: 60 loss: 0.014037246815860271\n",
      "batch: 70 loss: 0.013645945116877556\n",
      "batch: 80 loss: 0.013834509998559952\n",
      "batch: 90 loss: 0.013940528966486454\n",
      "batch: 100 loss: 0.013288878835737705\n",
      "batch: 110 loss: 0.016694890335202217\n",
      "batch: 120 loss: 0.016977554187178612\n",
      "batch: 130 loss: 0.014032181352376938\n",
      "batch: 140 loss: 0.013195419684052467\n",
      "batch: 150 loss: 0.01452880259603262\n",
      "batch: 160 loss: 0.018464012071490288\n",
      "batch: 170 loss: 0.017226513475179672\n",
      "batch: 180 loss: 0.014566989615559578\n",
      "batch: 190 loss: 0.012182424776256084\n",
      "batch: 200 loss: 0.016796987503767014\n",
      "batch: 210 loss: 0.013442006893455982\n",
      "batch: 220 loss: 0.014511597342789173\n",
      "batch: 230 loss: 0.017661303281784058\n",
      "batch: 240 loss: 0.013515983708202839\n",
      "batch: 250 loss: 0.014363904483616352\n",
      "batch: 260 loss: 0.014260202646255493\n",
      "batch: 270 loss: 0.01893478073179722\n",
      "batch: 280 loss: 0.013345104642212391\n",
      "batch: 290 loss: 0.01658009923994541\n",
      "batch: 300 loss: 0.013694800436496735\n",
      "batch: 310 loss: 0.014101765118539333\n",
      "batch: 320 loss: 0.012796442955732346\n",
      "batch: 330 loss: 0.01706702820956707\n",
      "batch: 340 loss: 0.015468008816242218\n",
      "batch: 350 loss: 0.01413057092577219\n",
      "batch: 360 loss: 0.01669163815677166\n",
      "batch: 370 loss: 0.01716175489127636\n",
      "batch: 380 loss: 0.015261654742062092\n",
      "batch: 390 loss: 0.015489758923649788\n",
      "batch: 400 loss: 0.012430357746779919\n",
      "batch: 410 loss: 0.015104171819984913\n",
      "batch: 420 loss: 0.01769106835126877\n",
      "batch: 430 loss: 0.013551289215683937\n",
      "batch: 440 loss: 0.019101951271295547\n",
      "batch: 450 loss: 0.01960257813334465\n",
      "batch: 460 loss: 0.015417961403727531\n",
      "batch: 470 loss: 0.016301365569233894\n",
      "batch: 480 loss: 0.015306022949516773\n",
      "batch: 490 loss: 0.012317243032157421\n",
      "batch: 500 loss: 0.013356262817978859\n",
      "batch: 510 loss: 0.017742255702614784\n",
      "batch: 520 loss: 0.017998134717345238\n",
      "batch: 530 loss: 0.016292588785290718\n",
      "batch: 540 loss: 0.013308756053447723\n",
      "batch: 550 loss: 0.01497651357203722\n",
      "batch: 560 loss: 0.013208958320319653\n",
      "batch: 570 loss: 0.011390677653253078\n",
      "batch: 580 loss: 0.014554828405380249\n",
      "batch: 590 loss: 0.01462540589272976\n",
      "batch: 600 loss: 0.01554069947451353\n",
      "batch: 610 loss: 0.02121552638709545\n",
      "batch: 620 loss: 0.016187159344553947\n",
      "batch: 630 loss: 0.015785809606313705\n",
      "batch: 640 loss: 0.01534842886030674\n",
      "batch: 650 loss: 0.015456915833055973\n",
      "batch: 660 loss: 0.019735189154744148\n",
      "batch: 670 loss: 0.016337990760803223\n",
      "batch: 680 loss: 0.012723475694656372\n",
      "batch: 690 loss: 0.010514756664633751\n",
      "batch: 700 loss: 0.022684117779135704\n",
      "batch: 710 loss: 0.01528368890285492\n",
      "batch: 720 loss: 0.01661299727857113\n",
      "batch: 730 loss: 0.013783708214759827\n",
      "batch: 740 loss: 0.011335588060319424\n",
      "batch: 750 loss: 0.013233599252998829\n",
      "batch: 760 loss: 0.013825621455907822\n",
      "batch: 770 loss: 0.018890993669629097\n",
      "batch: 780 loss: 0.014227122999727726\n",
      "batch: 790 loss: 0.016074422746896744\n",
      "batch: 800 loss: 0.0181451253592968\n",
      "batch: 810 loss: 0.015652332454919815\n",
      "batch: 820 loss: 0.01612156257033348\n",
      "batch: 830 loss: 0.013169976882636547\n",
      "batch: 840 loss: 0.014993452467024326\n",
      "batch: 850 loss: 0.01911192201077938\n",
      "batch: 860 loss: 0.01402717363089323\n",
      "batch: 870 loss: 0.01592656597495079\n",
      "batch: 880 loss: 0.015720130875706673\n",
      "batch: 890 loss: 0.015384787693619728\n",
      "batch: 900 loss: 0.01565810851752758\n",
      "batch: 910 loss: 0.01746116764843464\n",
      "batch: 920 loss: 0.017187535762786865\n",
      "batch: 930 loss: 0.013304085470736027\n",
      "batch: 940 loss: 0.014845320023596287\n",
      "batch: 950 loss: 0.0167203638702631\n",
      "batch: 960 loss: 0.012183930724859238\n",
      "batch: 970 loss: 0.01696394756436348\n",
      "batch: 980 loss: 0.016513308510184288\n",
      "batch: 990 loss: 0.011408470571041107\n",
      "batch: 1000 loss: 0.020415915176272392\n",
      "batch: 1010 loss: 0.016841651871800423\n",
      "batch: 1020 loss: 0.01832769252359867\n",
      "batch: 1030 loss: 0.018978511914610863\n",
      "batch: 1040 loss: 0.01134566031396389\n",
      "batch: 1050 loss: 0.014058759436011314\n",
      "batch: 1060 loss: 0.017424793913960457\n",
      "batch: 1070 loss: 0.015503153204917908\n",
      "batch: 1080 loss: 0.015325439162552357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1090 loss: 0.014944180846214294\n",
      "batch: 1100 loss: 0.014155104756355286\n",
      "batch: 1110 loss: 0.015275198966264725\n",
      "batch: 1120 loss: 0.019755197688937187\n",
      "batch: 1130 loss: 0.01774013414978981\n",
      "batch: 1140 loss: 0.018558964133262634\n",
      "batch: 1150 loss: 0.01522043626755476\n",
      "batch: 1160 loss: 0.011500449851155281\n",
      "batch: 1170 loss: 0.012279619462788105\n",
      "batch: 1180 loss: 0.012006685137748718\n",
      "batch: 1190 loss: 0.016015073284506798\n",
      "batch: 1200 loss: 0.013150862418115139\n",
      "batch: 1210 loss: 0.014531898312270641\n",
      "batch: 1220 loss: 0.02034199982881546\n",
      "batch: 1230 loss: 0.021789146587252617\n",
      "batch: 1240 loss: 0.011989663355052471\n",
      "batch: 1250 loss: 0.01938042975962162\n",
      "batch: 1260 loss: 0.01678895391523838\n",
      "batch: 1270 loss: 0.018903063610196114\n",
      "batch: 1280 loss: 0.018400194123387337\n",
      "batch: 1290 loss: 0.017415722832083702\n",
      "batch: 1300 loss: 0.017542639747262\n",
      "batch: 1310 loss: 0.012706076726317406\n",
      "batch: 1320 loss: 0.015914564952254295\n",
      "batch: 1330 loss: 0.015252262353897095\n",
      "batch: 1340 loss: 0.014378344640135765\n",
      "batch: 1350 loss: 0.013924508355557919\n",
      "batch: 1360 loss: 0.013962357304990292\n",
      "batch: 1370 loss: 0.01676497608423233\n",
      "batch: 1380 loss: 0.02285456471145153\n",
      "batch: 1390 loss: 0.016334619373083115\n",
      "batch: 1400 loss: 0.01655670255422592\n",
      "batch: 1410 loss: 0.018232015892863274\n",
      "batch: 1420 loss: 0.015666920691728592\n",
      "batch: 1430 loss: 0.02175229974091053\n",
      "batch: 1440 loss: 0.015305503271520138\n",
      "batch: 1450 loss: 0.012448468245565891\n",
      "batch: 1460 loss: 0.013951233588159084\n",
      "batch: 1470 loss: 0.01609136164188385\n",
      "batch: 1480 loss: 0.014764371328055859\n",
      "batch: 1490 loss: 0.013747693039476871\n",
      "batch: 1500 loss: 0.019800828769803047\n",
      "batch: 1510 loss: 0.02025473117828369\n",
      "batch: 1520 loss: 0.02009712904691696\n",
      "batch: 1530 loss: 0.01663823425769806\n",
      "batch: 1540 loss: 0.01593729667365551\n",
      "batch: 1550 loss: 0.014145256020128727\n",
      "batch: 1560 loss: 0.0154333570972085\n",
      "batch: 1570 loss: 0.013300484046339989\n",
      "batch: 1580 loss: 0.014356876723468304\n",
      "batch: 1590 loss: 0.0154070183634758\n",
      "batch: 1600 loss: 0.017313119024038315\n",
      "batch: 1610 loss: 0.014681708067655563\n",
      "batch: 1620 loss: 0.017041416838765144\n",
      "batch: 1630 loss: 0.016116971150040627\n",
      "batch: 1640 loss: 0.013029047288000584\n",
      "batch: 1650 loss: 0.015492349863052368\n",
      "batch: 1660 loss: 0.013512475416064262\n",
      "batch: 1670 loss: 0.018024954944849014\n",
      "batch: 1680 loss: 0.02032775618135929\n",
      "batch: 1690 loss: 0.017792770639061928\n",
      "batch: 1700 loss: 0.017624931409955025\n",
      "batch: 1710 loss: 0.015500329434871674\n",
      "batch: 1720 loss: 0.01861872524023056\n",
      "batch: 1730 loss: 0.021556952968239784\n",
      "batch: 1740 loss: 0.01315565686672926\n",
      "batch: 1750 loss: 0.01760551892220974\n",
      "batch: 1760 loss: 0.014785334467887878\n",
      "batch: 1770 loss: 0.016862334683537483\n",
      "batch: 1780 loss: 0.013544070534408092\n",
      "batch: 1790 loss: 0.013330608606338501\n",
      "batch: 1800 loss: 0.01708660088479519\n",
      "batch: 1810 loss: 0.014771630056202412\n",
      "batch: 1820 loss: 0.01635364256799221\n",
      "batch: 1830 loss: 0.012849547900259495\n",
      "batch: 1840 loss: 0.018672460690140724\n",
      "batch: 1850 loss: 0.013155002146959305\n",
      "batch: 1860 loss: 0.016517886891961098\n",
      "batch: 1870 loss: 0.016939247027039528\n",
      "batch: 1880 loss: 0.016032103449106216\n",
      "batch: 1890 loss: 0.01789218932390213\n",
      "batch: 1900 loss: 0.012208906002342701\n",
      "batch: 1910 loss: 0.013820212334394455\n",
      "batch: 1920 loss: 0.017344385385513306\n",
      "batch: 1930 loss: 0.01815665327012539\n",
      "batch: 1940 loss: 0.01904434524476528\n",
      "batch: 1950 loss: 0.012165809981524944\n",
      "batch: 1960 loss: 0.014167348854243755\n",
      "batch: 1970 loss: 0.016104815527796745\n",
      "batch: 1980 loss: 0.014879557304084301\n",
      "batch: 1990 loss: 0.01538864802569151\n",
      "batch: 2000 loss: 0.010767629370093346\n",
      "batch: 2010 loss: 0.012264498509466648\n",
      "batch: 2020 loss: 0.016591710969805717\n",
      "batch: 2030 loss: 0.013470453210175037\n",
      "batch: 2040 loss: 0.014947655610740185\n",
      "batch: 2050 loss: 0.014714377000927925\n",
      "batch: 2060 loss: 0.015871606767177582\n",
      "batch: 2070 loss: 0.01345645822584629\n",
      "batch: 2080 loss: 0.02075698971748352\n",
      "batch: 2090 loss: 0.013604982756078243\n",
      "batch: 2100 loss: 0.013794921338558197\n",
      "batch: 2110 loss: 0.017977802082896233\n",
      "batch: 2120 loss: 0.012888185679912567\n",
      "batch: 2130 loss: 0.012923852540552616\n",
      "batch: 2140 loss: 0.019544703885912895\n",
      "batch: 2150 loss: 0.01814527064561844\n",
      "batch: 2160 loss: 0.01363677904009819\n",
      "batch: 2170 loss: 0.013445151038467884\n",
      "batch: 2180 loss: 0.015172268263995647\n",
      "batch: 2190 loss: 0.01271761953830719\n",
      "batch: 2200 loss: 0.018976807594299316\n",
      "batch: 2210 loss: 0.015199434943497181\n",
      "batch: 2220 loss: 0.018745537847280502\n",
      "batch: 2230 loss: 0.021370165050029755\n",
      "batch: 2240 loss: 0.013820281252264977\n",
      "batch: 2250 loss: 0.012316673062741756\n",
      "batch: 2260 loss: 0.016633983701467514\n",
      "batch: 2270 loss: 0.016904959455132484\n",
      "batch: 2280 loss: 0.020839843899011612\n",
      "batch: 2290 loss: 0.023866990581154823\n",
      "batch: 2300 loss: 0.015348262153565884\n",
      "batch: 2310 loss: 0.01530426274985075\n",
      "batch: 2320 loss: 0.015270974487066269\n",
      "batch: 2330 loss: 0.01377902738749981\n",
      "batch: 2340 loss: 0.016695167869329453\n",
      "batch: 2350 loss: 0.012780486606061459\n",
      "batch: 2360 loss: 0.024558205157518387\n",
      "batch: 2370 loss: 0.014081920497119427\n",
      "batch: 2380 loss: 0.014806397259235382\n",
      "batch: 2390 loss: 0.019854530692100525\n",
      "batch: 2400 loss: 0.01465083658695221\n",
      "batch: 2410 loss: 0.0155723188072443\n",
      "batch: 2420 loss: 0.015877055004239082\n",
      "batch: 2430 loss: 0.012865292839705944\n",
      "batch: 2440 loss: 0.017174210399389267\n",
      "batch: 2450 loss: 0.010768918320536613\n",
      "batch: 2460 loss: 0.016319841146469116\n",
      "batch: 2470 loss: 0.0145700853317976\n",
      "batch: 2480 loss: 0.01431440468877554\n",
      "batch: 2490 loss: 0.016322750598192215\n",
      "batch: 2500 loss: 0.01632002554833889\n",
      "batch: 2510 loss: 0.01909959316253662\n",
      "batch: 2520 loss: 0.01724698394536972\n",
      "batch: 2530 loss: 0.01597912609577179\n",
      "batch: 2540 loss: 0.016345269978046417\n",
      "batch: 2550 loss: 0.017398618161678314\n",
      "batch: 2560 loss: 0.01868978515267372\n",
      "batch: 2570 loss: 0.010900224559009075\n",
      "batch: 2580 loss: 0.013607710599899292\n",
      "batch: 2590 loss: 0.013758081011474133\n",
      "batch: 2600 loss: 0.017618853598833084\n",
      "batch: 2610 loss: 0.01571091264486313\n",
      "batch: 2620 loss: 0.0202789306640625\n",
      "batch: 2630 loss: 0.016151776537299156\n",
      "batch: 2640 loss: 0.015500841662287712\n",
      "batch: 2650 loss: 0.0173635296523571\n",
      "batch: 2660 loss: 0.01734817586839199\n",
      "batch: 2670 loss: 0.012905944138765335\n",
      "batch: 2680 loss: 0.014714529737830162\n",
      "batch: 2690 loss: 0.011604130268096924\n",
      "batch: 2700 loss: 0.01780538633465767\n",
      "batch: 2710 loss: 0.018382124602794647\n",
      "batch: 2720 loss: 0.01916910521686077\n",
      "batch: 2730 loss: 0.01408909261226654\n",
      "batch: 2740 loss: 0.01953471451997757\n",
      "batch: 2750 loss: 0.013849190436303616\n",
      "batch: 2760 loss: 0.01255000289529562\n",
      "batch: 2770 loss: 0.01618640124797821\n",
      "batch: 2780 loss: 0.01677444949746132\n",
      "batch: 2790 loss: 0.018968814983963966\n",
      "batch: 2800 loss: 0.014937935397028923\n",
      "batch: 2810 loss: 0.0174032561480999\n",
      "batch: 2820 loss: 0.015501348301768303\n",
      "round  8 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.017800265923142433\n",
      "batch: 10 loss: 0.01690109819173813\n",
      "batch: 20 loss: 0.01835428923368454\n",
      "batch: 30 loss: 0.01707310788333416\n",
      "batch: 40 loss: 0.01428251527249813\n",
      "batch: 50 loss: 0.02068646438419819\n",
      "batch: 60 loss: 0.015594558790326118\n",
      "batch: 70 loss: 0.013395978137850761\n",
      "batch: 80 loss: 0.015849431976675987\n",
      "batch: 90 loss: 0.016157647594809532\n",
      "batch: 100 loss: 0.01851683109998703\n",
      "batch: 110 loss: 0.016731388866901398\n",
      "batch: 120 loss: 0.018812935799360275\n",
      "batch: 130 loss: 0.024633904919028282\n",
      "batch: 140 loss: 0.025961417704820633\n",
      "batch: 150 loss: 0.018102504312992096\n",
      "batch: 160 loss: 0.015428912825882435\n",
      "batch: 170 loss: 0.01427640113979578\n",
      "batch: 180 loss: 0.02090519480407238\n",
      "batch: 190 loss: 0.019891420379281044\n",
      "batch: 200 loss: 0.01821514591574669\n",
      "batch: 210 loss: 0.01423137728124857\n",
      "batch: 220 loss: 0.013902087695896626\n",
      "batch: 230 loss: 0.014660375192761421\n",
      "batch: 240 loss: 0.017752185463905334\n",
      "batch: 250 loss: 0.021715300157666206\n",
      "batch: 260 loss: 0.01681644842028618\n",
      "batch: 270 loss: 0.014672238379716873\n",
      "batch: 280 loss: 0.021244220435619354\n",
      "batch: 290 loss: 0.01587824709713459\n",
      "batch: 300 loss: 0.02309930883347988\n",
      "batch: 310 loss: 0.015658063814044\n",
      "batch: 320 loss: 0.014553510583937168\n",
      "batch: 330 loss: 0.01644851453602314\n",
      "batch: 340 loss: 0.01581806316971779\n",
      "batch: 350 loss: 0.016957568004727364\n",
      "batch: 360 loss: 0.015323731116950512\n",
      "batch: 370 loss: 0.02006949856877327\n",
      "batch: 380 loss: 0.014202835969626904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 390 loss: 0.01304622646421194\n",
      "batch: 400 loss: 0.012767079286277294\n",
      "batch: 410 loss: 0.018018122762441635\n",
      "batch: 420 loss: 0.014719733968377113\n",
      "batch: 430 loss: 0.01714586839079857\n",
      "batch: 440 loss: 0.02184535749256611\n",
      "batch: 450 loss: 0.01296610664576292\n",
      "batch: 460 loss: 0.01852951943874359\n",
      "batch: 470 loss: 0.01957450620830059\n",
      "batch: 480 loss: 0.017886288464069366\n",
      "batch: 490 loss: 0.013101349584758282\n",
      "batch: 500 loss: 0.012368024326860905\n",
      "batch: 510 loss: 0.013539410196244717\n",
      "batch: 520 loss: 0.015350500121712685\n",
      "batch: 530 loss: 0.017351841554045677\n",
      "batch: 540 loss: 0.012603864073753357\n",
      "batch: 550 loss: 0.017180610448122025\n",
      "batch: 560 loss: 0.02121751569211483\n",
      "batch: 570 loss: 0.022238213568925858\n",
      "batch: 580 loss: 0.017892692238092422\n",
      "batch: 590 loss: 0.022285975515842438\n",
      "batch: 600 loss: 0.019704269245266914\n",
      "batch: 610 loss: 0.01437795627862215\n",
      "batch: 620 loss: 0.014260105788707733\n",
      "batch: 630 loss: 0.015410888940095901\n",
      "batch: 640 loss: 0.01677156426012516\n",
      "batch: 650 loss: 0.014692909084260464\n",
      "batch: 660 loss: 0.015779878944158554\n",
      "batch: 670 loss: 0.022324398159980774\n",
      "batch: 680 loss: 0.017308324575424194\n",
      "batch: 690 loss: 0.015067558735609055\n",
      "batch: 700 loss: 0.019731098785996437\n",
      "batch: 710 loss: 0.01718623749911785\n",
      "batch: 720 loss: 0.01679069735109806\n",
      "batch: 730 loss: 0.018557026982307434\n",
      "batch: 740 loss: 0.01439265813678503\n",
      "batch: 750 loss: 0.0172419510781765\n",
      "batch: 760 loss: 0.017772920429706573\n",
      "batch: 770 loss: 0.018090838566422462\n",
      "batch: 780 loss: 0.01889040879905224\n",
      "batch: 790 loss: 0.01724977232515812\n",
      "batch: 800 loss: 0.015388989821076393\n",
      "batch: 810 loss: 0.02341083623468876\n",
      "batch: 820 loss: 0.017587898299098015\n",
      "batch: 830 loss: 0.024450650438666344\n",
      "batch: 840 loss: 0.02052370272576809\n",
      "batch: 850 loss: 0.022915206849575043\n",
      "batch: 860 loss: 0.015004151500761509\n",
      "batch: 870 loss: 0.013464641757309437\n",
      "batch: 880 loss: 0.016755713149905205\n",
      "batch: 890 loss: 0.01489927712827921\n",
      "batch: 900 loss: 0.020721882581710815\n",
      "batch: 910 loss: 0.017732514068484306\n",
      "batch: 920 loss: 0.018306244164705276\n",
      "batch: 930 loss: 0.022149812430143356\n",
      "batch: 940 loss: 0.017727138474583626\n",
      "batch: 950 loss: 0.014964357949793339\n",
      "batch: 960 loss: 0.019546326249837875\n",
      "batch: 970 loss: 0.018311219289898872\n",
      "batch: 980 loss: 0.016064785420894623\n",
      "batch: 990 loss: 0.0187078807502985\n",
      "batch: 1000 loss: 0.015407216735184193\n",
      "batch: 1010 loss: 0.013938977383077145\n",
      "batch: 1020 loss: 0.019015684723854065\n",
      "batch: 1030 loss: 0.022622037678956985\n",
      "batch: 1040 loss: 0.014262978918850422\n",
      "batch: 1050 loss: 0.016351956874132156\n",
      "batch: 1060 loss: 0.0184610765427351\n",
      "batch: 1070 loss: 0.0169195719063282\n",
      "batch: 1080 loss: 0.019085748121142387\n",
      "batch: 1090 loss: 0.021202992647886276\n",
      "batch: 1100 loss: 0.020933564752340317\n",
      "batch: 1110 loss: 0.018524132668972015\n",
      "batch: 1120 loss: 0.02499079331755638\n",
      "batch: 1130 loss: 0.019695941358804703\n",
      "batch: 1140 loss: 0.017307989299297333\n",
      "batch: 1150 loss: 0.01713491976261139\n",
      "batch: 1160 loss: 0.013514749705791473\n",
      "batch: 1170 loss: 0.01393416803330183\n",
      "batch: 1180 loss: 0.01661456748843193\n",
      "batch: 1190 loss: 0.020836390554904938\n",
      "batch: 1200 loss: 0.01578485034406185\n",
      "batch: 1210 loss: 0.017437340691685677\n",
      "batch: 1220 loss: 0.022962525486946106\n",
      "batch: 1230 loss: 0.02400650642812252\n",
      "batch: 1240 loss: 0.018145494163036346\n",
      "batch: 1250 loss: 0.018668819218873978\n",
      "batch: 1260 loss: 0.016573602333664894\n",
      "batch: 1270 loss: 0.01651877351105213\n",
      "batch: 1280 loss: 0.0186481773853302\n",
      "batch: 1290 loss: 0.016105979681015015\n",
      "batch: 1300 loss: 0.018736297264695168\n",
      "batch: 1310 loss: 0.020459933206439018\n",
      "batch: 1320 loss: 0.018625399097800255\n",
      "batch: 1330 loss: 0.017100825905799866\n",
      "batch: 1340 loss: 0.017225923016667366\n",
      "batch: 1350 loss: 0.021206283941864967\n",
      "batch: 1360 loss: 0.015354941599071026\n",
      "batch: 1370 loss: 0.016690701246261597\n",
      "batch: 1380 loss: 0.016287337988615036\n",
      "batch: 1390 loss: 0.017851855605840683\n",
      "batch: 1400 loss: 0.019362516701221466\n",
      "batch: 1410 loss: 0.01652994379401207\n",
      "batch: 1420 loss: 0.016398152336478233\n",
      "batch: 1430 loss: 0.013606652617454529\n",
      "batch: 1440 loss: 0.01900355890393257\n",
      "batch: 1450 loss: 0.0193499606102705\n",
      "batch: 1460 loss: 0.01596353016793728\n",
      "batch: 1470 loss: 0.01812167651951313\n",
      "batch: 1480 loss: 0.017163679003715515\n",
      "batch: 1490 loss: 0.017102917656302452\n",
      "batch: 1500 loss: 0.014628647826611996\n",
      "batch: 1510 loss: 0.020749034360051155\n",
      "batch: 1520 loss: 0.018952416256070137\n",
      "batch: 1530 loss: 0.022715959697961807\n",
      "batch: 1540 loss: 0.01864985190331936\n",
      "batch: 1550 loss: 0.018383629620075226\n",
      "batch: 1560 loss: 0.016213783994317055\n",
      "batch: 1570 loss: 0.01825663261115551\n",
      "batch: 1580 loss: 0.01986130326986313\n",
      "batch: 1590 loss: 0.01958988606929779\n",
      "batch: 1600 loss: 0.016640162095427513\n",
      "batch: 1610 loss: 0.01856457255780697\n",
      "batch: 1620 loss: 0.017412880435585976\n",
      "batch: 1630 loss: 0.02124044857919216\n",
      "batch: 1640 loss: 0.014747649431228638\n",
      "batch: 1650 loss: 0.013685383833944798\n",
      "batch: 1660 loss: 0.01693568006157875\n",
      "batch: 1670 loss: 0.014991765841841698\n",
      "batch: 1680 loss: 0.01888955570757389\n",
      "batch: 1690 loss: 0.0166259054094553\n",
      "batch: 1700 loss: 0.014173622243106365\n",
      "batch: 1710 loss: 0.015962209552526474\n",
      "batch: 1720 loss: 0.02048519067466259\n",
      "batch: 1730 loss: 0.01473954040557146\n",
      "batch: 1740 loss: 0.017011234536767006\n",
      "batch: 1750 loss: 0.016697799786925316\n",
      "batch: 1760 loss: 0.020550774410367012\n",
      "batch: 1770 loss: 0.020780686289072037\n",
      "batch: 1780 loss: 0.01641206443309784\n",
      "batch: 1790 loss: 0.011530369520187378\n",
      "batch: 1800 loss: 0.014792104251682758\n",
      "batch: 1810 loss: 0.016134915873408318\n",
      "batch: 1820 loss: 0.016903821378946304\n",
      "batch: 1830 loss: 0.01838042214512825\n",
      "batch: 1840 loss: 0.016537129878997803\n",
      "batch: 1850 loss: 0.01816643215715885\n",
      "batch: 1860 loss: 0.01903880201280117\n",
      "batch: 1870 loss: 0.021106518805027008\n",
      "batch: 1880 loss: 0.02197997085750103\n",
      "batch: 1890 loss: 0.01683536358177662\n",
      "batch: 1900 loss: 0.018935633823275566\n",
      "batch: 1910 loss: 0.01795165427029133\n",
      "batch: 1920 loss: 0.016597120091319084\n",
      "batch: 1930 loss: 0.024362515658140182\n",
      "batch: 1940 loss: 0.018323101103305817\n",
      "batch: 1950 loss: 0.015923533588647842\n",
      "batch: 1960 loss: 0.019052617251873016\n",
      "batch: 1970 loss: 0.017628001049160957\n",
      "batch: 1980 loss: 0.016950752586126328\n",
      "batch: 1990 loss: 0.0170433409512043\n",
      "batch: 2000 loss: 0.016366031020879745\n",
      "batch: 2010 loss: 0.019854655489325523\n",
      "batch: 2020 loss: 0.020411545410752296\n",
      "batch: 2030 loss: 0.016005944460630417\n",
      "batch: 2040 loss: 0.018131276592612267\n",
      "batch: 2050 loss: 0.016255948692560196\n",
      "batch: 2060 loss: 0.01626540720462799\n",
      "batch: 2070 loss: 0.016792278736829758\n",
      "batch: 2080 loss: 0.015031062997877598\n",
      "batch: 2090 loss: 0.018391180783510208\n",
      "batch: 2100 loss: 0.020103730261325836\n",
      "batch: 2110 loss: 0.015853924676775932\n",
      "batch: 2120 loss: 0.012324863113462925\n",
      "batch: 2130 loss: 0.014821905642747879\n",
      "batch: 2140 loss: 0.015820268541574478\n",
      "batch: 2150 loss: 0.023434432223439217\n",
      "batch: 2160 loss: 0.01895945891737938\n",
      "batch: 2170 loss: 0.014425262808799744\n",
      "batch: 2180 loss: 0.017904018983244896\n",
      "batch: 2190 loss: 0.020095374435186386\n",
      "batch: 2200 loss: 0.01943560130894184\n",
      "batch: 2210 loss: 0.021927420049905777\n",
      "batch: 2220 loss: 0.01554287038743496\n",
      "batch: 2230 loss: 0.018921786919236183\n",
      "batch: 2240 loss: 0.020279860123991966\n",
      "batch: 2250 loss: 0.018290720880031586\n",
      "batch: 2260 loss: 0.014142568223178387\n",
      "batch: 2270 loss: 0.01705187000334263\n",
      "batch: 2280 loss: 0.019233249127864838\n",
      "batch: 2290 loss: 0.014984969981014729\n",
      "batch: 2300 loss: 0.020125923678278923\n",
      "batch: 2310 loss: 0.016218673437833786\n",
      "batch: 2320 loss: 0.017703000456094742\n",
      "batch: 2330 loss: 0.013416746631264687\n",
      "batch: 2340 loss: 0.02222353033721447\n",
      "batch: 2350 loss: 0.018149269744753838\n",
      "batch: 2360 loss: 0.014644265174865723\n",
      "batch: 2370 loss: 0.019355420023202896\n",
      "batch: 2380 loss: 0.014510242268443108\n",
      "batch: 2390 loss: 0.015914205461740494\n",
      "batch: 2400 loss: 0.02065199986100197\n",
      "batch: 2410 loss: 0.014686099253594875\n",
      "batch: 2420 loss: 0.016425149515271187\n",
      "batch: 2430 loss: 0.019018154591321945\n",
      "batch: 2440 loss: 0.01760588400065899\n",
      "batch: 2450 loss: 0.0180303156375885\n",
      "batch: 2460 loss: 0.01528037991374731\n",
      "batch: 2470 loss: 0.01987389475107193\n",
      "batch: 2480 loss: 0.019327040761709213\n",
      "batch: 2490 loss: 0.01900736801326275\n",
      "batch: 2500 loss: 0.017950264737010002\n",
      "batch: 2510 loss: 0.015207850374281406\n",
      "batch: 2520 loss: 0.013596978038549423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2530 loss: 0.018516741693019867\n",
      "batch: 2540 loss: 0.018826410174369812\n",
      "batch: 2550 loss: 0.017639292404055595\n",
      "batch: 2560 loss: 0.014550922438502312\n",
      "batch: 2570 loss: 0.017739634960889816\n",
      "batch: 2580 loss: 0.015144401229918003\n",
      "batch: 2590 loss: 0.01685287617146969\n",
      "batch: 2600 loss: 0.01795647293329239\n",
      "batch: 2610 loss: 0.020104987546801567\n",
      "batch: 2620 loss: 0.017901772633194923\n",
      "batch: 2630 loss: 0.01642182469367981\n",
      "batch: 2640 loss: 0.019375639036297798\n",
      "batch: 2650 loss: 0.02046200819313526\n",
      "batch: 2660 loss: 0.02274981699883938\n",
      "batch: 2670 loss: 0.016050055623054504\n",
      "batch: 2680 loss: 0.01525417435914278\n",
      "batch: 2690 loss: 0.01652488484978676\n",
      "batch: 2700 loss: 0.016445716843008995\n",
      "batch: 2710 loss: 0.018095919862389565\n",
      "batch: 2720 loss: 0.01725047640502453\n",
      "batch: 2730 loss: 0.018567688763141632\n",
      "batch: 2740 loss: 0.02204064279794693\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.016130104660987854\n",
      "batch: 10 loss: 0.01778442971408367\n",
      "batch: 20 loss: 0.020265992730855942\n",
      "batch: 30 loss: 0.014229756779968739\n",
      "batch: 40 loss: 0.016885023564100266\n",
      "batch: 50 loss: 0.013380377553403378\n",
      "batch: 60 loss: 0.018063437193632126\n",
      "batch: 70 loss: 0.013774978928267956\n",
      "batch: 80 loss: 0.01585494540631771\n",
      "batch: 90 loss: 0.01394778024405241\n",
      "batch: 100 loss: 0.014818091876804829\n",
      "batch: 110 loss: 0.018401313573122025\n",
      "batch: 120 loss: 0.01931452378630638\n",
      "batch: 130 loss: 0.01710130274295807\n",
      "batch: 140 loss: 0.014865360222756863\n",
      "batch: 150 loss: 0.011084879748523235\n",
      "batch: 160 loss: 0.02126898430287838\n",
      "batch: 170 loss: 0.017599426209926605\n",
      "batch: 180 loss: 0.018922531977295876\n",
      "batch: 190 loss: 0.01962331123650074\n",
      "batch: 200 loss: 0.014786262065172195\n",
      "batch: 210 loss: 0.012206907384097576\n",
      "batch: 220 loss: 0.015201259404420853\n",
      "batch: 230 loss: 0.02282974123954773\n",
      "batch: 240 loss: 0.018983040004968643\n",
      "batch: 250 loss: 0.015625014901161194\n",
      "batch: 260 loss: 0.01738820970058441\n",
      "batch: 270 loss: 0.014684646390378475\n",
      "batch: 280 loss: 0.02146758697926998\n",
      "batch: 290 loss: 0.016657358035445213\n",
      "batch: 300 loss: 0.018371058627963066\n",
      "batch: 310 loss: 0.021913772448897362\n",
      "batch: 320 loss: 0.014890444464981556\n",
      "batch: 330 loss: 0.016923299059271812\n",
      "batch: 340 loss: 0.015287145972251892\n",
      "batch: 350 loss: 0.016793595626950264\n",
      "batch: 360 loss: 0.014950943179428577\n",
      "batch: 370 loss: 0.016118893399834633\n",
      "batch: 380 loss: 0.014529853127896786\n",
      "batch: 390 loss: 0.020112017169594765\n",
      "batch: 400 loss: 0.02039075829088688\n",
      "batch: 410 loss: 0.014677129685878754\n",
      "batch: 420 loss: 0.019709846004843712\n",
      "batch: 430 loss: 0.01806217059493065\n",
      "batch: 440 loss: 0.02236809954047203\n",
      "batch: 450 loss: 0.014618676155805588\n",
      "batch: 460 loss: 0.016986418515443802\n",
      "batch: 470 loss: 0.013402237556874752\n",
      "batch: 480 loss: 0.014540825970470905\n",
      "batch: 490 loss: 0.018369318917393684\n",
      "batch: 500 loss: 0.01682596653699875\n",
      "batch: 510 loss: 0.01807568036019802\n",
      "batch: 520 loss: 0.01775025576353073\n",
      "batch: 530 loss: 0.018351726233959198\n",
      "batch: 540 loss: 0.01589861698448658\n",
      "batch: 550 loss: 0.01744193211197853\n",
      "batch: 560 loss: 0.013018309138715267\n",
      "batch: 570 loss: 0.02250547893345356\n",
      "batch: 580 loss: 0.01849241368472576\n",
      "batch: 590 loss: 0.019608644768595695\n",
      "batch: 600 loss: 0.016805171966552734\n",
      "batch: 610 loss: 0.018321143463253975\n",
      "batch: 620 loss: 0.018438925966620445\n",
      "batch: 630 loss: 0.016109775751829147\n",
      "batch: 640 loss: 0.018697084859013557\n",
      "batch: 650 loss: 0.018762607127428055\n",
      "batch: 660 loss: 0.017760634422302246\n",
      "batch: 670 loss: 0.016128528863191605\n",
      "batch: 680 loss: 0.018397504463791847\n",
      "batch: 690 loss: 0.015408709645271301\n",
      "batch: 700 loss: 0.02070631831884384\n",
      "batch: 710 loss: 0.02062295936048031\n",
      "batch: 720 loss: 0.014995500445365906\n",
      "batch: 730 loss: 0.01677781529724598\n",
      "batch: 740 loss: 0.015231216326355934\n",
      "batch: 750 loss: 0.013566720299422741\n",
      "batch: 760 loss: 0.016698269173502922\n",
      "batch: 770 loss: 0.015601410530507565\n",
      "batch: 780 loss: 0.01663392409682274\n",
      "batch: 790 loss: 0.017345236614346504\n",
      "batch: 800 loss: 0.014158559031784534\n",
      "batch: 810 loss: 0.017631761729717255\n",
      "batch: 820 loss: 0.021007539704442024\n",
      "batch: 830 loss: 0.017149798572063446\n",
      "batch: 840 loss: 0.019827287644147873\n",
      "batch: 850 loss: 0.01923435367643833\n",
      "batch: 860 loss: 0.021894337609410286\n",
      "batch: 870 loss: 0.020827515050768852\n",
      "batch: 880 loss: 0.01629367657005787\n",
      "batch: 890 loss: 0.01880512945353985\n",
      "batch: 900 loss: 0.020762739703059196\n",
      "batch: 910 loss: 0.01643715798854828\n",
      "batch: 920 loss: 0.019207749515771866\n",
      "batch: 930 loss: 0.020861055701971054\n",
      "batch: 940 loss: 0.014583371579647064\n",
      "batch: 950 loss: 0.01742463745176792\n",
      "batch: 960 loss: 0.018424738198518753\n",
      "batch: 970 loss: 0.01663568615913391\n",
      "batch: 980 loss: 0.017699941992759705\n",
      "batch: 990 loss: 0.01981993578374386\n",
      "batch: 1000 loss: 0.01568002998828888\n",
      "batch: 1010 loss: 0.021211180835962296\n",
      "batch: 1020 loss: 0.017195558175444603\n",
      "batch: 1030 loss: 0.021403392776846886\n",
      "batch: 1040 loss: 0.014157596975564957\n",
      "batch: 1050 loss: 0.01140072662383318\n",
      "batch: 1060 loss: 0.013055236078798771\n",
      "batch: 1070 loss: 0.02372618205845356\n",
      "batch: 1080 loss: 0.018550759181380272\n",
      "batch: 1090 loss: 0.017287489026784897\n",
      "batch: 1100 loss: 0.020910078659653664\n",
      "batch: 1110 loss: 0.018798168748617172\n",
      "batch: 1120 loss: 0.01589140295982361\n",
      "batch: 1130 loss: 0.0197116881608963\n",
      "batch: 1140 loss: 0.012413465417921543\n",
      "batch: 1150 loss: 0.014751066453754902\n",
      "batch: 1160 loss: 0.021296104416251183\n",
      "batch: 1170 loss: 0.01670028641819954\n",
      "batch: 1180 loss: 0.016354776918888092\n",
      "batch: 1190 loss: 0.016207555308938026\n",
      "batch: 1200 loss: 0.01529408898204565\n",
      "batch: 1210 loss: 0.017661524936556816\n",
      "batch: 1220 loss: 0.018718522042036057\n",
      "batch: 1230 loss: 0.020626211538910866\n",
      "batch: 1240 loss: 0.015573359094560146\n",
      "batch: 1250 loss: 0.015372445806860924\n",
      "batch: 1260 loss: 0.015418020077049732\n",
      "batch: 1270 loss: 0.012570357881486416\n",
      "batch: 1280 loss: 0.017138615250587463\n",
      "batch: 1290 loss: 0.014409695751965046\n",
      "batch: 1300 loss: 0.019512610509991646\n",
      "batch: 1310 loss: 0.017227692529559135\n",
      "batch: 1320 loss: 0.018153579905629158\n",
      "batch: 1330 loss: 0.017376568168401718\n",
      "batch: 1340 loss: 0.016061387956142426\n",
      "batch: 1350 loss: 0.01712142489850521\n",
      "batch: 1360 loss: 0.020883657038211823\n",
      "batch: 1370 loss: 0.01944185234606266\n",
      "batch: 1380 loss: 0.017110997810959816\n",
      "batch: 1390 loss: 0.01365271769464016\n",
      "batch: 1400 loss: 0.020759055390954018\n",
      "batch: 1410 loss: 0.015315121039748192\n",
      "batch: 1420 loss: 0.015990156680345535\n",
      "batch: 1430 loss: 0.017719456925988197\n",
      "batch: 1440 loss: 0.01275322213768959\n",
      "batch: 1450 loss: 0.012133586220443249\n",
      "batch: 1460 loss: 0.01663246937096119\n",
      "batch: 1470 loss: 0.015564065426588058\n",
      "batch: 1480 loss: 0.0207180418074131\n",
      "batch: 1490 loss: 0.017465747892856598\n",
      "batch: 1500 loss: 0.01638791710138321\n",
      "batch: 1510 loss: 0.018703479319810867\n",
      "batch: 1520 loss: 0.01827402226626873\n",
      "batch: 1530 loss: 0.015186257660388947\n",
      "batch: 1540 loss: 0.016150610521435738\n",
      "batch: 1550 loss: 0.01764373667538166\n",
      "batch: 1560 loss: 0.01478532887995243\n",
      "batch: 1570 loss: 0.01611345075070858\n",
      "batch: 1580 loss: 0.018601136282086372\n",
      "batch: 1590 loss: 0.01642991043627262\n",
      "batch: 1600 loss: 0.010835280641913414\n",
      "batch: 1610 loss: 0.017679976299405098\n",
      "batch: 1620 loss: 0.011581799015402794\n",
      "batch: 1630 loss: 0.01461679395288229\n",
      "batch: 1640 loss: 0.01578652486205101\n",
      "batch: 1650 loss: 0.015175133012235165\n",
      "batch: 1660 loss: 0.016564901918172836\n",
      "batch: 1670 loss: 0.014581238850951195\n",
      "batch: 1680 loss: 0.022191988304257393\n",
      "batch: 1690 loss: 0.015950646251440048\n",
      "batch: 1700 loss: 0.014716769568622112\n",
      "batch: 1710 loss: 0.021832363680005074\n",
      "batch: 1720 loss: 0.021969066932797432\n",
      "batch: 1730 loss: 0.01576492376625538\n",
      "batch: 1740 loss: 0.02056111954152584\n",
      "batch: 1750 loss: 0.018966468051075935\n",
      "batch: 1760 loss: 0.01951250620186329\n",
      "batch: 1770 loss: 0.01736973598599434\n",
      "batch: 1780 loss: 0.018053792417049408\n",
      "batch: 1790 loss: 0.017783792689442635\n",
      "batch: 1800 loss: 0.018861638382077217\n",
      "batch: 1810 loss: 0.012055750004947186\n",
      "batch: 1820 loss: 0.02062775380909443\n",
      "batch: 1830 loss: 0.01834297552704811\n",
      "batch: 1840 loss: 0.016229795292019844\n",
      "batch: 1850 loss: 0.01682920753955841\n",
      "batch: 1860 loss: 0.014890525490045547\n",
      "batch: 1870 loss: 0.016575384885072708\n",
      "batch: 1880 loss: 0.020828207954764366\n",
      "batch: 1890 loss: 0.01939326710999012\n",
      "batch: 1900 loss: 0.016364822164177895\n",
      "batch: 1910 loss: 0.02167680114507675\n",
      "batch: 1920 loss: 0.01591932587325573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1930 loss: 0.019183192402124405\n",
      "batch: 1940 loss: 0.015377332456409931\n",
      "batch: 1950 loss: 0.016952479258179665\n",
      "batch: 1960 loss: 0.015494447201490402\n",
      "batch: 1970 loss: 0.018508661538362503\n",
      "batch: 1980 loss: 0.01207511406391859\n",
      "batch: 1990 loss: 0.015534973703324795\n",
      "batch: 2000 loss: 0.01543459389358759\n",
      "batch: 2010 loss: 0.011683701537549496\n",
      "batch: 2020 loss: 0.01496739313006401\n",
      "batch: 2030 loss: 0.014410317875444889\n",
      "batch: 2040 loss: 0.02184094302356243\n",
      "batch: 2050 loss: 0.012810503132641315\n",
      "batch: 2060 loss: 0.017000513151288033\n",
      "batch: 2070 loss: 0.017054244875907898\n",
      "batch: 2080 loss: 0.015177401714026928\n",
      "batch: 2090 loss: 0.016250643879175186\n",
      "batch: 2100 loss: 0.013873495161533356\n",
      "batch: 2110 loss: 0.01746547222137451\n",
      "batch: 2120 loss: 0.01718352921307087\n",
      "batch: 2130 loss: 0.018470974639058113\n",
      "batch: 2140 loss: 0.0230945385992527\n",
      "batch: 2150 loss: 0.01810435764491558\n",
      "batch: 2160 loss: 0.01583355851471424\n",
      "batch: 2170 loss: 0.01801961660385132\n",
      "batch: 2180 loss: 0.018034890294075012\n",
      "batch: 2190 loss: 0.017528779804706573\n",
      "batch: 2200 loss: 0.015145193785429\n",
      "batch: 2210 loss: 0.01634553074836731\n",
      "batch: 2220 loss: 0.015180694870650768\n",
      "batch: 2230 loss: 0.014407217502593994\n",
      "batch: 2240 loss: 0.014259974472224712\n",
      "batch: 2250 loss: 0.014945142902433872\n",
      "batch: 2260 loss: 0.014120626263320446\n",
      "batch: 2270 loss: 0.016758393496274948\n",
      "batch: 2280 loss: 0.01503949798643589\n",
      "batch: 2290 loss: 0.0144301513209939\n",
      "batch: 2300 loss: 0.016912011429667473\n",
      "batch: 2310 loss: 0.01605255901813507\n",
      "batch: 2320 loss: 0.01902644895017147\n",
      "batch: 2330 loss: 0.017102491110563278\n",
      "batch: 2340 loss: 0.014608016237616539\n",
      "batch: 2350 loss: 0.020051851868629456\n",
      "batch: 2360 loss: 0.018715331330895424\n",
      "batch: 2370 loss: 0.020213458687067032\n",
      "batch: 2380 loss: 0.01409328356385231\n",
      "batch: 2390 loss: 0.019063957035541534\n",
      "batch: 2400 loss: 0.015192857012152672\n",
      "batch: 2410 loss: 0.013942931778728962\n",
      "batch: 2420 loss: 0.016029734164476395\n",
      "batch: 2430 loss: 0.01825069449841976\n",
      "batch: 2440 loss: 0.018169697374105453\n",
      "batch: 2450 loss: 0.01439304556697607\n",
      "batch: 2460 loss: 0.018836302682757378\n",
      "batch: 2470 loss: 0.015780873596668243\n",
      "batch: 2480 loss: 0.013984848745167255\n",
      "batch: 2490 loss: 0.018334006890654564\n",
      "batch: 2500 loss: 0.018250299617648125\n",
      "batch: 2510 loss: 0.022080503404140472\n",
      "batch: 2520 loss: 0.019204257056117058\n",
      "batch: 2530 loss: 0.018709972500801086\n",
      "batch: 2540 loss: 0.018997512757778168\n",
      "batch: 2550 loss: 0.016314398497343063\n",
      "batch: 2560 loss: 0.01706749014556408\n",
      "batch: 2570 loss: 0.016464926302433014\n",
      "batch: 2580 loss: 0.015033728443086147\n",
      "batch: 2590 loss: 0.013359454460442066\n",
      "batch: 2600 loss: 0.02108670026063919\n",
      "batch: 2610 loss: 0.022803081199526787\n",
      "batch: 2620 loss: 0.023195000365376472\n",
      "batch: 2630 loss: 0.015260330401360989\n",
      "batch: 2640 loss: 0.01789906620979309\n",
      "batch: 2650 loss: 0.017864778637886047\n",
      "batch: 2660 loss: 0.017368417233228683\n",
      "batch: 2670 loss: 0.014311227016150951\n",
      "batch: 2680 loss: 0.020754901692271233\n",
      "batch: 2690 loss: 0.01661381684243679\n",
      "batch: 2700 loss: 0.018448319286108017\n",
      "batch: 2710 loss: 0.014004068449139595\n",
      "batch: 2720 loss: 0.019732965156435966\n",
      "batch: 2730 loss: 0.02086765319108963\n",
      "batch: 2740 loss: 0.023802518844604492\n",
      "round  9 done\n",
      "------- Epoch: 1 -------\n",
      "batch: 0 loss: 0.015216201543807983\n",
      "batch: 10 loss: 0.01434074342250824\n",
      "batch: 20 loss: 0.018882161006331444\n",
      "batch: 30 loss: 0.01567125879228115\n",
      "batch: 40 loss: 0.015814661979675293\n",
      "batch: 50 loss: 0.01631164364516735\n",
      "batch: 60 loss: 0.01409839279949665\n",
      "batch: 70 loss: 0.011716853827238083\n",
      "batch: 80 loss: 0.012014582753181458\n",
      "batch: 90 loss: 0.016248906031250954\n",
      "batch: 100 loss: 0.016548912972211838\n",
      "batch: 110 loss: 0.014571464620530605\n",
      "batch: 120 loss: 0.015483162365853786\n",
      "batch: 130 loss: 0.015212922357022762\n",
      "batch: 140 loss: 0.015087676234543324\n",
      "batch: 150 loss: 0.0175212100148201\n",
      "batch: 160 loss: 0.015983212739229202\n",
      "batch: 170 loss: 0.012651476077735424\n",
      "batch: 180 loss: 0.01077878288924694\n",
      "batch: 190 loss: 0.014903208240866661\n",
      "batch: 200 loss: 0.014262164011597633\n",
      "batch: 210 loss: 0.014034037478268147\n",
      "batch: 220 loss: 0.017450852319598198\n",
      "batch: 230 loss: 0.015979522839188576\n",
      "batch: 240 loss: 0.01632586121559143\n",
      "batch: 250 loss: 0.015879478305578232\n",
      "batch: 260 loss: 0.015245423652231693\n",
      "batch: 270 loss: 0.016214895993471146\n",
      "batch: 280 loss: 0.015252326615154743\n",
      "batch: 290 loss: 0.015994343906641006\n",
      "batch: 300 loss: 0.01408576499670744\n",
      "batch: 310 loss: 0.014319357462227345\n",
      "batch: 320 loss: 0.012679612264037132\n",
      "batch: 330 loss: 0.016308104619383812\n",
      "batch: 340 loss: 0.013094432651996613\n",
      "batch: 350 loss: 0.01602090708911419\n",
      "batch: 360 loss: 0.014149787835776806\n",
      "batch: 370 loss: 0.015042846091091633\n",
      "batch: 380 loss: 0.01711815781891346\n",
      "batch: 390 loss: 0.011959188617765903\n",
      "batch: 400 loss: 0.014816118404269218\n",
      "batch: 410 loss: 0.01227976381778717\n",
      "batch: 420 loss: 0.013648669235408306\n",
      "batch: 430 loss: 0.013771220110356808\n",
      "batch: 440 loss: 0.012266102246940136\n",
      "batch: 450 loss: 0.014205394312739372\n",
      "batch: 460 loss: 0.013110954314470291\n",
      "batch: 470 loss: 0.018565775826573372\n",
      "batch: 480 loss: 0.014360131695866585\n",
      "batch: 490 loss: 0.012690147385001183\n",
      "batch: 500 loss: 0.01726054958999157\n",
      "batch: 510 loss: 0.014777153730392456\n",
      "batch: 520 loss: 0.014176943339407444\n",
      "batch: 530 loss: 0.012951504439115524\n",
      "batch: 540 loss: 0.01603912189602852\n",
      "batch: 550 loss: 0.013107586652040482\n",
      "batch: 560 loss: 0.016488688066601753\n",
      "batch: 570 loss: 0.01497669704258442\n",
      "batch: 580 loss: 0.017605997622013092\n",
      "batch: 590 loss: 0.01650565303862095\n",
      "batch: 600 loss: 0.01638646423816681\n",
      "batch: 610 loss: 0.01813534088432789\n",
      "batch: 620 loss: 0.012991942465305328\n",
      "batch: 630 loss: 0.015909655019640923\n",
      "batch: 640 loss: 0.014415804296731949\n",
      "batch: 650 loss: 0.01635148376226425\n",
      "batch: 660 loss: 0.01195122953504324\n",
      "batch: 670 loss: 0.012875332497060299\n",
      "batch: 680 loss: 0.014949031174182892\n",
      "batch: 690 loss: 0.01763569563627243\n",
      "batch: 700 loss: 0.015721257776021957\n",
      "batch: 710 loss: 0.013367149978876114\n",
      "batch: 720 loss: 0.017295019701123238\n",
      "batch: 730 loss: 0.015796391293406487\n",
      "batch: 740 loss: 0.015981525182724\n",
      "batch: 750 loss: 0.013035283423960209\n",
      "batch: 760 loss: 0.016088265925645828\n",
      "batch: 770 loss: 0.017505140975117683\n",
      "batch: 780 loss: 0.012773317284882069\n",
      "batch: 790 loss: 0.012780016288161278\n",
      "batch: 800 loss: 0.01472098845988512\n",
      "batch: 810 loss: 0.01013560127466917\n",
      "batch: 820 loss: 0.009374888613820076\n",
      "batch: 830 loss: 0.018256042152643204\n",
      "batch: 840 loss: 0.013837375678122044\n",
      "batch: 850 loss: 0.013807018287479877\n",
      "batch: 860 loss: 0.012519747018814087\n",
      "batch: 870 loss: 0.014785705134272575\n",
      "batch: 880 loss: 0.01262711826711893\n",
      "batch: 890 loss: 0.015877189114689827\n",
      "batch: 900 loss: 0.015406846068799496\n",
      "batch: 910 loss: 0.014532470144331455\n",
      "batch: 920 loss: 0.01210315153002739\n",
      "batch: 930 loss: 0.012014249339699745\n",
      "batch: 940 loss: 0.012905617244541645\n",
      "batch: 950 loss: 0.013195568695664406\n",
      "batch: 960 loss: 0.014106779359281063\n",
      "batch: 970 loss: 0.015826251357793808\n",
      "batch: 980 loss: 0.014604422263801098\n",
      "batch: 990 loss: 0.015983618795871735\n",
      "batch: 1000 loss: 0.013878659345209599\n",
      "batch: 1010 loss: 0.015752987936139107\n",
      "batch: 1020 loss: 0.016782354563474655\n",
      "batch: 1030 loss: 0.011120553128421307\n",
      "batch: 1040 loss: 0.015288903377950191\n",
      "batch: 1050 loss: 0.00920709315687418\n",
      "batch: 1060 loss: 0.012265924364328384\n",
      "batch: 1070 loss: 0.016848072409629822\n",
      "batch: 1080 loss: 0.01599949412047863\n",
      "batch: 1090 loss: 0.01697663404047489\n",
      "batch: 1100 loss: 0.01610707864165306\n",
      "batch: 1110 loss: 0.014142476953566074\n",
      "batch: 1120 loss: 0.016224782913923264\n",
      "batch: 1130 loss: 0.014402361586689949\n",
      "batch: 1140 loss: 0.016128679737448692\n",
      "batch: 1150 loss: 0.017940405756235123\n",
      "batch: 1160 loss: 0.013481530360877514\n",
      "batch: 1170 loss: 0.01359634380787611\n",
      "batch: 1180 loss: 0.014077133499085903\n",
      "batch: 1190 loss: 0.013732600025832653\n",
      "batch: 1200 loss: 0.016727520152926445\n",
      "batch: 1210 loss: 0.017066778615117073\n",
      "batch: 1220 loss: 0.012574078515172005\n",
      "batch: 1230 loss: 0.01236472837626934\n",
      "batch: 1240 loss: 0.014453868381679058\n",
      "batch: 1250 loss: 0.015188395045697689\n",
      "batch: 1260 loss: 0.016479672864079475\n",
      "batch: 1270 loss: 0.015140697360038757\n",
      "batch: 1280 loss: 0.011784005910158157\n",
      "batch: 1290 loss: 0.014992455951869488\n",
      "batch: 1300 loss: 0.01080252043902874\n",
      "batch: 1310 loss: 0.017175836488604546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1320 loss: 0.012822108343243599\n",
      "batch: 1330 loss: 0.014381872490048409\n",
      "batch: 1340 loss: 0.01571386121213436\n",
      "batch: 1350 loss: 0.016897300258278847\n",
      "batch: 1360 loss: 0.015123337507247925\n",
      "batch: 1370 loss: 0.01772736944258213\n",
      "batch: 1380 loss: 0.017719268798828125\n",
      "batch: 1390 loss: 0.011565767228603363\n",
      "batch: 1400 loss: 0.0179890263825655\n",
      "batch: 1410 loss: 0.01705707050859928\n",
      "batch: 1420 loss: 0.01489122025668621\n",
      "batch: 1430 loss: 0.012673119083046913\n",
      "batch: 1440 loss: 0.014872977510094643\n",
      "batch: 1450 loss: 0.013929014094173908\n",
      "batch: 1460 loss: 0.017323032021522522\n",
      "batch: 1470 loss: 0.014918838627636433\n",
      "batch: 1480 loss: 0.017826618626713753\n",
      "batch: 1490 loss: 0.019274713471531868\n",
      "batch: 1500 loss: 0.017951836809515953\n",
      "batch: 1510 loss: 0.015653587877750397\n",
      "batch: 1520 loss: 0.013614221476018429\n",
      "batch: 1530 loss: 0.01695735938847065\n",
      "batch: 1540 loss: 0.014780634082853794\n",
      "batch: 1550 loss: 0.012519611977040768\n",
      "batch: 1560 loss: 0.012116506695747375\n",
      "batch: 1570 loss: 0.010664247907698154\n",
      "batch: 1580 loss: 0.015666361898183823\n",
      "batch: 1590 loss: 0.01741201803088188\n",
      "batch: 1600 loss: 0.014126785099506378\n",
      "batch: 1610 loss: 0.013190105557441711\n",
      "batch: 1620 loss: 0.015444524586200714\n",
      "batch: 1630 loss: 0.011972011998295784\n",
      "batch: 1640 loss: 0.01481146365404129\n",
      "batch: 1650 loss: 0.017128778621554375\n",
      "batch: 1660 loss: 0.01596728339791298\n",
      "batch: 1670 loss: 0.013218902982771397\n",
      "batch: 1680 loss: 0.01075666956603527\n",
      "batch: 1690 loss: 0.012543966062366962\n",
      "batch: 1700 loss: 0.015177096240222454\n",
      "batch: 1710 loss: 0.014975413680076599\n",
      "batch: 1720 loss: 0.013633908703923225\n",
      "batch: 1730 loss: 0.012188831344246864\n",
      "batch: 1740 loss: 0.011439925990998745\n",
      "batch: 1750 loss: 0.013836415484547615\n",
      "batch: 1760 loss: 0.013220011256635189\n",
      "batch: 1770 loss: 0.014569646678864956\n",
      "batch: 1780 loss: 0.01409654226154089\n",
      "batch: 1790 loss: 0.013599014841020107\n",
      "batch: 1800 loss: 0.01752367615699768\n",
      "batch: 1810 loss: 0.013906802050769329\n",
      "batch: 1820 loss: 0.012324980460107327\n",
      "batch: 1830 loss: 0.012999388389289379\n",
      "batch: 1840 loss: 0.012646002694964409\n",
      "batch: 1850 loss: 0.011074320413172245\n",
      "batch: 1860 loss: 0.016009610146284103\n",
      "batch: 1870 loss: 0.01352605875581503\n",
      "batch: 1880 loss: 0.01689060591161251\n",
      "batch: 1890 loss: 0.011729543097317219\n",
      "batch: 1900 loss: 0.014992937445640564\n",
      "batch: 1910 loss: 0.014933116734027863\n",
      "batch: 1920 loss: 0.014407460577785969\n",
      "batch: 1930 loss: 0.015100326389074326\n",
      "batch: 1940 loss: 0.014776249416172504\n",
      "batch: 1950 loss: 0.012788736261427402\n",
      "batch: 1960 loss: 0.01180217880755663\n",
      "batch: 1970 loss: 0.018104637041687965\n",
      "batch: 1980 loss: 0.014157470315694809\n",
      "batch: 1990 loss: 0.016049273312091827\n",
      "batch: 2000 loss: 0.017108650878071785\n",
      "batch: 2010 loss: 0.012721508741378784\n",
      "batch: 2020 loss: 0.01408765371888876\n",
      "batch: 2030 loss: 0.016737621277570724\n",
      "batch: 2040 loss: 0.015546047128736973\n",
      "batch: 2050 loss: 0.015556439757347107\n",
      "batch: 2060 loss: 0.012739657424390316\n",
      "batch: 2070 loss: 0.015405806712806225\n",
      "batch: 2080 loss: 0.016711188480257988\n",
      "batch: 2090 loss: 0.014706237241625786\n",
      "batch: 2100 loss: 0.014606705866754055\n",
      "batch: 2110 loss: 0.015196964144706726\n",
      "batch: 2120 loss: 0.013246222399175167\n",
      "batch: 2130 loss: 0.018995165824890137\n",
      "batch: 2140 loss: 0.014122656546533108\n",
      "batch: 2150 loss: 0.013752005062997341\n",
      "batch: 2160 loss: 0.013911797665059566\n",
      "batch: 2170 loss: 0.016252124682068825\n",
      "batch: 2180 loss: 0.016474785283207893\n",
      "batch: 2190 loss: 0.010389341041445732\n",
      "batch: 2200 loss: 0.013823608867824078\n",
      "batch: 2210 loss: 0.013790538534522057\n",
      "batch: 2220 loss: 0.014445933513343334\n",
      "batch: 2230 loss: 0.016870558261871338\n",
      "batch: 2240 loss: 0.012448284775018692\n",
      "batch: 2250 loss: 0.01574615016579628\n",
      "batch: 2260 loss: 0.014821087010204792\n",
      "batch: 2270 loss: 0.012714753858745098\n",
      "batch: 2280 loss: 0.014638839289546013\n",
      "batch: 2290 loss: 0.013292394578456879\n",
      "batch: 2300 loss: 0.01345109473913908\n",
      "batch: 2310 loss: 0.018370086327195168\n",
      "batch: 2320 loss: 0.015315301716327667\n",
      "batch: 2330 loss: 0.01301764976233244\n",
      "batch: 2340 loss: 0.014743445441126823\n",
      "batch: 2350 loss: 0.014823504723608494\n",
      "batch: 2360 loss: 0.014506915584206581\n",
      "batch: 2370 loss: 0.01351108867675066\n",
      "batch: 2380 loss: 0.0145844342187047\n",
      "batch: 2390 loss: 0.014711159281432629\n",
      "batch: 2400 loss: 0.01594393700361252\n",
      "batch: 2410 loss: 0.014908095821738243\n",
      "batch: 2420 loss: 0.018441714346408844\n",
      "batch: 2430 loss: 0.013313638977706432\n",
      "batch: 2440 loss: 0.01671193726360798\n",
      "batch: 2450 loss: 0.020142994821071625\n",
      "batch: 2460 loss: 0.011255276389420033\n",
      "batch: 2470 loss: 0.01710033416748047\n",
      "batch: 2480 loss: 0.016986621543765068\n",
      "batch: 2490 loss: 0.015585415065288544\n",
      "batch: 2500 loss: 0.01487023662775755\n",
      "batch: 2510 loss: 0.013586738146841526\n",
      "batch: 2520 loss: 0.017567815259099007\n",
      "batch: 2530 loss: 0.014790510758757591\n",
      "batch: 2540 loss: 0.016802847385406494\n",
      "batch: 2550 loss: 0.013909287750720978\n",
      "batch: 2560 loss: 0.015246400609612465\n",
      "batch: 2570 loss: 0.013889452442526817\n",
      "batch: 2580 loss: 0.019012466073036194\n",
      "batch: 2590 loss: 0.012587698176503181\n",
      "batch: 2600 loss: 0.01515169721096754\n",
      "batch: 2610 loss: 0.015661954879760742\n",
      "batch: 2620 loss: 0.01503660622984171\n",
      "batch: 2630 loss: 0.013863830827176571\n",
      "batch: 2640 loss: 0.01704242080450058\n",
      "batch: 2650 loss: 0.012056202627718449\n",
      "batch: 2660 loss: 0.015827536582946777\n",
      "batch: 2670 loss: 0.01567986235022545\n",
      "batch: 2680 loss: 0.013498378917574883\n",
      "batch: 2690 loss: 0.01516471616923809\n",
      "batch: 2700 loss: 0.015105084516108036\n",
      "batch: 2710 loss: 0.013318481855094433\n",
      "batch: 2720 loss: 0.014176148921251297\n",
      "batch: 2730 loss: 0.014140558429062366\n",
      "------- Epoch: 2 -------\n",
      "batch: 0 loss: 0.013268519192934036\n",
      "batch: 10 loss: 0.01634271815419197\n",
      "batch: 20 loss: 0.01607457362115383\n",
      "batch: 30 loss: 0.015386269427835941\n",
      "batch: 40 loss: 0.013228112831711769\n",
      "batch: 50 loss: 0.014019081369042397\n",
      "batch: 60 loss: 0.012707088142633438\n",
      "batch: 70 loss: 0.012348990887403488\n",
      "batch: 80 loss: 0.016100753098726273\n",
      "batch: 90 loss: 0.015168176032602787\n",
      "batch: 100 loss: 0.012863743118941784\n",
      "batch: 110 loss: 0.013341482728719711\n",
      "batch: 120 loss: 0.011736963875591755\n",
      "batch: 130 loss: 0.01684769056737423\n",
      "batch: 140 loss: 0.01388351246714592\n",
      "batch: 150 loss: 0.011645520105957985\n",
      "batch: 160 loss: 0.014549481682479382\n",
      "batch: 170 loss: 0.014371158555150032\n",
      "batch: 180 loss: 0.016239412128925323\n",
      "batch: 190 loss: 0.014486963860690594\n",
      "batch: 200 loss: 0.012627623975276947\n",
      "batch: 210 loss: 0.01624576933681965\n",
      "batch: 220 loss: 0.011212858371436596\n",
      "batch: 230 loss: 0.014724456705152988\n",
      "batch: 240 loss: 0.01608230359852314\n",
      "batch: 250 loss: 0.013199936598539352\n",
      "batch: 260 loss: 0.014345433562994003\n",
      "batch: 270 loss: 0.014171341434121132\n",
      "batch: 280 loss: 0.015429115854203701\n",
      "batch: 290 loss: 0.014770302921533585\n",
      "batch: 300 loss: 0.01363830454647541\n",
      "batch: 310 loss: 0.014505201950669289\n",
      "batch: 320 loss: 0.012842255644500256\n",
      "batch: 330 loss: 0.014219406060874462\n",
      "batch: 340 loss: 0.016697609797120094\n",
      "batch: 350 loss: 0.014170297421514988\n",
      "batch: 360 loss: 0.013828545808792114\n",
      "batch: 370 loss: 0.01173514686524868\n",
      "batch: 380 loss: 0.015741016715765\n",
      "batch: 390 loss: 0.011818140745162964\n",
      "batch: 400 loss: 0.01100787241011858\n",
      "batch: 410 loss: 0.01639571413397789\n",
      "batch: 420 loss: 0.013281059451401234\n",
      "batch: 430 loss: 0.016074582934379578\n",
      "batch: 440 loss: 0.01511165127158165\n",
      "batch: 450 loss: 0.010184884071350098\n",
      "batch: 460 loss: 0.01338467188179493\n",
      "batch: 470 loss: 0.013254249468445778\n",
      "batch: 480 loss: 0.016227765008807182\n",
      "batch: 490 loss: 0.01152544654905796\n",
      "batch: 500 loss: 0.011538458056747913\n",
      "batch: 510 loss: 0.01301006879657507\n",
      "batch: 520 loss: 0.013395116664469242\n",
      "batch: 530 loss: 0.014597509987652302\n",
      "batch: 540 loss: 0.012343954294919968\n",
      "batch: 550 loss: 0.017677508294582367\n",
      "batch: 560 loss: 0.016882946714758873\n",
      "batch: 570 loss: 0.017099302262067795\n",
      "batch: 580 loss: 0.013415480963885784\n",
      "batch: 590 loss: 0.013279573991894722\n",
      "batch: 600 loss: 0.016304094344377518\n",
      "batch: 610 loss: 0.013098434545099735\n",
      "batch: 620 loss: 0.013029216788709164\n",
      "batch: 630 loss: 0.014169786125421524\n",
      "batch: 640 loss: 0.01380937546491623\n",
      "batch: 650 loss: 0.02048889547586441\n",
      "batch: 660 loss: 0.01447316911071539\n",
      "batch: 670 loss: 0.01923121139407158\n",
      "batch: 680 loss: 0.012831716798245907\n",
      "batch: 690 loss: 0.01562783122062683\n",
      "batch: 700 loss: 0.019603976979851723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 710 loss: 0.01745493896305561\n",
      "batch: 720 loss: 0.009481442160904408\n",
      "batch: 730 loss: 0.013633203692734241\n",
      "batch: 740 loss: 0.015990717336535454\n",
      "batch: 750 loss: 0.013586512766778469\n",
      "batch: 760 loss: 0.014676467515528202\n",
      "batch: 770 loss: 0.015703117474913597\n",
      "batch: 780 loss: 0.015667304396629333\n",
      "batch: 790 loss: 0.014875391498208046\n",
      "batch: 800 loss: 0.014008891768753529\n",
      "batch: 810 loss: 0.01939435862004757\n",
      "batch: 820 loss: 0.014438273385167122\n",
      "batch: 830 loss: 0.013022570870816708\n",
      "batch: 840 loss: 0.014613673090934753\n",
      "batch: 850 loss: 0.016953609883785248\n",
      "batch: 860 loss: 0.01370998378843069\n",
      "batch: 870 loss: 0.014851631596684456\n",
      "batch: 880 loss: 0.014222599565982819\n",
      "batch: 890 loss: 0.012736164964735508\n",
      "batch: 900 loss: 0.01287664845585823\n",
      "batch: 910 loss: 0.017598437145352364\n",
      "batch: 920 loss: 0.015963584184646606\n",
      "batch: 930 loss: 0.01602577231824398\n",
      "batch: 940 loss: 0.012383642606437206\n",
      "batch: 950 loss: 0.011257519945502281\n",
      "batch: 960 loss: 0.014397677034139633\n",
      "batch: 970 loss: 0.015660226345062256\n",
      "batch: 980 loss: 0.01837080717086792\n",
      "batch: 990 loss: 0.011841018684208393\n",
      "batch: 1000 loss: 0.011536533012986183\n",
      "batch: 1010 loss: 0.014186670072376728\n",
      "batch: 1020 loss: 0.018161950632929802\n",
      "batch: 1030 loss: 0.01627911999821663\n",
      "batch: 1040 loss: 0.014888664707541466\n",
      "batch: 1050 loss: 0.018185313791036606\n",
      "batch: 1060 loss: 0.014407807029783726\n",
      "batch: 1070 loss: 0.012022445909678936\n",
      "batch: 1080 loss: 0.013798611238598824\n",
      "batch: 1090 loss: 0.014286489225924015\n",
      "batch: 1100 loss: 0.015815583989024162\n",
      "batch: 1110 loss: 0.016226746141910553\n",
      "batch: 1120 loss: 0.01565122976899147\n",
      "batch: 1130 loss: 0.014050651341676712\n",
      "batch: 1140 loss: 0.015293771401047707\n",
      "batch: 1150 loss: 0.012350624427199364\n",
      "batch: 1160 loss: 0.015495353378355503\n",
      "batch: 1170 loss: 0.01381282601505518\n",
      "batch: 1180 loss: 0.01571502722799778\n",
      "batch: 1190 loss: 0.013100673444569111\n",
      "batch: 1200 loss: 0.01685570366680622\n",
      "batch: 1210 loss: 0.017450571060180664\n",
      "batch: 1220 loss: 0.013217763975262642\n",
      "batch: 1230 loss: 0.011586246080696583\n",
      "batch: 1240 loss: 0.015388661995530128\n",
      "batch: 1250 loss: 0.014125047251582146\n",
      "batch: 1260 loss: 0.015545916743576527\n",
      "batch: 1270 loss: 0.011015956290066242\n",
      "batch: 1280 loss: 0.013858813792467117\n",
      "batch: 1290 loss: 0.01543453335762024\n",
      "batch: 1300 loss: 0.015001492574810982\n",
      "batch: 1310 loss: 0.016145950183272362\n",
      "batch: 1320 loss: 0.013292624615132809\n",
      "batch: 1330 loss: 0.014387296512722969\n",
      "batch: 1340 loss: 0.013819213956594467\n",
      "batch: 1350 loss: 0.01376429758965969\n",
      "batch: 1360 loss: 0.013884619809687138\n",
      "batch: 1370 loss: 0.01090556662529707\n",
      "batch: 1380 loss: 0.012598047964274883\n",
      "batch: 1390 loss: 0.012754612602293491\n",
      "batch: 1400 loss: 0.011355644091963768\n",
      "batch: 1410 loss: 0.018125522881746292\n",
      "batch: 1420 loss: 0.01318932045251131\n",
      "batch: 1430 loss: 0.013198002241551876\n",
      "batch: 1440 loss: 0.012256153859198093\n",
      "batch: 1450 loss: 0.01267762016505003\n",
      "batch: 1460 loss: 0.01593164913356304\n",
      "batch: 1470 loss: 0.014216220006346703\n",
      "batch: 1480 loss: 0.014972359873354435\n",
      "batch: 1490 loss: 0.014626936987042427\n",
      "batch: 1500 loss: 0.014488928951323032\n",
      "batch: 1510 loss: 0.015925385057926178\n",
      "batch: 1520 loss: 0.014408747665584087\n",
      "batch: 1530 loss: 0.013873693533241749\n",
      "batch: 1540 loss: 0.016647832468152046\n",
      "batch: 1550 loss: 0.01419437862932682\n",
      "batch: 1560 loss: 0.013890338130295277\n",
      "batch: 1570 loss: 0.01642736606299877\n",
      "batch: 1580 loss: 0.015326038934290409\n",
      "batch: 1590 loss: 0.017577417194843292\n",
      "batch: 1600 loss: 0.0135806854814291\n",
      "batch: 1610 loss: 0.016924463212490082\n",
      "batch: 1620 loss: 0.015053115785121918\n",
      "batch: 1630 loss: 0.013858456164598465\n",
      "batch: 1640 loss: 0.012913797050714493\n",
      "batch: 1650 loss: 0.014861760661005974\n",
      "batch: 1660 loss: 0.01529620960354805\n",
      "batch: 1670 loss: 0.01681891642510891\n",
      "batch: 1680 loss: 0.014155474491417408\n",
      "batch: 1690 loss: 0.012609527446329594\n",
      "batch: 1700 loss: 0.012440036050975323\n",
      "batch: 1710 loss: 0.016736846417188644\n",
      "batch: 1720 loss: 0.016706889495253563\n",
      "batch: 1730 loss: 0.015931757166981697\n",
      "batch: 1740 loss: 0.014285207726061344\n",
      "batch: 1750 loss: 0.016014529392123222\n",
      "batch: 1760 loss: 0.012217639945447445\n",
      "batch: 1770 loss: 0.01492482703179121\n",
      "batch: 1780 loss: 0.013915125280618668\n",
      "batch: 1790 loss: 0.01354558952152729\n",
      "batch: 1800 loss: 0.010525939054787159\n",
      "batch: 1810 loss: 0.012382754124701023\n",
      "batch: 1820 loss: 0.012456486001610756\n",
      "batch: 1830 loss: 0.014027881436049938\n",
      "batch: 1840 loss: 0.01450968999415636\n",
      "batch: 1850 loss: 0.017318453639745712\n",
      "batch: 1860 loss: 0.013699586503207684\n",
      "batch: 1870 loss: 0.014580685645341873\n",
      "batch: 1880 loss: 0.015464754775166512\n",
      "batch: 1890 loss: 0.012902024202048779\n",
      "batch: 1900 loss: 0.0162812490016222\n",
      "batch: 1910 loss: 0.011734153144061565\n",
      "batch: 1920 loss: 0.013418036513030529\n",
      "batch: 1930 loss: 0.018563412129878998\n",
      "batch: 1940 loss: 0.01534625981003046\n",
      "batch: 1950 loss: 0.01759861223399639\n",
      "batch: 1960 loss: 0.01229550875723362\n",
      "batch: 1970 loss: 0.013618833385407925\n",
      "batch: 1980 loss: 0.012849449180066586\n",
      "batch: 1990 loss: 0.013616827316582203\n",
      "batch: 2000 loss: 0.016448890790343285\n",
      "batch: 2010 loss: 0.013405178673565388\n",
      "batch: 2020 loss: 0.011414398439228535\n",
      "batch: 2030 loss: 0.01315166987478733\n",
      "batch: 2040 loss: 0.016054153442382812\n",
      "batch: 2050 loss: 0.011052507907152176\n",
      "batch: 2060 loss: 0.011898073367774487\n",
      "batch: 2070 loss: 0.012709449045360088\n",
      "batch: 2080 loss: 0.018109900876879692\n",
      "batch: 2090 loss: 0.01498013362288475\n",
      "batch: 2100 loss: 0.012086339294910431\n",
      "batch: 2110 loss: 0.01598212495446205\n",
      "batch: 2120 loss: 0.014266029931604862\n",
      "batch: 2130 loss: 0.00914978887885809\n",
      "batch: 2140 loss: 0.012519440613687038\n",
      "batch: 2150 loss: 0.01640581339597702\n",
      "batch: 2160 loss: 0.013998095877468586\n",
      "batch: 2170 loss: 0.013907688669860363\n",
      "batch: 2180 loss: 0.014856784604489803\n",
      "batch: 2190 loss: 0.013602050021290779\n",
      "batch: 2200 loss: 0.0163943562656641\n",
      "batch: 2210 loss: 0.01318417675793171\n",
      "batch: 2220 loss: 0.017041416838765144\n",
      "batch: 2230 loss: 0.010099717415869236\n",
      "batch: 2240 loss: 0.014580066315829754\n",
      "batch: 2250 loss: 0.013255477882921696\n",
      "batch: 2260 loss: 0.017083974555134773\n",
      "batch: 2270 loss: 0.01305205374956131\n",
      "batch: 2280 loss: 0.010146323591470718\n",
      "batch: 2290 loss: 0.016604259610176086\n",
      "batch: 2300 loss: 0.01347433403134346\n",
      "batch: 2310 loss: 0.015357181429862976\n",
      "batch: 2320 loss: 0.012903256341814995\n",
      "batch: 2330 loss: 0.014923987910151482\n",
      "batch: 2340 loss: 0.01488585863262415\n",
      "batch: 2350 loss: 0.011308168061077595\n",
      "batch: 2360 loss: 0.012205804698169231\n",
      "batch: 2370 loss: 0.013316748663783073\n",
      "batch: 2380 loss: 0.018625380471348763\n",
      "batch: 2390 loss: 0.014099006541073322\n",
      "batch: 2400 loss: 0.01491351518779993\n",
      "batch: 2410 loss: 0.014830515719950199\n",
      "batch: 2420 loss: 0.01348364818841219\n",
      "batch: 2430 loss: 0.015724344179034233\n",
      "batch: 2440 loss: 0.014428537338972092\n",
      "batch: 2450 loss: 0.01697566919028759\n",
      "batch: 2460 loss: 0.012665905989706516\n",
      "batch: 2470 loss: 0.016421273350715637\n",
      "batch: 2480 loss: 0.014406310394406319\n",
      "batch: 2490 loss: 0.013907046057283878\n",
      "batch: 2500 loss: 0.0150041738525033\n",
      "batch: 2510 loss: 0.014971543103456497\n",
      "batch: 2520 loss: 0.014251716434955597\n",
      "batch: 2530 loss: 0.014152714051306248\n",
      "batch: 2540 loss: 0.0170468520373106\n",
      "batch: 2550 loss: 0.01607392355799675\n",
      "batch: 2560 loss: 0.013814098201692104\n",
      "batch: 2570 loss: 0.011580470949411392\n",
      "batch: 2580 loss: 0.015926681458950043\n",
      "batch: 2590 loss: 0.015515689738094807\n",
      "batch: 2600 loss: 0.013486521318554878\n",
      "batch: 2610 loss: 0.014251132495701313\n",
      "batch: 2620 loss: 0.015210126526653767\n",
      "batch: 2630 loss: 0.016496090218424797\n",
      "batch: 2640 loss: 0.011750300414860249\n",
      "batch: 2650 loss: 0.016033153980970383\n",
      "batch: 2660 loss: 0.016646364703774452\n",
      "batch: 2670 loss: 0.009680506773293018\n",
      "batch: 2680 loss: 0.01582052931189537\n",
      "batch: 2690 loss: 0.014055431820452213\n",
      "batch: 2700 loss: 0.01586030051112175\n",
      "batch: 2710 loss: 0.013634110800921917\n",
      "batch: 2720 loss: 0.016334982588887215\n",
      "batch: 2730 loss: 0.015266547910869122\n",
      "round  10 done\n"
     ]
    }
   ],
   "source": [
    "# rotating through the training set 10 times.\n",
    "# could technically do more but since there's only 167 MPs doesn't feel necessary.\n",
    "\n",
    "corr_coefs = []\n",
    "for i in np.arange(10):\n",
    "    # rotate the training set\n",
    "    random_h = np.random.choice(tweets_with_proportions['handle'].unique(), 120, replace=False)\n",
    "    train_df = tweets_with_proportions.loc[tweets_with_proportions['handle'].isin(random_h)]\n",
    "    train_df = train_df.reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "    training_dataset = TwitterDataset(train_df, target_function=target_function)\n",
    "    train(net, training_dataset, num_epochs=2, lr=1.)\n",
    "    \n",
    "    # test the training algorithm\n",
    "    train_mps = train_df['handle'].unique()\n",
    "    test_df = tweets_with_proportions.loc[~tweets_with_proportions['handle'].isin(train_mps)]\n",
    "    test_unique = val['handle'].unique()\n",
    "    classification = []\n",
    "    for mp in test_unique:\n",
    "        classification.append([mp, classify_mp(net, mp)])\n",
    "    classification_df = pd.DataFrame(classification, columns=['handle', 'classification'])\n",
    "    test_classifications = pd.merge(left=test_df, right=classification_df,\n",
    "                                    left_on='handle', right_on='handle')\n",
    "    \n",
    "    # get the correlation\n",
    "    corr_coef = np.corrcoef(test_classifications['chowkidar yes/no mean'], test_classifications['classification'])\n",
    "    corr_coefs.append(corr_coef[0].item(1))\n",
    "    \n",
    "    # print message\n",
    "    print('round ', i+1, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.725258853858522,\n",
       " 0.6328639988584503,\n",
       " 0.8172319346091865,\n",
       " 0.8403431983068012,\n",
       " 0.8115976387369959,\n",
       " 0.9100448341298284,\n",
       " 0.844362887638476,\n",
       " 0.7718130831261466,\n",
       " 0.8946074190923581,\n",
       " 0.8777266060500063]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.812585045440677"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(corr_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how to find the weight of a word.  First, we can find it's position in the dictionary.\n",
    "dictionary.token2id['people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52623])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ok so we have to find the weight of that feature number in the input set.\n",
    "# These are the net weights:\n",
    "net.layer1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0014601205475628376"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we just need:\n",
    "net.layer1.weight[0, 514].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating it: \n",
    "def get_weight_of_word(word):\n",
    "    i = dictionary.token2id[word]\n",
    "    return net.layer1.weight[0, i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004132282454520464"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weight_of_word('government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00426780991256237"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weight_of_word('rally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words:\n",
      "0.004356058780103922 condemnable\n",
      "0.00435617147013545 jitubhai\n",
      "0.0043562171049416065 22\n",
      "0.004356327932327986 startups\n",
      "0.004356450866907835 evaluation\n",
      "0.004356687422841787 wvoapwf617\n",
      "0.004356791730970144 ताकतवर\n",
      "0.004356903489679098 nxzkigj7ym\n",
      "0.0043573700822889805 निन्दा\n",
      "0.00435752933844924 शून्यकाल\n",
      "0.004357533995062113 xfgbpm4kaq\n",
      "0.00435763830319047 इ\n",
      "0.004357812460511923 extremely\n",
      "0.004357919562608004 digi\n",
      "0.004358117002993822 रामलाल\n",
      "0.004358556587249041 उठानी\n",
      "0.004358809906989336 bajrang\n",
      "0.004358827602118254 भोजपुर\n",
      "0.004359106067568064 शिल्पकारों\n",
      "0.004359180573374033 ऋषि\n",
      "Bottom words:\n",
      "-0.004359054379165173 उतारे\n",
      "-0.004359048791229725 एपीजे\n",
      "-0.004358836915344 underprivileged\n",
      "-0.004358622245490551 awards\n",
      "-0.004358520731329918 prateek\n",
      "-0.004358514677733183 #amitshahinkashi\n",
      "-0.004358379635959864 2007\n",
      "-0.004358278121799231 पहुंची।\n",
      "-0.00435826787725091 thronged\n",
      "-0.004358107224106789 વડનગર\n",
      "-0.0043578920885920525 #contestalert\n",
      "-0.004357641097158194 trinity\n",
      "-0.004357613157480955 बावजूद\n",
      "-0.004357188008725643 הקשר\n",
      "-0.0043570236302912235 aao\n",
      "-0.004356894176453352 niti-led\n",
      "-0.0043563530780375 والوصول\n",
      "-0.004355938173830509 #vishvassarang\n",
      "-0.004355849698185921 हमीरपुर\n",
      "-0.004355642944574356 token\n"
     ]
    }
   ],
   "source": [
    "word_weights = []\n",
    "for w in dictionary.token2id.keys():\n",
    "    word_weights.append((get_weight_of_word(w), w))\n",
    "word_weights.sort()\n",
    "print(\"Top words:\")\n",
    "for x, w in word_weights[-20:]:\n",
    "    print(x, w)\n",
    "print(\"Bottom words:\")\n",
    "for x, w in word_weights[:20]:\n",
    "    print(x, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
